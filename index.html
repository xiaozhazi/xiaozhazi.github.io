<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Frances Hu&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Frances Hu's Blog">
<meta property="og:url" content="http://xiaozhazi.win/index.html">
<meta property="og:site_name" content="Frances Hu's Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Frances Hu's Blog">
  
    <link rel="alternate" href="/atom.xml" title="Frances Hu&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Frances Hu&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">My idol is Chris Lee!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://xiaozhazi.win"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/" class="article-date">
  <time datetime="2016-11-27T07:46:44.000Z" itemprop="datePublished">2016-11-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第六周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="6、Advice-for-Applying-Machine-Learning"><a href="#6、Advice-for-Applying-Machine-Learning" class="headerlink" title="6、Advice for Applying Machine Learning"></a>6、Advice for Applying Machine Learning</h2><h3 id="6-1、Evaluating-a-learning-Algorithm"><a href="#6-1、Evaluating-a-learning-Algorithm" class="headerlink" title="6.1、Evaluating a learning Algorithm"></a>6.1、Evaluating a learning Algorithm</h3><h4 id="6-1-1、Deciding-What-to-Try-Next"><a href="#6-1-1、Deciding-What-to-Try-Next" class="headerlink" title="6.1.1、Deciding What to Try Next"></a>6.1.1、Deciding What to Try Next</h4><p>Which of the following statements about diagnostics are true? (BCD)</p>
<p>A. It’s hard to tell what will work to improve a learning algorithm, so the best approach is to go with gut feeling and just see what works.</p>
<p>B. Diagnostics can give guidance as to what might be more fruitful things to try to improve a learning algorithm.</p>
<p>C. Diagnostics can be time-consuming to implement and try, but they can still be a very good use of your time.</p>
<p>D. A diagnostic can sometimes rule out certain courses of action(changes to your learning algorithm) as being unlikely to improve its performance signigicantly.</p>
<h4 id="6-1-2、Evaluating-a-Hypothesis"><a href="#6-1-2、Evaluating-a-Hypothesis" class="headerlink" title="6.1.2、Evaluating a Hypothesis"></a>6.1.2、Evaluating a Hypothesis</h4><p>如何评估假设函数，后面可以以此为基础讨论如何避免过拟合和欠拟合的问题。</p>
<p>Suppose an implementation of linear regression(without reguarization) is badly overfitting the training set. In this case, we would expect:</p>
<p>The training error to be low, and the test error to be high.</p>
<p>数据划分为训练集和测试集，一般比例为7:3,random select。如果数据不是随机的，最好自己随机排序或打乱顺序后再选取70%.</p>
<p>Misclassification error(0/1 misclassification error)</p>
<h4 id="6-1-3、Model-Selection-and-Training-Validation-Test-Sets"><a href="#6-1-3、Model-Selection-and-Training-Validation-Test-Sets" class="headerlink" title="6.1.3、Model Selection and Training/Validation/Test Sets"></a>6.1.3、Model Selection and Training/Validation/Test Sets</h4><p>如何确定对于某组数据，最合适的多项式次数是几次？如何选择学习算法中的正规化参数lamda？</p>
<p>模型选择问题。（泛化误差）</p>
<p>我们将数据集分为三部分，training set(60%), cross validation set(20%), test set(20%).</p>
<p>Training error</p>
<p>cross validation error</p>
<p>test error</p>
<p>我们使用交叉验证集来选择模型，看这些假设在交叉验证集表现如何。选择最小交叉验证集误差的模型。</p>
<p>Consider the model selection procedure where we choose the degree of polynomial using a cross validation set. For the final model, we might generally expect Jcv to be lower than Jtest because:</p>
<p>An extra parameter(d,the degree of the polynomial) has been fit to the cross validation set.</p>
<h3 id="6-2、Bias-vs-Variance-偏差和方差"><a href="#6-2、Bias-vs-Variance-偏差和方差" class="headerlink" title="6.2、Bias vs. Variance(偏差和方差)"></a>6.2、Bias vs. Variance(偏差和方差)</h3><h4 id="6-2-1、Diagnosing-Bias-vs-Variance"><a href="#6-2-1、Diagnosing-Bias-vs-Variance" class="headerlink" title="6.2.1、Diagnosing Bias vs. Variance"></a>6.2.1、Diagnosing Bias vs. Variance</h4><p>High Bias Overfitting</p>
<p>High Variance Underfitting</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/6_1.png" alt="6_1"></p>
<h4 id="6-2-2、Regularization-and-Bias-Variance"><a href="#6-2-2、Regularization-and-Bias-Variance" class="headerlink" title="6.2.2、Regularization and Bias/Variance"></a>6.2.2、Regularization and Bias/Variance</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_2.png" alt="6_2"></p>
<h4 id="6-2-3、Learning-Curves"><a href="#6-2-3、Learning-Curves" class="headerlink" title="6.2.3、Learning Curves"></a>6.2.3、Learning Curves</h4><p>判断一个假设是否存在偏差方差问题</p>
<pre><code>High Bias
If a learning algorithm is suffering from high bias, getting more training data will not help much.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_3.png" alt="6_3"><br>因此知道自己的假设是否存在高偏差/方差问题，非常有用。</p>
<pre><code>High variance
If a learning algorithm is suffering from high variance, getting more training data is likely to help.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_4.png" alt="6_4"></p>
<h4 id="6-2-4、Deciding-What-to-Do-Next-Revisited"><a href="#6-2-4、Deciding-What-to-Do-Next-Revisited" class="headerlink" title="6.2.4、Deciding What to Do Next Revisited"></a>6.2.4、Deciding What to Do Next Revisited</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_5.png" alt="6_5"></p>
<h3 id="6-3、Machine-Learning-system-design"><a href="#6-3、Machine-Learning-system-design" class="headerlink" title="6.3、Machine Learning system design"></a>6.3、Machine Learning system design</h3><h4 id="6-3-1、Priorityzing-what-to-work-on"><a href="#6-3-1、Priorityzing-what-to-work-on" class="headerlink" title="6.3.1、Priorityzing what to work on"></a>6.3.1、Priorityzing what to work on</h4><h4 id="6-3-2、Error-Analysis"><a href="#6-3-2、Error-Analysis" class="headerlink" title="6.3.2、Error Analysis"></a>6.3.2、Error Analysis</h4><p>建议的方法：</p>
<p>1、尽快的实现一个最基本的算法，然后在交叉验证数据集上进行验证；</p>
<p>2、画出learning curves，看是否更多的数据、更多的features能提升正确性；</p>
<p>3、Error Analysis：主要分析那些算法预测出错的例子，看能否发现一些系统趋势。</p>
<pre><code>Error analysis may not be helpful for deciding is this is likely to improve performance, only solution is to try it and say if it works.
</code></pre><h4 id="6-3-3-Error-Metrics-for-Skewed-Classes"><a href="#6-3-3-Error-Metrics-for-Skewed-Classes" class="headerlink" title="6.3.3 Error Metrics for Skewed Classes"></a>6.3.3 Error Metrics for Skewed Classes</h4><p>不能仅仅依靠错误率来判断算法的性能，还需要利用Precision 和 Recall<br><img src="http://ofacak8l3.bkt.clouddn.com/6_6.png" alt="6_6"></p>
<h4 id="6-3-4-Trading-off-precision-and-recall"><a href="#6-3-4-Trading-off-precision-and-recall" class="headerlink" title="6.3.4 Trading off precision and recall"></a>6.3.4 Trading off precision and recall</h4><p>我们可以通过计算F1来自动选择性能较高的算法。</p>
<pre><code>F1 = 2*(P*R)/(P+R);
</code></pre><h4 id="6-3-5-Data-for-Machine-Learning"><a href="#6-3-5-Data-for-Machine-Learning" class="headerlink" title="6.3.5 Data for Machine Learning"></a>6.3.5 Data for Machine Learning</h4><p>Banko anf Brill 在2001年提出了一个观点：</p>
<pre><code>It&apos;s not who has the best algorithm that wins, 
It&apos;s who has the most data.
</code></pre><p>这个观点有时候不一定正确：</p>
<p>当feature很少不足以预测时（例，预测房价时仅知道尺寸），此时再多的数据可能也无法提升算法的性能； </p>
<p>当算法参数很多时（例，逻辑回归和线性回归有很多的feature，或者神经网络有很多的隐藏单元），更大的测试集可以尽可能的减少过拟合。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/" data-id="ciw0jflia0001ulo3qsavf04v" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/" class="article-date">
  <time datetime="2016-11-24T07:10:34.000Z" itemprop="datePublished">2016-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第五周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="5、Neural-Networks：Learning"><a href="#5、Neural-Networks：Learning" class="headerlink" title="5、Neural Networks：Learning"></a>5、Neural Networks：Learning</h2><h3 id="5-1、Cost-Function-and-Backpropagation"><a href="#5-1、Cost-Function-and-Backpropagation" class="headerlink" title="5.1、Cost Function and Backpropagation"></a>5.1、Cost Function and Backpropagation</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_1.png" alt="5_1"></p>
<h3 id="5-2、Backprogation-algorithm"><a href="#5-2、Backprogation-algorithm" class="headerlink" title="5.2、Backprogation algorithm"></a>5.2、Backprogation algorithm</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_2.png" alt="5_2"></p>
<h3 id="5-3、Gradient-checking"><a href="#5-3、Gradient-checking" class="headerlink" title="5.3、Gradient checking"></a>5.3、Gradient checking</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_3.png" alt="5_3"></p>
<pre><code>Important:
Be sure to disable your gradient checking code before training your claaifier. otherwise code will be very slow.
</code></pre><h3 id="5-4-Random-initialization"><a href="#5-4-Random-initialization" class="headerlink" title="5.4 Random initialization"></a>5.4 Random initialization</h3><pre><code>initial each initialtheta to a random value in[-EPSILON,EPSILON]
</code></pre><h3 id="5-5-Put-it-together"><a href="#5-5-Put-it-together" class="headerlink" title="5.5 Put it together"></a>5.5 Put it together</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_4.png" alt="5_4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/5_5.png" alt="5_5"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/" data-id="ciw0jflik0004ulo3xvva02ah" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-毕设论文相关知识索引笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/24/毕设论文相关知识索引笔记/" class="article-date">
  <time datetime="2016-11-24T06:14:40.000Z" itemprop="datePublished">2016-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/24/毕设论文相关知识索引笔记/">毕设论文相关知识索引笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Cgroup介绍、应用实例及原理描述"><a href="#Cgroup介绍、应用实例及原理描述" class="headerlink" title="Cgroup介绍、应用实例及原理描述"></a>Cgroup介绍、应用实例及原理描述</h2><p><a href="http://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html" target="_blank" rel="external">Cgroup介绍、应用实例及原理描述</a></p>
<h3 id="Cgroup设计原理分析"><a href="#Cgroup设计原理分析" class="headerlink" title="Cgroup设计原理分析"></a>Cgroup设计原理分析</h3><p>Linux中，管理进程的数据结构是task_struct</p>
<pre><code>#ifdef CONFIG_CGROUPS
/*Control Group info protected by css_set_lock*/
struct css_set *cgroups;
/*cg_list protected by css_set_lock and tsk-&gt;alloc_lock*/
struct list_head cg_list;
#endif
</code></pre><p>其中*cgroups指向css_set结构，css_set存储了与进程相关的cgroups信息。cg_list是一个嵌入式的list_head结构，将链到同一个css_set的进程组织成一个链表。</p>
<pre><code>struct css_set{
  atomic_t refcount;
  struct hlist_node hlist;
  struct list_head tasks;
  struct list_head cg_links;
  struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
  struct rcu_head ruc_head;
};
</code></pre><p>其中，refcount是该css_set的引用数，因为一个css_set可以被多个进程公用，只要这些进程的cgroups信息相同。</p>
<p>hlist是嵌入的hlist_node，用于把所有css_set组成一个hash表，这样内核可以快速定位css_set。</p>
<p>tasks指向所有连到此css_set的进程指向的链表。</p>
<p>cg_links指向一个由struct_cg_group_link连城的链表。</p>
<p>subsys是一个指针数组，存储一组指向cgroup_subsys_state的指针，一个cgroup_subsys_state就是进程与一个特定子系统相关的信息。通过这个数组，进程可以获得相应的cgroup控制信息了。</p>
<pre><code>struct cgroup_subsys_state{
  struct cgroup *cgroup;
  atomic_t refcnt;
  unsigned long flags;
  struct css_id *id;
};
</code></pre><p>*cgroup指向一个cgroup结构，也就是进程属于的cgroup。</p>
<pre><code>task_struct--&gt;css_set--&gt;cgroup_subsys_state--&gt;cgroup
</code></pre><p>这样进程就和cgroup连接起来了。</p>
<pre><code>struct cgroup{
  unsigned long flags;
  atomic_t count;
  struct list_head sibling;
  struct list_head children;
  struct cgroup *parent;
  struct dentry *dentry;
  struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT]
  struct cgroupfs_root *root;
  struct cgroup *top_cgroup;
  struct list_head css_sets;
  struct list_head release_list;
  struct list_head pidlists;
  struct mutex pidlist_mutex;
  struct rcu_head rcu_head;
  struct list_head event_list;
  spinlock_t event_list_lock;
};
</code></pre><p>sibling,children,parent三个嵌入的list_head负责将同一层级的cgroup连接成一颗cgroup树。</p>
<p>subsys指针数组，存储一组指向cgroup_subsys_state的指针。这组指针指向了此cgroup跟各个子系统相关的信息。类似css_set中。</p>
<p>root指向一个cgroupfs_root结构，就是cgroup所在层级对应的结构体。</p>
<p>top_cgroup指向了所在层级的根cgroup，即创建层级时自动创建的那个cgroup。</p>
<p>css_set指向一个由struct_cg_cgroup_link连成的链表。</p>
<pre><code>struct cg_cgroup_link{
  struct list_head cgrp_link_list;
  struct cgroup *cgrp;
  struct list_head cg_link_list;
  struct css_set *cg;
};
</code></pre><p>cgrp_link_list连入到 cgroup-&gt;css_set链表，cgrp指向此cg_group_link相关的cgroup。</p>
<p>cg_link_list连入到css_set-&gt;cg_links指向的链表，cg指向此cg_group_link相关的css_set。</p>
<p>cgroup和css_set是一个多对多的关系，必须添加一个中间结构来将两者联系起来，这既是cg_cgroup_link的作用。cgrp和cg为此结构体的联合主键，cgrp_link_list和cg_link_list分别连入cgroup和css_set，使后两者都可以尽心遍历查询。</p>
<p>####为什么cgroup和css_set是多对多的关系？<br>一个进程对应一个css_set，一个css_set存储了一组进程跟各个子系统相关的信息，但是这些信息由可能不是从一个cgroup那里获得的，因为一个进程可以同时属于多个cgroup，只要这些cgroup不在同一个层级。</p>
<pre><code>eg：
我们创建一个层级A，A上面附加了CPU和memory两个子系统，进程a属于A的根cgroup；创建一个层级B，B上面附加了ns和blkio两个子系统，进程a也属于B的根cgroup；那么进程a对应的CPU和memory是从A的根cgroup获得的，ns和blkio则是从B的根cgroup获得。因此一个css_set存储的cgroup_subsys_state可以对应多个cgroup。另一方面cgroup也存储了一组cgroup_subsys_state，这一组则是cgroup从所在的层级附加的子系统获得的。一个cgroup可以有多个进程，这些进程的css_set不一定都相同，因为有些进程可能还加入了其他cgroup，但是同一个cgroup中的进程与该cgroup关联的cgroup_subsys_state都受到该cgroup的管理，所以一个cgroup也可以对应多个css_set。
</code></pre><p>从前面的分析，我们可以看出从 task 到 cgroup 是很容易定位的，但是从 cgroup 获取此 cgroup 的所有的 task 就必须通过这个结构了。每个进程都回指向一个 css_set，而与这个 css_set 关联的所有进程都会链入到 css_set-&gt;tasks 链表，而 cgroup 又通过一个中间结构 cg_cgroup_link 来寻找所有与之关联的所有 css_set，从而可以得到与 cgroup 关联的所有进程。    </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/24/毕设论文相关知识索引笔记/" data-id="ciw0jflj6000iulo3t6gq4krq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/" class="article-date">
  <time datetime="2016-11-13T07:48:54.000Z" itemprop="datePublished">2016-11-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第四周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="4、Neural-Networks"><a href="#4、Neural-Networks" class="headerlink" title="4、Neural Networks"></a>4、Neural Networks</h2><h3 id="4-1-Motivations"><a href="#4-1-Motivations" class="headerlink" title="4.1 Motivations"></a>4.1 Motivations</h3><pre><code>Neural Networks
Origins: Algorithms that try to mimic the brain.
Was very widely used in 80s and early 90s;
popularity diminished in late 90s.
Recent resurgence: 
    state-of-the-art technique for many applications. 
</code></pre><h3 id="4-2-Neural-Networks"><a href="#4-2-Neural-Networks" class="headerlink" title="4.2 Neural Networks"></a>4.2 Neural Networks</h3><h4 id="4-2-1-Model-Representation-I"><a href="#4-2-1-Model-Representation-I" class="headerlink" title="4.2.1 Model Representation I"></a>4.2.1 Model Representation I</h4><p>Sigmoid(logistic) activation function.<br><img src="http://ofacak8l3.bkt.clouddn.com/4_1.png" alt="4_1"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/4_2.png" alt="4_2"></p>
<h4 id="4-2-2-Model-Representation-II"><a href="#4-2-2-Model-Representation-II" class="headerlink" title="4.2.2 Model Representation II"></a>4.2.2 Model Representation II</h4><pre><code>input层
hidden层
output层
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/4_3.png" alt="4_3"></p>
<h3 id="4-3-Multiple-output-units-One-vs-all"><a href="#4-3-Multiple-output-units-One-vs-all" class="headerlink" title="4.3 Multiple output units:One-vs-all."></a>4.3 Multiple output units:One-vs-all.</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/4_4.png" alt="4_4"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/" data-id="ciw0jflin0005ulo3waw870lp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning-AI-course/">Machine learning, AI, course</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Machine-Learning编程作业2——Logistic-Regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/" class="article-date">
  <time datetime="2016-11-07T02:31:30.000Z" itemprop="datePublished">2016-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/">Machine Learning编程作业2——Logistic Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>##ex2.m<br>=============plotting================</p>
<p>data = load(‘ex2data1.txt’);</p>
<p>X = data(:,[1,2]);</p>
<p>y = data(:,3);</p>
<p>plotData(X,y);</p>
<pre><code>plotData.m

function plotData(X,y)
figure;hold on;

pos = find(y==1);
neg = find(y==0);
plot(X(pos,1),X(pos,2),&apos;k+&apos;,&apos;LineWidth&apos;,2,...
    &apos;MarkerSize&apos;,7);
plot(X(neg,1),X(neg,2),&apos;ko&apos;,&apos;MarkerFaceColor&apos;,&apos;y&apos;,...
    &apos;MarkerSize&apos;,7);

hold off;
end
</code></pre><p>=============compute cost and gradient===========</p>
<p>[m,n] = size(X);</p>
<p>X = [ones(m,1) X];</p>
<p>initial_theta = zeros(n+1,1);</p>
<p>[cost,grad] = costFunction(initial_theta,X,y);</p>
<pre><code>costFunction.m

function [J,grad] = costFunction(theta,X,y)
m = length(y);
J = 0;
grad = zeros(size(theta));

J = (-1)/m *(log(sigmoid(X*theta))&apos;*y +
            log(1-sigmoid(X*theta))&apos;*(1-y));
for i = 1: size(X,2)
  grad(i) = 1/m * sum((sigmoid(X*theta)-y) .* X(:,i));
end
end
</code></pre><p>plotDecisionBoundary(theta,X,y);</p>
<p>=============predict and accuracies===============</p>
<p>prob = sigmoid([1 45 85] * theta);</p>
<p>p = predict(theta,X);</p>
<pre><code>predict.m

function p = predict(theta,X)

m = size(X,1);
p = zeros(m,1);

for i = 1:m
    if(sigmoid(X(i,:) * theta)) &gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
end
</code></pre><h2 id="ex2-reg-m"><a href="#ex2-reg-m" class="headerlink" title="ex2_reg.m"></a>ex2_reg.m</h2><p>clear;</p>
<p>data = load(‘ex2data2.txt’);</p>
<p>X = data(:,[1,2]);</p>
<p>y = data(:,3);</p>
<p>plotData(X,y);</p>
<p>=====================regularized Logistic Regression======</p>
<p>X = mapFeature(X(:,1),X(:,2));</p>
<p>initial_theta = zeros(size(X,2),1);</p>
<p>lambda = 1;</p>
<p>[cost,grad] = costFunctionReg(initial_theta,X,y,lambda);</p>
<pre><code>costFunctionReg.m

function [J,grad] = costFunctionReg(theta,X,y,lambda)
m = length(y);
J = 0;
grad = zeros(size(theta));

temp = theta(2:size(theta,1),:) .^2;
value = sum(temp);
J = (-1)/m * (log(sigmoid(X*theta))&apos;*y +
            log(1-sigmoid(X*theta))&apos;*(1-y))
        +lambda/(2*m) * value;
grad(1) = 1/m*sum((sigmoid(X*theta) - y) .* X(:,1));

for i = 2: size(X,2)
    grad(i) = 1 / m *sum((sigmoid(X*theta) -y).*X(:,i))
     +lambda/m * theta(i);
end
end
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/" data-id="ciw0jfliz000culo3vpbo8toc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/octave-program/">octave, program</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第三周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/01/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第三周/" class="article-date">
  <time datetime="2016-11-01T07:30:52.000Z" itemprop="datePublished">2016-11-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/01/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第三周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第三周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="3、Logistic-Regression"><a href="#3、Logistic-Regression" class="headerlink" title="3、Logistic Regression"></a>3、Logistic Regression</h2><p>主要分为三部分：Classification and Representation、Logistic Regression Model、Multiclass Classfication</p>
<h3 id="3-1-Classification-and-Representation"><a href="#3-1-Classification-and-Representation" class="headerlink" title="3.1 Classification and Representation"></a>3.1 Classification and Representation</h3><h4 id="3-1-1-Classification"><a href="#3-1-1-Classification" class="headerlink" title="3.1.1 Classification"></a>3.1.1 Classification</h4><p>Y ∈ {0，1} 0：‘Negative Class’ 1：‘Positive Class’<br><img src="http://ofacak8l3.bkt.clouddn.com/3_1.png" alt="3_1"></p>
<p>Classification: hθ(x)可以 &gt;1 or &lt;0</p>
<p>Logistic Regression:  0&lt;= hθ(x) &lt;=1</p>
<h4 id="3-1-2-Hypothesis-Representation"><a href="#3-1-2-Hypothesis-Representation" class="headerlink" title="3.1.2 Hypothesis Representation"></a>3.1.2 Hypothesis Representation</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/3_2.png" alt="3_2"></p>
<pre><code>hθ(x) = estimated probability that y=1 on input x
</code></pre><h4 id="3-1-3-Decision-Boundary"><a href="#3-1-3-Decision-Boundary" class="headerlink" title="3.1.3 Decision Boundary"></a>3.1.3 Decision Boundary</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/3_3.png" alt="3_3"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/3_4.png" alt="3_4"></p>
<h3 id="3-2-Logistic-Regression-Model"><a href="#3-2-Logistic-Regression-Model" class="headerlink" title="3.2 Logistic Regression Model"></a>3.2 Logistic Regression Model</h3><h4 id="3-2-1-Cost-Function"><a href="#3-2-1-Cost-Function" class="headerlink" title="3.2.1 Cost Function"></a>3.2.1 Cost Function</h4><pre><code>Cost(hθ(x),y) = -log(hθ(x))    if y=1
                -log(1-hθ(x))  if y=0
</code></pre><h4 id="3-2-2-Simplified-Cost-Function-and-Gradient-Descent"><a href="#3-2-2-Simplified-Cost-Function-and-Gradient-Descent" class="headerlink" title="3.2.2 Simplified Cost Function and Gradient Descent"></a>3.2.2 Simplified Cost Function and Gradient Descent</h4><pre><code>Cost(hθ(x),y) = -ylog(hθ(x)) - (1-y)log(1-hθ(x))
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/3_5.png" alt="3_5"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/3_6.png" alt="3_6"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/3_7.png" alt="3_7"></p>
<h3 id="3-3-Multiclass-Classification"><a href="#3-3-Multiclass-Classification" class="headerlink" title="3.3 Multiclass Classification"></a>3.3 Multiclass Classification</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/3_8.png" alt="3_8"></p>
<h3 id="3-4-Solving-the-Problem-of-Overfitting"><a href="#3-4-Solving-the-Problem-of-Overfitting" class="headerlink" title="3.4 Solving the Problem of Overfitting"></a>3.4 Solving the Problem of Overfitting</h3><pre><code>Overfitting: If we have too many features, the 
learning hypoyhesis may fit the training set very well,
but fail to generalize to new examples.
</code></pre><p>Addressing overfitting:</p>
<p>Options:</p>
<pre><code>1. Reduce number of features.
    --Manually select which features to keep.
    --Model selection algorithm.
2. Regularization.
    --Keep all the features,but reduce magnitude
      /values of parameters θj.
    --Works well when we have a lo
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/01/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第三周/" data-id="ciw0jfli60000ulo3eqiyrqoe" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/course-AI-machine-Learning/">course, AI, machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Machine-Learning编程作业1——Linear-Regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/31/Machine-Learning编程作业1——Linear-Regression/" class="article-date">
  <time datetime="2016-10-31T02:50:02.000Z" itemprop="datePublished">2016-10-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/31/Machine-Learning编程作业1——Linear-Regression/">Machine Learning编程作业1——Linear Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="作业源码"><a href="#作业源码" class="headerlink" title="作业源码"></a>作业源码</h2><h3 id="warmUpExercise-m-basic-function"><a href="#warmUpExercise-m-basic-function" class="headerlink" title="warmUpExercise.m   ____basic function"></a>warmUpExercise.m   ____basic function</h3><p>fprintf(‘Running warmUpExercise …\n’);</p>
<p>fprintf(‘5*5 Identity Matrix: \n”);</p>
<p>warmUpExercise()</p>
<pre><code>function A = warmUpExercise()

A = [];
A = eye(5);

end
</code></pre><h3 id="plotData-m-Plotting"><a href="#plotData-m-Plotting" class="headerlink" title="plotData.m ____Plotting"></a>plotData.m ____Plotting</h3><p>fprintf(‘Plotting Data…\n’)</p>
<p>data = load(‘ex1data1.txt’);</p>
<p>X = data(:,1); y = data(;,2);</p>
<p>m = length(y); %num of training examples</p>
<p>plotData(X,y)</p>
<pre><code>function plotData(x,y)

figure;
plot(x,y,&apos;rx&apos;,&apos;MarkerSize&apos;,10);
ylabel(&apos;Profit in $10,000s&apos;);
xlabel(&apos;Poputation of City in 10,000s&apos;);

end
</code></pre><h3 id="gradientDescent-m-Gradient-Descent"><a href="#gradientDescent-m-Gradient-Descent" class="headerlink" title="gradientDescent.m ____Gradient Descent"></a>gradientDescent.m ____Gradient Descent</h3><p>fprintf(‘Running Gradient Descent …\n’)</p>
<p>X = [ones[m,1],data(;,1)];</p>
<p>theta = zeros(2,1);</p>
<p>iterations = 1500;</p>
<p>alpha = 0.01;</p>
<p>computeCost(X,y,theta);</p>
<h4 id="computeCost-m-compute-initial-cost"><a href="#computeCost-m-compute-initial-cost" class="headerlink" title="computeCost.m ____compute initial cost"></a>computeCost.m ____compute initial cost</h4><pre><code>function J = computeCost(X,y,theta)

m = length(y);
J = 0;
cost = 0;
for i = 1 : m
    cost += (theta(1,1) * X(i,1) + theta(2,1) * X(i,2)
            - y(i))^2;
end

J = 1/(2*m) * cost;

end
</code></pre><p>theta = gradientDescent(X,y,theta,alpha,iterations);</p>
<pre><code>function [theta,J_history] = gradientDescent(X,y,theta,
                alpha,num_iters)

m = length(y);
J_history = zeros(num_iters,1);

for iter = 1 : num_iters
    cost_theta1 = 0;
    cost_theta2 = 0;
    for i = 1 : m
        cost_theta1 += (theta(1,1) * X(i,1) + 
            theta(2,1) * X(i,2) - y(i)) * X(i,1);
        cost_theta2 += (theta(1,1) * X(i,1) +
            theta(2,1) * X(i,2) - y(i)) * X(i,2);
    end
    new_theta1 = theta(1,1) - alpha*cost_theta1 * 1/m;
    new_theta2 = theta(2,1) - alpha*cost_theta2 * 1/m;
    theta(1,1) = new_theta1;
    theta(2,1) = new_theta2;

    J_history(iter) = computeCost(X,y,theta);
end
end
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/10/31/Machine-Learning编程作业1——Linear-Regression/" data-id="ciw0jfliv0009ulo3byk0x8ew" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/octave-program/">octave, program</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第二周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第二周/" class="article-date">
  <time datetime="2016-10-24T06:00:42.000Z" itemprop="datePublished">2016-10-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第二周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第二周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="课程内容简介"><a href="#课程内容简介" class="headerlink" title="课程内容简介"></a>课程内容简介</h2><p>课程主要介绍机器学习、数据挖掘和统计模式识别。相关主题包括：<br>i) 监督式学习（参数和非参数算法、支持向量机、核函数、神经网络）<br>ii）无监督学习（集群、降维、推荐系统、深度学习）<br>iii) 机器学习实例（偏见／方差理论、机器学习和AI领域的创新）</p>
<h3 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h3><h4 id="第二周"><a href="#第二周" class="headerlink" title="第二周"></a>第二周</h4><p>1、环境搭建</p>
<p>2、Multivariate Linear Regression，多元线性回归。 </p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9.png" alt="9"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/10.png" alt="10"></p>
<pre><code>Feature Scaling:
Make sure features are on a similar scale.

为什么使用feature scaling？
It speeds up gradient by making it require fewer
iterations to get to 


(Get every feature into approximately a -1&lt;= xi &lt;=1)
(Mean normalization: replace xi with xi-ui to make
 features have approximately zero mean.)
</code></pre><p>如何保证梯度下降正确完成？</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/11.png" alt="11"></p>
<pre><code>Making sure gradient descent is working correctly.
如果随着迭代次数的增加J(θ)反而越来越大，应该用更小的α。
If α is too small: slow convergence.
If α is too large: J(θ) may be not decrease on
            every interation, may not converge.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/12.png" alt="12"></p>
<p>3、Computing Parameters Analytically    </p>
<p>正态方程<br><img src="http://ofacak8l3.bkt.clouddn.com/13.png" alt="13"></p>
<p>Octave: pinv(X’ <em> X) </em> X’ * Y   (X’ 即XT)</p>
<p>什么时候用梯度下降？什么时候用正态方程？</p>
<pre><code>m training examples, n features.

Gradient Descent
* Need to choose α.
* Need many iterations.
* Work well even when n is large.

Normal Equation
* No need to choose α.
* Don&apos;t need to iterate.
* Need to cumpute (XTX)-1.   O(n3) 
* Slow if n is very large.
</code></pre><p>如果XTX是不可逆的怎么办？</p>
<pre><code>Octave 有两种计算矩阵逆的函数——pinv、inv
如果使用pinv计算的话都会得到实际的结果，不管矩阵是否可逆。

* Redundant features(linearly dependent).
* Too many features(eg. m &lt;= n)
    delete some features,or use regularization.
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/10/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第二周/" data-id="ciw0jflii0003ulo3ibc9pxur" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI-Machine-Learning-coursera/">AI, Machine Learning, coursera</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门笔记一" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/10/18/AI入门笔记一/" class="article-date">
  <time datetime="2016-10-18T02:44:48.000Z" itemprop="datePublished">2016-10-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/10/18/AI入门笔记一/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第一周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="课程内容简介"><a href="#课程内容简介" class="headerlink" title="课程内容简介"></a>课程内容简介</h2><p>课程主要介绍机器学习、数据挖掘和统计模式识别。相关主题包括：<br>i) 监督式学习（参数和非参数算法、支持向量机、核函数、神经网络）<br>ii）无监督学习（集群、降维、推荐系统、深度学习）<br>iii) 机器学习实例（偏见／方差理论、机器学习和AI领域的创新）</p>
<h3 id="课程学习"><a href="#课程学习" class="headerlink" title="课程学习"></a>课程学习</h3><h4 id="第一周"><a href="#第一周" class="headerlink" title="第一周"></a>第一周</h4><p>1、10月18日学习Introduction章节。主要介绍了什么是Machine Learning及其意义，后续介绍了监督学习和无监督学习，其中监督学习主要介绍了regression和classification，无监督学习主要是cluster。(review三遍才过，对于有无监督学习理解不深刻)</p>
<pre><code>Machine Learning is the field of study that gives computers 
the ability to learn without being explicitly programmed.
</code></pre><p>2、10月19日学习Model and Cost Function章节。分为四个小节，Model Representation、Cost Function、Cost Function-Intuition I、Cost Function-Intuition II。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/hypothesis_structure.png" alt="hypothsis structure"></p>
<pre><code>Cost Function —— square error function
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/2.png" alt="2"></p>
<p>我们的目标就是尽可能的使Cost Function值最小。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/3.png" alt="3"></p>
<p>3、10月19日学习Parameter Learning章节。分为三个小节Gradient Descent、Gradient Descent Intuition、Gradient Descent For Linear Regression。</p>
<p>因此采用梯度下降的方法</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/4.png" alt="4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/5.png" alt="5"></p>
<p>注意，上述temp0和temp1是同步变化的。</p>
<p>参数a的选取也至关重要，不仅影响算法的效率，还对是否能寻找到局部最优解起至关重要的作用。<br><img src="http://ofacak8l3.bkt.clouddn.com/6.png" alt="6"></p>
<pre><code>As we approach a local minimum, gradient descent will sutomatically
take smaller steps. So, no need to decrease a over time.
</code></pre><p>最终算法表示如下：<br><img src="http://ofacak8l3.bkt.clouddn.com/7.png" alt="7"><br><img src="http://ofacak8l3.bkt.clouddn.com/8.png" alt="8"></p>
<p>4、10月20日学习Linear Algebra Review，主要是复习了线性代数的相关知识，矩阵计算、矩阵的逆和转置等。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/10/18/AI入门笔记一/" data-id="ciw0jflir0008ulo322hjcv8j" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI-Machine-Learning-coursera/">AI, Machine Learning, coursera</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-如何在MAC上搭建博客——hexo-github" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/09/27/如何在MAC上搭建博客——hexo-github/" class="article-date">
  <time datetime="2016-09-27T03:08:23.000Z" itemprop="datePublished">2016-09-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/09/27/如何在MAC上搭建博客——hexo-github/">如何在MAC上搭建博客——hexo+github</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>1、git——git-scm官网<a href="http://git-scm.com" target="_blank" rel="external">git-scm</a>下载mac版</p>
<p>2、node.js——直接到<a href="https://nodejs.org" target="_blank" rel="external">node.js官网</a>下载安装 </p>
<p>3、markdown编辑器——mac下推荐<a href="25.io/mou/">mou</a>或者<a href="http://macdown.uranusjr.com/" target="_blank" rel="external">macDown</a> 本人下载mou时其还未支持macOS sierra</p>
<p>4、域名——阿里云购买即可</p>
<h3 id="安装和初始化"><a href="#安装和初始化" class="headerlink" title="安装和初始化"></a>安装和初始化</h3><p>mac下在命令行下安装hexo</p>
<pre><code>sudo npm install hexo -g
hexo init blog
cd blog
sudo npm install
hexo server
</code></pre><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置站点文件_config.yml,主要修改Site部分，URL部分，deploy部分</p>
<pre><code>#Site
title: Frances Hu&apos;s Blog
subtitle: 技术迷，本命李宇春
author: Frances Hu
...
language: zh_CN

#URL
url: http://xiaozhazi.win

deploy:
    type: git
    repository: https://github.com/xiaozhazi/xiaozhazi.github.io.git
    branch: master
</code></pre><h3 id="部署网页"><a href="#部署网页" class="headerlink" title="部署网页"></a>部署网页</h3><p>在blog文件夹目录下执行生成静态页面的命令：</p>
<pre><code>hexo generate
hexo deploy
</code></pre><p>如果出错，无法识别git，则执行如下命令来安装hexo-deployer-git</p>
<pre><code>sudo npm install hexo-deployer-git --save
</code></pre><h3 id="绑定个人域名"><a href="#绑定个人域名" class="headerlink" title="绑定个人域名"></a>绑定个人域名</h3><p>在/blog/themes/landscape/source目录下新建CNAME文件，将自己的域名 xiaozhazi.win写入即可</p>
<p>重新部署</p>
<pre><code>hexo clean
hexo d -g
</code></pre><p>###安装主题<br>在blog目录下执行如下命令</p>
<pre><code>git clone https://github.com/iissnan/hexo-theme-next themes/next
</code></pre><p>将blog目录下的_config.yml里theme的名称改为next即可</p>
<p>也可以到<a href="http://hexo.io.themes" target="_blank" rel="external">hexo官网主题页</a>下载自己喜欢的主题</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/09/27/如何在MAC上搭建博客——hexo-github/" data-id="ciw0jflj4000gulo39noycjn1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Machine-Learning-coursera/">AI, Machine Learning, coursera</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-learning-AI-course/">Machine learning, AI, course</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/course-AI-machine-Learning/">course, AI, machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave-program/">octave, program</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI-Machine-Learning-coursera/" style="font-size: 20px;">AI, Machine Learning, coursera</a> <a href="/tags/Machine-learning-AI-course/" style="font-size: 10px;">Machine learning, AI, course</a> <a href="/tags/course-AI-machine-Learning/" style="font-size: 10px;">course, AI, machine Learning</a> <a href="/tags/octave-program/" style="font-size: 20px;">octave, program</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第六周</a>
          </li>
        
          <li>
            <a href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第五周</a>
          </li>
        
          <li>
            <a href="/2016/11/24/毕设论文相关知识索引笔记/">毕设论文相关知识索引笔记</a>
          </li>
        
          <li>
            <a href="/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第四周</a>
          </li>
        
          <li>
            <a href="/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/">Machine Learning编程作业2——Logistic Regression</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Frances Hu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>