<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Frances Hu&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Frances Hu's Blog">
<meta property="og:url" content="http://xiaozhazi.win/index.html">
<meta property="og:site_name" content="Frances Hu's Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Frances Hu's Blog">
  
    <link rel="alternate" href="/atom.xml" title="Frances Hu&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Frances Hu&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">My idol is Chris Lee!</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://xiaozhazi.win"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/31/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周/" class="article-date">
  <time datetime="2016-12-31T07:54:03.000Z" itemprop="datePublished">2016-12-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/31/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十一周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="11、Application-Example-Photo-OCR"><a href="#11、Application-Example-Photo-OCR" class="headerlink" title="11、Application Example: Photo OCR"></a>11、Application Example: Photo OCR</h1><h2 id="11-1、Photo-OCR"><a href="#11-1、Photo-OCR" class="headerlink" title="11.1、Photo OCR"></a>11.1、Photo OCR</h2><h3 id="11-1-1、problem-description-and-pipeline"><a href="#11-1-1、problem-description-and-pipeline" class="headerlink" title="11.1.1、problem description and pipeline"></a>11.1.1、problem description and pipeline</h3><p>photo OCR pipeline </p>
<ol>
<li>Text detection</li>
<li>Character segmentation</li>
<li>Character classification</li>
</ol>
<h3 id="11-1-2、sliding-window"><a href="#11-1-2、sliding-window" class="headerlink" title="11.1.2、sliding window"></a>11.1.2、sliding window</h3><p>对整个图片进行分别窗口化检测</p>
<h3 id="11-1-3、Getting-lots-of-data-Artificial-data-synthesis"><a href="#11-1-3、Getting-lots-of-data-Artificial-data-synthesis" class="headerlink" title="11.1.3、Getting lots of data: Artificial data synthesis"></a>11.1.3、Getting lots of data: Artificial data synthesis</h3><p>Synthesizing data by introducing distortions</p>
<ol>
<li>Distortion introduced should be representation of the type of noise/distortions in the test set.</li>
<li>Usually does not help to add purely random/meaningless noise to your data. </li>
</ol>
<h3 id="11-1-4、What-part-of-the-pipeline-to-work-on-next"><a href="#11-1-4、What-part-of-the-pipeline-to-work-on-next" class="headerlink" title="11.1.4、What part of the pipeline to work on next"></a>11.1.4、What part of the pipeline to work on next</h3><p>进行分块处理的目的是，我们可以很容易的分析出那一步骤是系统性能瓶颈，需要在那一步骤上投入更多的精力。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/11_1.png" alt="11_1"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/11_2.png" alt="11_2"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/12/31/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周/" data-id="cixcyzm5v0005q8o3b6vjsn61" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/α-β-δ-ε-η-θ-ξ-μ-λ-ν-ξ-ο-π-ρ-σ-τ-υ-φ-χ-ψ-ω/">α β δ ε η θ ξ μ λ ν ξ ο π ρ σ τ υ φ χ ψ ω</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/23/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周/" class="article-date">
  <time datetime="2016-12-23T07:53:56.000Z" itemprop="datePublished">2016-12-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/23/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="10、Large-Scale-Machine-Learning"><a href="#10、Large-Scale-Machine-Learning" class="headerlink" title="10、Large Scale Machine Learning"></a>10、Large Scale Machine Learning</h1><h2 id="10-1、Gradient-Descent-with-Large-Datasets"><a href="#10-1、Gradient-Descent-with-Large-Datasets" class="headerlink" title="10.1、Gradient Descent with Large Datasets"></a>10.1、Gradient Descent with Large Datasets</h2><h3 id="10-1-1、Learning-with-large-dataset"><a href="#10-1-1、Learning-with-large-dataset" class="headerlink" title="10.1.1、Learning with large dataset"></a>10.1.1、Learning with large dataset</h3><p>It’s not who has the best algorithm that wins.<br>It’s who has the most data.</p>
<h3 id="10-1-2、Stochastic-gradient-descent"><a href="#10-1-2、Stochastic-gradient-descent" class="headerlink" title="10.1.2、Stochastic gradient descent"></a>10.1.2、Stochastic gradient descent</h3><p>不像Batch gradient descent每次迭代时都需要将数据集代入计算。</p>
<p>当数据集大的时候我们需要随机shuffle数据集，<br>然后选取第一个数据进行计算，然后进行梯度下降，再接着使用接下来的数据依次进行此过程。</p>
<h3 id="10-1-3、Mini-batch-gradient-descent"><a href="#10-1-3、Mini-batch-gradient-descent" class="headerlink" title="10.1.3、Mini-batch gradient descent"></a>10.1.3、Mini-batch gradient descent</h3><p>batch 和  stochastic的结合。每次选一组数据进行计算，然后再接着使用下一组重复此过程。</p>
<h3 id="10-1-4、Stochastic-gradient-descent-convergence"><a href="#10-1-4、Stochastic-gradient-descent-convergence" class="headerlink" title="10.1.4、Stochastic gradient descent convergence"></a>10.1.4、Stochastic gradient descent convergence</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/10_1.png" alt="10_1"></p>
<p>Learning rate α is tapically held constant. Can slowly descrease α over time if we want θ to converge.</p>
<h2 id="10-2、Online-Learning"><a href="#10-2、Online-Learning" class="headerlink" title="10.2、Online Learning"></a>10.2、Online Learning</h2><p>上述方法可以应用在实时在线学习上，数据集大小不固定，以数据流的形式出现。</p>
<h2 id="10-3、Map-reduce-and-data-parallelism"><a href="#10-3、Map-reduce-and-data-parallelism" class="headerlink" title="10.3、Map-reduce and data parallelism"></a>10.3、Map-reduce and data parallelism</h2><p>在进行梯度下降计算时，中间有步骤需要求和，我们可以利用Map-reduce和并行计算来缩短处理时间。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/12/23/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周/" data-id="cixcyzm5x0008q8o3c30iskon" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/16/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周/" class="article-date">
  <time datetime="2016-12-16T02:56:39.000Z" itemprop="datePublished">2016-12-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/16/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第九周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="9、Anomaly-detection"><a href="#9、Anomaly-detection" class="headerlink" title="9、Anomaly detection"></a>9、Anomaly detection</h1><h2 id="9-1、Density-Estimation"><a href="#9-1、Density-Estimation" class="headerlink" title="9.1、Density Estimation"></a>9.1、Density Estimation</h2><h3 id="9-1-1、Problem-motivation"><a href="#9-1-1、Problem-motivation" class="headerlink" title="9.1.1、Problem motivation"></a>9.1.1、Problem motivation</h3><p>密度估计，判断一个test实例是否为不正常的。</p>
<p>Anomaly detection example：</p>
<ol>
<li><p>Fraud detection</p>
<pre><code>xi = features of user i&apos;s ativities.
Model p(x) from data.
Identify unusual users by checking which have p(x)&lt;ε
</code></pre></li>
<li><p>Manufacturing</p>
</li>
<li><p>Monitoring computers in a data center.</p>
<pre><code>xi = features of machine i.
memory use,number of disk access/sec,cpu load...
</code></pre></li>
</ol>
<h3 id="9-1-2、Gaussian-distribution"><a href="#9-1-2、Gaussian-distribution" class="headerlink" title="9.1.2、Gaussian distribution"></a>9.1.2、Gaussian distribution</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/9_1.png" alt="9_1"></p>
<h3 id="9-1-3、Anomaly-detection-algorithm"><a href="#9-1-3、Anomaly-detection-algorithm" class="headerlink" title="9.1.3、Anomaly detection algorithm"></a>9.1.3、Anomaly detection algorithm</h3><ol>
<li>Choose features xi that might be indicative of anomalous examples.</li>
<li><p>Fit parameters μ1, … μn,σ12,…σn2</p>
<pre><code>μj = 1/m ξxji
σj2 = 1/m ξ(xji-μj)2
</code></pre></li>
<li>Given new example x, compute p(x),Anomaly if p(x)&lt;ε.</li>
</ol>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_2.png" alt="9_2"></p>
<h3 id="9-1-4、Developing-and-evaluating-an-anomaly-detection-system"><a href="#9-1-4、Developing-and-evaluating-an-anomaly-detection-system" class="headerlink" title="9.1.4、Developing and evaluating an anomaly detection system"></a>9.1.4、Developing and evaluating an anomaly detection system</h3><ol>
<li>The importance of real-number evaluation</li>
</ol>
<p>例如 10000 good engines， 20 flawed engines，我们可以进行如下划分：</p>
<pre><code>Training set：6000 good engines
CV:2000 good engines,10 anomalous
Test:2000 good engines,10 anomalous
</code></pre><ol>
<li>Algorithm evaluation  </li>
</ol>
<p>可以利用F1-score来评估算法，我们也可以用CV来选择参数ε。</p>
<h3 id="9-1-5、Anomaly-detection-VS-supervised-learning"><a href="#9-1-5、Anomaly-detection-VS-supervised-learning" class="headerlink" title="9.1.5、Anomaly detection VS supervised learning"></a>9.1.5、Anomaly detection VS supervised learning</h3><p>Anomaly detection:</p>
<ol>
<li>Very small number of positive example.</li>
<li>Large number of negative example.</li>
<li>Many different types od anomalies. 很难通过positive实例来学习异常的特征</li>
<li>未来和异常和目前的异常实例不相关</li>
</ol>
<p>Supervised Learning：</p>
<ol>
<li>Large number of positive and negative examples.</li>
<li>可以根据大量的positive值推断出其特征值，未来的positive和现在的训练集非常相似</li>
</ol>
<h3 id="9-1-6、多元高斯分布"><a href="#9-1-6、多元高斯分布" class="headerlink" title="9.1.6、多元高斯分布"></a>9.1.6、多元高斯分布</h3><p>通过μ矩阵和ξ矩阵来对多远高斯分布进行调整。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_3.png" alt="9_3"></p>
<h2 id="9-2-推荐系统"><a href="#9-2-推荐系统" class="headerlink" title="9.2 推荐系统"></a>9.2 推荐系统</h2><h3 id="9-3-1-基于内容的推荐"><a href="#9-3-1-基于内容的推荐" class="headerlink" title="9.3.1 基于内容的推荐"></a>9.3.1 基于内容的推荐</h3><p>问题描述：</p>
<ol>
<li>r(i,j)=1 if user j has rated movie i</li>
<li>y(i,j)=rating by user j on movie i</li>
<li>θ(j)=paramater vector for user j</li>
<li>x(i)=feature vector for movie i</li>
<li>For user j,movie i, predicted rating θ(j)T(x(i))</li>
<li>m(j)=no. of movies rated by user j</li>
</ol>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_4.png" alt="9_4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_5.png" alt="9_5"></p>
<h3 id="9-3-2-正交过滤"><a href="#9-3-2-正交过滤" class="headerlink" title="9.3.2 正交过滤"></a>9.3.2 正交过滤</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/9_6.png" alt="9_6"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_7.png" alt="9_7"></p>
<h3 id="9-3-2-实现技巧"><a href="#9-3-2-实现技巧" class="headerlink" title="9.3.2 实现技巧"></a>9.3.2 实现技巧</h3><p>归一化，计算平均值，然后同时减去该值</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/12/16/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周/" data-id="cixcyzm5m0003q8o3vrbhynjk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/08/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周/" class="article-date">
  <time datetime="2016-12-08T11:59:23.000Z" itemprop="datePublished">2016-12-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/08/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第八周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="8、Unsupervised-Learning"><a href="#8、Unsupervised-Learning" class="headerlink" title="8、Unsupervised Learning"></a>8、Unsupervised Learning</h1><h2 id="8-1-Clustering"><a href="#8-1-Clustering" class="headerlink" title="8.1 Clustering"></a>8.1 Clustering</h2><h3 id="8-1-1-Unsupervised-Learning-Introduction"><a href="#8-1-1-Unsupervised-Learning-Introduction" class="headerlink" title="8.1.1 Unsupervised Learning: Introduction"></a>8.1.1 Unsupervised Learning: Introduction</h3><p>Applications of clustering:</p>
<ol>
<li>Market segementation</li>
<li>Social network analysis</li>
<li>Organize computing clusters</li>
<li>Astronomical data analysis</li>
</ol>
<p>无监督学习，数据是没有label的。</p>
<h3 id="8-1-2-K-Means-Algorithm"><a href="#8-1-2-K-Means-Algorithm" class="headerlink" title="8.1.2 K-Means Algorithm"></a>8.1.2 K-Means Algorithm</h3><p>Input:</p>
<pre><code>K (number of clusters)
Training set {x1,x2,...xm}
</code></pre><p>算法：</p>
<p>随机初始化K个聚类簇（cluster centroids）u1,u2…uK</p>
<p>然后repeat操作：</p>
<ol>
<li><p>cluster assignment step：</p>
<pre><code>for     i = 1 to m
  c(i):=index(from 1 to K) of cluster centroids closest to xi
</code></pre></li>
<li><p>Move centroid:</p>
<pre><code>for k = 1 to K
  uk := average(mean)of points assigned to cluster k
</code></pre></li>
</ol>
<h3 id="8-1-3-Optimization-objective"><a href="#8-1-3-Optimization-objective" class="headerlink" title="8.1.3 Optimization objective"></a>8.1.3 Optimization objective</h3><p>最小化distortion：<br><img src="http://ofacak8l3.bkt.clouddn.com/8_1.png" alt="8_1"></p>
<p>因此K-means算法也可以表示为：</p>
<ol>
<li>Randomly initialize K cluster centroids u1,u2,…uk</li>
<li><p>Repeat:</p>
<pre><code>for i = 1 to m
    c(i):= index(from 1 to K) of cluster centroid closest to xi
</code></pre></li>
</ol>
<pre><code>for k = 1 to K
    uk:= average(mean) of points assigned to cluster k [minize J]
</code></pre><h3 id="8-1-4-Random-initialization"><a href="#8-1-4-Random-initialization" class="headerlink" title="8.1.4 Random initialization"></a>8.1.4 Random initialization</h3><p>Random initialize:</p>
<p>首先满足 K &lt; m；</p>
<p>然后随机选择K个训练实例；</p>
<p>将u1,u2,…uk等于这K个实例；</p>
<pre><code>for i = 1 to 100{
    Randomly initialize K-means.
    Run K-means. Get c1,c2,...cm,u1,...uk.
    Compute cost function(distortion)
</code></pre><p>选择最小cost的聚类</p>
<h3 id="8-1-5-Choosing-the-number-of-clusters"><a href="#8-1-5-Choosing-the-number-of-clusters" class="headerlink" title="8.1.5 Choosing the number of clusters"></a>8.1.5 Choosing the number of clusters</h3><p>目前最好的方法还是手动选择；</p>
<p>Elbow cost：手肘方法，不是所有情况都能画出这样的图形，有时候会下降趋势较平滑。</p>
<p>sometimes, you’re running K-means to get clusters to use for some later/downstream purpose.</p>
<p>Evaluate K-means based on a metric for how well it performs for that later purpose.    </p>
<h2 id="8-2-Dimensionality-Reduction"><a href="#8-2-Dimensionality-Reduction" class="headerlink" title="8.2 Dimensionality Reduction"></a>8.2 Dimensionality Reduction</h2><p>介绍了第二种无监督学习算法：维数约减。</p>
<p>主要分为两大类应用：</p>
<ol>
<li>Data Compression</li>
<li>Data Visualization</li>
</ol>
<h3 id="8-2-1-Data-Compression"><a href="#8-2-1-Data-Compression" class="headerlink" title="8.2.1 Data Compression"></a>8.2.1 Data Compression</h3><p>将数据从2D 降维到 1D</p>
<p>将数据从3D 降维到 2D （将三维中的点投影到一个平面上）</p>
<h3 id="8-2-2-Data-Visualization"><a href="#8-2-2-Data-Visualization" class="headerlink" title="8.2.2 Data Visualization"></a>8.2.2 Data Visualization</h3><p>数据可视化，便于用户更好的观察数据，理解数据之间的含义。</p>
<p>将数据比较相关的进行降维，原来N维的数据降低到K维</p>
<p>k &lt;= N； 并且通常K=2或3， 便于进行可视化视图分析 </p>
<h2 id="8-3-Principal-Component-Analysis"><a href="#8-3-Principal-Component-Analysis" class="headerlink" title="8.3 Principal Component Analysis"></a>8.3 Principal Component Analysis</h2><h3 id="8-3-1-Principal-Component-Analysis-Problem-Formulation"><a href="#8-3-1-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="8.3.1 Principal Component Analysis Problem Formulation"></a>8.3.1 Principal Component Analysis Problem Formulation</h3><p>Reduce from 2-dimension to 1-dimension:</p>
<p>Find a direction (a vector v1) onto which to project the data so as to minimize the projection error.</p>
<p>Reduce from n-dimension to k-dimension:</p>
<p>Find k vectors v1,v2…vk on to which to project the data, so as to minimize the projection error.</p>
<p>PCA 不是线性回归：线性回归是竖着映射，PCA是垂直映射。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/8_2.png" alt="8_2"></p>
<h3 id="8-3-2-Principal-Component-Analysis-algorithm"><a href="#8-3-2-Principal-Component-Analysis-algorithm" class="headerlink" title="8.3.2 Principal Component Analysis algorithm"></a>8.3.2 Principal Component Analysis algorithm</h3><p><strong>Data Preprocessing:</strong></p>
<p>Training set: x1,x2,…xm</p>
<p>Preprocessing(feature scaling/mean normalization):</p>
<pre><code>uj = 1/m(x1+x2+...+xm)
xj&apos; = xj - uj
</code></pre><p>If different features on different scales, scale features to have comparable range of values.</p>
<p><strong>Principal Component Analysis (PCA) algorithm</strong></p>
<p>Reduce data from n-dimensions to k-dimensions</p>
<p>compute “covariance matrix”: sigma = 1/m(x1<em>x1T+…+xn</em>xnT)</p>
<p>compute “eigenvectors” of matrix sigma: [U,S,V] = svd(sigma)</p>
<p>Ureduce = U(:,1:k)</p>
<p>z = Ureduce’ * x</p>
<h3 id="8-4-Applying-PCA"><a href="#8-4-Applying-PCA" class="headerlink" title="8.4 Applying PCA"></a>8.4 Applying PCA</h3><h4 id="8-4-1-Reconstruction-from-Compressed-Representation"><a href="#8-4-1-Reconstruction-from-Compressed-Representation" class="headerlink" title="8.4.1 Reconstruction from Compressed Representation"></a>8.4.1 Reconstruction from Compressed Representation</h4><p>Xapprox = Ureduce*z</p>
<h4 id="8-4-2-Choosing-the-number-of-principal-components"><a href="#8-4-2-Choosing-the-number-of-principal-components" class="headerlink" title="8.4.2 Choosing the number of principal components"></a>8.4.2 Choosing the number of principal components</h4><p><strong>choosing k    </strong></p>
<p>(1)Average squared projection error: 1/m(||x1-xapprox1||2 +…)</p>
<p>(2)Total variation in the data : 1/m(||x1||2 + … +||xm||2)</p>
<p>Typically, choose k to be smallest value so that (1)/(2)&lt;=0.01</p>
<p>“99% of variance is retained”    </p>
<p><strong>Advice for applying PCA</strong></p>
<p>Supervised learning speedup</p>
<p>Compression:</p>
<ol>
<li>Reduce memory/disk needed to store data</li>
<li>speed up learning algorithm</li>
</ol>
<p>Visualization</p>
<p><strong>Bad use of PCA: To prevent overfitting</strong></p>
<p>fewer features, less likely to overfit.</p>
<p>This might work ok, but isn’t good way to address overfitting. Use regularization instead.</p>
<p>Before implementing PCA, first try running whatever you want to do with the original/raw data xi. Only if that doesn’t do what you want, then implement PCA and consider using zi.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/12/08/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周/" data-id="cixcyzm600009q8o32ot0wsox" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/04/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周/" class="article-date">
  <time datetime="2016-12-04T08:41:16.000Z" itemprop="datePublished">2016-12-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/04/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第七周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="7、Support-Vector-Machines"><a href="#7、Support-Vector-Machines" class="headerlink" title="7、Support Vector Machines"></a>7、Support Vector Machines</h2><h3 id="7-1-Optimization-objective"><a href="#7-1-Optimization-objective" class="headerlink" title="7.1 Optimization objective"></a>7.1 Optimization objective</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/7_1.png" alt="7_1"><br><img src="http://ofacak8l3.bkt.clouddn.com/7_2.png" alt="7_2"></p>
<h3 id="7-2-Large-Margin-Intuition"><a href="#7-2-Large-Margin-Intuition" class="headerlink" title="7.2 Large Margin Intuition"></a>7.2 Large Margin Intuition</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/7_3.png" alt="7_3"></p>
<p>C如果越大，两个分类的间距越小。</p>
<h3 id="7-3-The-Mathematics-behind-large-margin-classification"><a href="#7-3-The-Mathematics-behind-large-margin-classification" class="headerlink" title="7.3 The Mathematics behind large margin classification"></a>7.3 The Mathematics behind large margin classification</h3><p>先讲解了||u||符号的含义,可以利用向量的长度来计算矩阵的乘积.</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_4.png" alt="7_4"><br>SVM Decision Boundary 计算时可以利用这个特性。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_5.png" alt="7_5"></p>
<h3 id="7-4-Kernels"><a href="#7-4-Kernels" class="headerlink" title="7.4 Kernels"></a>7.4 Kernels</h3><h4 id="7-4-1-Kernels-I"><a href="#7-4-1-Kernels-I" class="headerlink" title="7.4.1 Kernels I"></a>7.4.1 Kernels I</h4><p>Gaussian Kernel.</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_6.png" alt="7_6"><br><img src="http://ofacak8l3.bkt.clouddn.com/7_7.png" alt="7_7"></p>
<h4 id="7-5-Kernels-II"><a href="#7-5-Kernels-II" class="headerlink" title="7.5 Kernels II"></a>7.5 Kernels II</h4><p>SVM parameters:<br>Large C: Lower bias, high variance</p>
<p>Small C: High bias, low variance</p>
<p>Large segma*segma: more smoothly. High bias, low variance.</p>
<p>Small segma*segma: less smoothly. Lower bias, higher variance.</p>
<h3 id="7-6-Using-an-SVM"><a href="#7-6-Using-an-SVM" class="headerlink" title="7.6 Using an SVM"></a>7.6 Using an SVM</h3><p>Use SVM software package(liblinear,libsvm…)to solve for parameters theta.</p>
<p><strong>Need to specify:</strong></p>
<pre><code>Choice of parameter C.
Choice of Kernel(similarity function):
    No Kernel(&apos;Linear Kernel&apos;)
    Gaussian Kernel.(need to choose segma)
</code></pre><p><strong>Note:</strong></p>
<pre><code>Do not perform feature scaling before using the Gaussian kernel.
</code></pre><p><strong>Other Choice of kernel:</strong></p>
<pre><code>Not all similarity functions similarity(x,l) make valid kernels. 
(Need to satisfy &apos;mercer&apos;s Theorem&apos; to make sure SVM package&apos;s optimications run correctly, and do not diverge)    
</code></pre><p><strong>Multi-class classification:</strong></p>
<p>Train K SVMs, 需要对K个类进行分类，而非k-1。</p>
<p><strong>Logistic regression vs SVM:</strong></p>
<p>n= number of features, m =number of training examples</p>
<pre><code>if n is large（相对于m）:
    using Logistic regression, or SVM without kernel.
if n is small, m is intermediate:
    use SVM with Gaussian Kernel.
if n is small, m is large:
    create/add more features,
    then use logistic regression or SVM without kernel.

Neural network likely to work well for most od these settings,
but many slower to train.
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/12/04/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周/" data-id="cixcyzm5h0001q8o3guc4ded1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/" class="article-date">
  <time datetime="2016-11-27T07:46:44.000Z" itemprop="datePublished">2016-11-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第六周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="6、Advice-for-Applying-Machine-Learning"><a href="#6、Advice-for-Applying-Machine-Learning" class="headerlink" title="6、Advice for Applying Machine Learning"></a>6、Advice for Applying Machine Learning</h2><h3 id="6-1、Evaluating-a-learning-Algorithm"><a href="#6-1、Evaluating-a-learning-Algorithm" class="headerlink" title="6.1、Evaluating a learning Algorithm"></a>6.1、Evaluating a learning Algorithm</h3><h4 id="6-1-1、Deciding-What-to-Try-Next"><a href="#6-1-1、Deciding-What-to-Try-Next" class="headerlink" title="6.1.1、Deciding What to Try Next"></a>6.1.1、Deciding What to Try Next</h4><p>Which of the following statements about diagnostics are true? (BCD)</p>
<p>A. It’s hard to tell what will work to improve a learning algorithm, so the best approach is to go with gut feeling and just see what works.</p>
<p>B. Diagnostics can give guidance as to what might be more fruitful things to try to improve a learning algorithm.</p>
<p>C. Diagnostics can be time-consuming to implement and try, but they can still be a very good use of your time.</p>
<p>D. A diagnostic can sometimes rule out certain courses of action(changes to your learning algorithm) as being unlikely to improve its performance signigicantly.</p>
<h4 id="6-1-2、Evaluating-a-Hypothesis"><a href="#6-1-2、Evaluating-a-Hypothesis" class="headerlink" title="6.1.2、Evaluating a Hypothesis"></a>6.1.2、Evaluating a Hypothesis</h4><p>如何评估假设函数，后面可以以此为基础讨论如何避免过拟合和欠拟合的问题。</p>
<p>Suppose an implementation of linear regression(without reguarization) is badly overfitting the training set. In this case, we would expect:</p>
<p>The training error to be low, and the test error to be high.</p>
<p>数据划分为训练集和测试集，一般比例为7:3,random select。如果数据不是随机的，最好自己随机排序或打乱顺序后再选取70%.</p>
<p>Misclassification error(0/1 misclassification error)</p>
<h4 id="6-1-3、Model-Selection-and-Training-Validation-Test-Sets"><a href="#6-1-3、Model-Selection-and-Training-Validation-Test-Sets" class="headerlink" title="6.1.3、Model Selection and Training/Validation/Test Sets"></a>6.1.3、Model Selection and Training/Validation/Test Sets</h4><p>如何确定对于某组数据，最合适的多项式次数是几次？如何选择学习算法中的正规化参数lamda？</p>
<p>模型选择问题。（泛化误差）</p>
<p>我们将数据集分为三部分，training set(60%), cross validation set(20%), test set(20%).</p>
<p>Training error</p>
<p>cross validation error</p>
<p>test error</p>
<p>我们使用交叉验证集来选择模型，看这些假设在交叉验证集表现如何。选择最小交叉验证集误差的模型。</p>
<p>Consider the model selection procedure where we choose the degree of polynomial using a cross validation set. For the final model, we might generally expect Jcv to be lower than Jtest because:</p>
<p>An extra parameter(d,the degree of the polynomial) has been fit to the cross validation set.</p>
<h3 id="6-2、Bias-vs-Variance-偏差和方差"><a href="#6-2、Bias-vs-Variance-偏差和方差" class="headerlink" title="6.2、Bias vs. Variance(偏差和方差)"></a>6.2、Bias vs. Variance(偏差和方差)</h3><h4 id="6-2-1、Diagnosing-Bias-vs-Variance"><a href="#6-2-1、Diagnosing-Bias-vs-Variance" class="headerlink" title="6.2.1、Diagnosing Bias vs. Variance"></a>6.2.1、Diagnosing Bias vs. Variance</h4><p>High Bias Overfitting</p>
<p>High Variance Underfitting</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/6_1.png" alt="6_1"></p>
<h4 id="6-2-2、Regularization-and-Bias-Variance"><a href="#6-2-2、Regularization-and-Bias-Variance" class="headerlink" title="6.2.2、Regularization and Bias/Variance"></a>6.2.2、Regularization and Bias/Variance</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_2.png" alt="6_2"></p>
<h4 id="6-2-3、Learning-Curves"><a href="#6-2-3、Learning-Curves" class="headerlink" title="6.2.3、Learning Curves"></a>6.2.3、Learning Curves</h4><p>判断一个假设是否存在偏差方差问题</p>
<pre><code>High Bias
If a learning algorithm is suffering from high bias, getting more training data will not help much.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_3.png" alt="6_3"><br>因此知道自己的假设是否存在高偏差/方差问题，非常有用。</p>
<pre><code>High variance
If a learning algorithm is suffering from high variance, getting more training data is likely to help.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_4.png" alt="6_4"></p>
<h4 id="6-2-4、Deciding-What-to-Do-Next-Revisited"><a href="#6-2-4、Deciding-What-to-Do-Next-Revisited" class="headerlink" title="6.2.4、Deciding What to Do Next Revisited"></a>6.2.4、Deciding What to Do Next Revisited</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_5.png" alt="6_5"></p>
<h3 id="6-3、Machine-Learning-system-design"><a href="#6-3、Machine-Learning-system-design" class="headerlink" title="6.3、Machine Learning system design"></a>6.3、Machine Learning system design</h3><h4 id="6-3-1、Priorityzing-what-to-work-on"><a href="#6-3-1、Priorityzing-what-to-work-on" class="headerlink" title="6.3.1、Priorityzing what to work on"></a>6.3.1、Priorityzing what to work on</h4><h4 id="6-3-2、Error-Analysis"><a href="#6-3-2、Error-Analysis" class="headerlink" title="6.3.2、Error Analysis"></a>6.3.2、Error Analysis</h4><p>建议的方法：</p>
<p>1、尽快的实现一个最基本的算法，然后在交叉验证数据集上进行验证；</p>
<p>2、画出learning curves，看是否更多的数据、更多的features能提升正确性；</p>
<p>3、Error Analysis：主要分析那些算法预测出错的例子，看能否发现一些系统趋势。</p>
<pre><code>Error analysis may not be helpful for deciding is this is likely to improve performance, only solution is to try it and say if it works.
</code></pre><h4 id="6-3-3-Error-Metrics-for-Skewed-Classes"><a href="#6-3-3-Error-Metrics-for-Skewed-Classes" class="headerlink" title="6.3.3 Error Metrics for Skewed Classes"></a>6.3.3 Error Metrics for Skewed Classes</h4><p>不能仅仅依靠错误率来判断算法的性能，还需要利用Precision 和 Recall<br><img src="http://ofacak8l3.bkt.clouddn.com/6_6.png" alt="6_6"></p>
<h4 id="6-3-4-Trading-off-precision-and-recall"><a href="#6-3-4-Trading-off-precision-and-recall" class="headerlink" title="6.3.4 Trading off precision and recall"></a>6.3.4 Trading off precision and recall</h4><p>我们可以通过计算F1来自动选择性能较高的算法。</p>
<pre><code>F1 = 2*(P*R)/(P+R);
</code></pre><h4 id="6-3-5-Data-for-Machine-Learning"><a href="#6-3-5-Data-for-Machine-Learning" class="headerlink" title="6.3.5 Data for Machine Learning"></a>6.3.5 Data for Machine Learning</h4><p>Banko anf Brill 在2001年提出了一个观点：</p>
<pre><code>It&apos;s not who has the best algorithm that wins, 
It&apos;s who has the most data.
</code></pre><p>这个观点有时候不一定正确：</p>
<p>当feature很少不足以预测时（例，预测房价时仅知道尺寸），此时再多的数据可能也无法提升算法的性能； </p>
<p>当算法参数很多时（例，逻辑回归和线性回归有很多的feature，或者神经网络有很多的隐藏单元），更大的测试集可以尽可能的减少过拟合。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/" data-id="cixcyzm65000dq8o3r4vjdibv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/" class="article-date">
  <time datetime="2016-11-24T07:10:34.000Z" itemprop="datePublished">2016-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第五周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="5、Neural-Networks：Learning"><a href="#5、Neural-Networks：Learning" class="headerlink" title="5、Neural Networks：Learning"></a>5、Neural Networks：Learning</h2><h3 id="5-1、Cost-Function-and-Backpropagation"><a href="#5-1、Cost-Function-and-Backpropagation" class="headerlink" title="5.1、Cost Function and Backpropagation"></a>5.1、Cost Function and Backpropagation</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_1.png" alt="5_1"></p>
<h3 id="5-2、Backprogation-algorithm"><a href="#5-2、Backprogation-algorithm" class="headerlink" title="5.2、Backprogation algorithm"></a>5.2、Backprogation algorithm</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_2.png" alt="5_2"></p>
<h3 id="5-3、Gradient-checking"><a href="#5-3、Gradient-checking" class="headerlink" title="5.3、Gradient checking"></a>5.3、Gradient checking</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_3.png" alt="5_3"></p>
<pre><code>Important:
Be sure to disable your gradient checking code before training your claaifier. otherwise code will be very slow.
</code></pre><h3 id="5-4-Random-initialization"><a href="#5-4-Random-initialization" class="headerlink" title="5.4 Random initialization"></a>5.4 Random initialization</h3><pre><code>initial each initialtheta to a random value in[-EPSILON,EPSILON]
</code></pre><h3 id="5-5-Put-it-together"><a href="#5-5-Put-it-together" class="headerlink" title="5.5 Put it together"></a>5.5 Put it together</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_4.png" alt="5_4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/5_5.png" alt="5_5"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/" data-id="cixcyzm64000aq8o3thvdvlcc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-毕设论文相关知识索引笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/24/毕设论文相关知识索引笔记/" class="article-date">
  <time datetime="2016-11-24T06:14:40.000Z" itemprop="datePublished">2016-11-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/24/毕设论文相关知识索引笔记/">毕设论文相关知识索引笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="压缩相关"><a href="#压缩相关" class="headerlink" title="压缩相关"></a>压缩相关</h2><p><a href="http://blog.csdn.net/zhangskd/article/details/17009111" target="_blank" rel="external">速度之王-lz4压缩算法</a></p>
<p><a href="http://blog.csdn.net/zhangskd/article/details/17009111" target="_blank" rel="external">使用tar+lz4+ssh更快的数据传输</a></p>
<p><a href="http://mogu.io/docker-162" target="_blank" rel="external">Docker源码分析之Docker daemon启动和销毁</a></p>
<p><a href="http://www.readern.com/sublime-text-latex-chinese-under-mac.html" target="_blank" rel="external">部署MAC上的Sublime Text + LaTex 中文环境</a></p>
<h2 id="Cgroup介绍、应用实例及原理描述"><a href="#Cgroup介绍、应用实例及原理描述" class="headerlink" title="Cgroup介绍、应用实例及原理描述"></a>Cgroup介绍、应用实例及原理描述</h2><p><a href="http://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html" target="_blank" rel="external">Cgroup介绍、应用实例及原理描述</a></p>
<p><a href="http://tech.meituan.com/cgroups.html" target="_blank" rel="external">Linux资源管理之cgroups简介</a></p>
<p><a href="https://www.linuxplumbersconf.org/2015/ocw//system/presentations/2619/original/2015-08-20-CRIU_Support_in_Docker_for_Native_Checkpoint_and_Restore.pdf" target="_blank" rel="external">CRIU support in Docker C/R</a></p>
<h3 id="Cgroup设计原理分析"><a href="#Cgroup设计原理分析" class="headerlink" title="Cgroup设计原理分析"></a>Cgroup设计原理分析</h3><p>Linux中，管理进程的数据结构是task_struct</p>
<pre><code>#ifdef CONFIG_CGROUPS
/*Control Group info protected by css_set_lock*/
struct css_set *cgroups;
/*cg_list protected by css_set_lock and tsk-&gt;alloc_lock*/
struct list_head cg_list;
#endif
</code></pre><p>其中*cgroups指向css_set结构，css_set存储了与进程相关的cgroups信息。cg_list是一个嵌入式的list_head结构，将链到同一个css_set的进程组织成一个链表。</p>
<pre><code>struct css_set{
  atomic_t refcount;
  struct hlist_node hlist;
  struct list_head tasks;
  struct list_head cg_links;
  struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT];
  struct rcu_head ruc_head;
};
</code></pre><p>其中，refcount是该css_set的引用数，因为一个css_set可以被多个进程公用，只要这些进程的cgroups信息相同。</p>
<p>hlist是嵌入的hlist_node，用于把所有css_set组成一个hash表，这样内核可以快速定位css_set。</p>
<p>tasks指向所有连到此css_set的进程指向的链表。</p>
<p>cg_links指向一个由struct_cg_group_link连城的链表。</p>
<p>subsys是一个指针数组，存储一组指向cgroup_subsys_state的指针，一个cgroup_subsys_state就是进程与一个特定子系统相关的信息。通过这个数组，进程可以获得相应的cgroup控制信息了。</p>
<pre><code>struct cgroup_subsys_state{
  struct cgroup *cgroup;
  atomic_t refcnt;
  unsigned long flags;
  struct css_id *id;
};
</code></pre><p>*cgroup指向一个cgroup结构，也就是进程属于的cgroup。</p>
<pre><code>task_struct--&gt;css_set--&gt;cgroup_subsys_state--&gt;cgroup
</code></pre><p>这样进程就和cgroup连接起来了。</p>
<pre><code>struct cgroup{
  unsigned long flags;
  atomic_t count;
  struct list_head sibling;
  struct list_head children;
  struct cgroup *parent;
  struct dentry *dentry;
  struct cgroup_subsys_state *subsys[CGROUP_SUBSYS_COUNT]
  struct cgroupfs_root *root;
  struct cgroup *top_cgroup;
  struct list_head css_sets;
  struct list_head release_list;
  struct list_head pidlists;
  struct mutex pidlist_mutex;
  struct rcu_head rcu_head;
  struct list_head event_list;
  spinlock_t event_list_lock;
};
</code></pre><p>sibling,children,parent三个嵌入的list_head负责将同一层级的cgroup连接成一颗cgroup树。</p>
<p>subsys指针数组，存储一组指向cgroup_subsys_state的指针。这组指针指向了此cgroup跟各个子系统相关的信息。类似css_set中。</p>
<p>root指向一个cgroupfs_root结构，就是cgroup所在层级对应的结构体。</p>
<p>top_cgroup指向了所在层级的根cgroup，即创建层级时自动创建的那个cgroup。</p>
<p>css_set指向一个由struct_cg_cgroup_link连成的链表。</p>
<pre><code>struct cg_cgroup_link{
  struct list_head cgrp_link_list;
  struct cgroup *cgrp;
  struct list_head cg_link_list;
  struct css_set *cg;
};
</code></pre><p>cgrp_link_list连入到 cgroup-&gt;css_set链表，cgrp指向此cg_group_link相关的cgroup。</p>
<p>cg_link_list连入到css_set-&gt;cg_links指向的链表，cg指向此cg_group_link相关的css_set。</p>
<p>cgroup和css_set是一个多对多的关系，必须添加一个中间结构来将两者联系起来，这既是cg_cgroup_link的作用。cgrp和cg为此结构体的联合主键，cgrp_link_list和cg_link_list分别连入cgroup和css_set，使后两者都可以尽心遍历查询。</p>
<p>####为什么cgroup和css_set是多对多的关系？<br>一个进程对应一个css_set，一个css_set存储了一组进程跟各个子系统相关的信息，但是这些信息由可能不是从一个cgroup那里获得的，因为一个进程可以同时属于多个cgroup，只要这些cgroup不在同一个层级。</p>
<pre><code>eg：
我们创建一个层级A，A上面附加了CPU和memory两个子系统，进程a属于A的根cgroup；创建一个层级B，B上面附加了ns和blkio两个子系统，进程a也属于B的根cgroup；那么进程a对应的CPU和memory是从A的根cgroup获得的，ns和blkio则是从B的根cgroup获得。因此一个css_set存储的cgroup_subsys_state可以对应多个cgroup。另一方面cgroup也存储了一组cgroup_subsys_state，这一组则是cgroup从所在的层级附加的子系统获得的。一个cgroup可以有多个进程，这些进程的css_set不一定都相同，因为有些进程可能还加入了其他cgroup，但是同一个cgroup中的进程与该cgroup关联的cgroup_subsys_state都受到该cgroup的管理，所以一个cgroup也可以对应多个css_set。
</code></pre><p>从前面的分析，我们可以看出从 task 到 cgroup 是很容易定位的，但是从 cgroup 获取此 cgroup 的所有的 task 就必须通过这个结构了。每个进程都回指向一个 css_set，而与这个 css_set 关联的所有进程都会链入到 css_set-&gt;tasks 链表，而 cgroup 又通过一个中间结构 cg_cgroup_link 来寻找所有与之关联的所有 css_set，从而可以得到与 cgroup 关联的所有进程。<br><img src="http://ofacak8l3.bkt.clouddn.com/cgroups_1.png" alt="cgroups_1"></p>
<p>上面这个图从整体结构上描述了进程与 cgroups 之间的关系。最下面的P代表一个进程。每一个进程的描述符中有一个指针指向了一个辅助数据结构css_set（cgroups subsystem set）。 指向某一个css_set的进程会被加入到当前css_set的进程链表中。一个进程只能隶属于一个css_set，一个css_set可以包含多个进程，隶属于同一css_set的进程受到同一个css_set所关联的资源限制。</p>
<p>上图中的”M×N Linkage”说明的是css_set通过辅助数据结构可以与 cgroups 节点进行多对多的关联。但是 cgroups 的实现不允许css_set同时关联同一个cgroups层级结构下多个节点。 这是因为 cgroups 对同一种资源不允许有多个限制配置。</p>
<p>一个css_set关联多个 cgroups 层级结构的节点时，表明需要对当前css_set下的进程进行多种资源的控制。而一个 cgroups 节点关联多个css_set时，表明多个css_set下的进程列表受到同一份资源的相同限制。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/24/毕设论文相关知识索引笔记/" data-id="cixcyzm6o000pq8o3prnbmzhg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/" class="article-date">
  <time datetime="2016-11-13T07:48:54.000Z" itemprop="datePublished">2016-11-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第四周</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="4、Neural-Networks"><a href="#4、Neural-Networks" class="headerlink" title="4、Neural Networks"></a>4、Neural Networks</h2><h3 id="4-1-Motivations"><a href="#4-1-Motivations" class="headerlink" title="4.1 Motivations"></a>4.1 Motivations</h3><pre><code>Neural Networks
Origins: Algorithms that try to mimic the brain.
Was very widely used in 80s and early 90s;
popularity diminished in late 90s.
Recent resurgence: 
    state-of-the-art technique for many applications. 
</code></pre><h3 id="4-2-Neural-Networks"><a href="#4-2-Neural-Networks" class="headerlink" title="4.2 Neural Networks"></a>4.2 Neural Networks</h3><h4 id="4-2-1-Model-Representation-I"><a href="#4-2-1-Model-Representation-I" class="headerlink" title="4.2.1 Model Representation I"></a>4.2.1 Model Representation I</h4><p>Sigmoid(logistic) activation function.<br><img src="http://ofacak8l3.bkt.clouddn.com/4_1.png" alt="4_1"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/4_2.png" alt="4_2"></p>
<h4 id="4-2-2-Model-Representation-II"><a href="#4-2-2-Model-Representation-II" class="headerlink" title="4.2.2 Model Representation II"></a>4.2.2 Model Representation II</h4><pre><code>input层
hidden层
output层
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/4_3.png" alt="4_3"></p>
<h3 id="4-3-Multiple-output-units-One-vs-all"><a href="#4-3-Multiple-output-units-One-vs-all" class="headerlink" title="4.3 Multiple output units:One-vs-all."></a>4.3 Multiple output units:One-vs-all.</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/4_4.png" alt="4_4"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/13/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第四周/" data-id="cixcyzm67000eq8o3uxk5vssa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-learning-AI-course/">Machine learning, AI, course</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Machine-Learning编程作业2——Logistic-Regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/" class="article-date">
  <time datetime="2016-11-07T02:31:30.000Z" itemprop="datePublished">2016-11-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/">Machine Learning编程作业2——Logistic Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>##ex2.m<br>=============plotting================</p>
<p>data = load(‘ex2data1.txt’);</p>
<p>X = data(:,[1,2]);</p>
<p>y = data(:,3);</p>
<p>plotData(X,y);</p>
<pre><code>plotData.m

function plotData(X,y)
figure;hold on;

pos = find(y==1);
neg = find(y==0);
plot(X(pos,1),X(pos,2),&apos;k+&apos;,&apos;LineWidth&apos;,2,...
    &apos;MarkerSize&apos;,7);
plot(X(neg,1),X(neg,2),&apos;ko&apos;,&apos;MarkerFaceColor&apos;,&apos;y&apos;,...
    &apos;MarkerSize&apos;,7);

hold off;
end
</code></pre><p>=============compute cost and gradient===========</p>
<p>[m,n] = size(X);</p>
<p>X = [ones(m,1) X];</p>
<p>initial_theta = zeros(n+1,1);</p>
<p>[cost,grad] = costFunction(initial_theta,X,y);</p>
<pre><code>costFunction.m

function [J,grad] = costFunction(theta,X,y)
m = length(y);
J = 0;
grad = zeros(size(theta));

J = (-1)/m *(log(sigmoid(X*theta))&apos;*y +
            log(1-sigmoid(X*theta))&apos;*(1-y));
for i = 1: size(X,2)
  grad(i) = 1/m * sum((sigmoid(X*theta)-y) .* X(:,i));
end
end
</code></pre><p>plotDecisionBoundary(theta,X,y);</p>
<p>=============predict and accuracies===============</p>
<p>prob = sigmoid([1 45 85] * theta);</p>
<p>p = predict(theta,X);</p>
<pre><code>predict.m

function p = predict(theta,X)

m = size(X,1);
p = zeros(m,1);

for i = 1:m
    if(sigmoid(X(i,:) * theta)) &gt;= 0.5
        p(i) = 1;
    else
        p(i) = 0;
    end
end
end
</code></pre><h2 id="ex2-reg-m"><a href="#ex2-reg-m" class="headerlink" title="ex2_reg.m"></a>ex2_reg.m</h2><p>clear;</p>
<p>data = load(‘ex2data2.txt’);</p>
<p>X = data(:,[1,2]);</p>
<p>y = data(:,3);</p>
<p>plotData(X,y);</p>
<p>=====================regularized Logistic Regression======</p>
<p>X = mapFeature(X(:,1),X(:,2));</p>
<p>initial_theta = zeros(size(X,2),1);</p>
<p>lambda = 1;</p>
<p>[cost,grad] = costFunctionReg(initial_theta,X,y,lambda);</p>
<pre><code>costFunctionReg.m

function [J,grad] = costFunctionReg(theta,X,y,lambda)
m = length(y);
J = 0;
grad = zeros(size(theta));

temp = theta(2:size(theta,1),:) .^2;
value = sum(temp);
J = (-1)/m * (log(sigmoid(X*theta))&apos;*y +
            log(1-sigmoid(X*theta))&apos;*(1-y))
        +lambda/(2*m) * value;
grad(1) = 1/m*sum((sigmoid(X*theta) - y) .* X(:,1));

for i = 2: size(X,2)
    grad(i) = 1 / m *sum((sigmoid(X*theta) -y).*X(:,i))
     +lambda/m * theta(i);
end
end
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://xiaozhazi.win/2016/11/07/Machine-Learning编程作业2——Logistic-Regression/" data-id="cixcyzm6i000lq8o3t5bse7ye" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/octave-program/">octave, program</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Machine-Learning-coursera/">AI, Machine Learning, coursera</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-learning-AI-course/">Machine learning, AI, course</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/course-AI-machine-Learning/">course, AI, machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/octave-program/">octave, program</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/α-β-δ-ε-η-θ-ξ-μ-λ-ν-ξ-ο-π-ρ-σ-τ-υ-φ-χ-ψ-ω/">α β δ ε η θ ξ μ λ ν ξ ο π ρ σ τ υ φ χ ψ ω</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI-Machine-Learning-coursera/" style="font-size: 20px;">AI, Machine Learning, coursera</a> <a href="/tags/Machine-learning-AI-course/" style="font-size: 10px;">Machine learning, AI, course</a> <a href="/tags/course-AI-machine-Learning/" style="font-size: 10px;">course, AI, machine Learning</a> <a href="/tags/octave-program/" style="font-size: 20px;">octave, program</a> <a href="/tags/α-β-δ-ε-η-θ-ξ-μ-λ-ν-ξ-ο-π-ρ-σ-τ-υ-φ-χ-ψ-ω/" style="font-size: 10px;">α β δ ε η θ ξ μ λ ν ξ ο π ρ σ τ υ φ χ ψ ω</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">九月 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/12/31/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十一周</a>
          </li>
        
          <li>
            <a href="/2016/12/23/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十周</a>
          </li>
        
          <li>
            <a href="/2016/12/16/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第九周</a>
          </li>
        
          <li>
            <a href="/2016/12/08/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第八周</a>
          </li>
        
          <li>
            <a href="/2016/12/04/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周/">AI入门之——Andrew Ng “Machine Learning”课程学习笔记第七周</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Frances Hu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>