<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Frances Hu's Blog">
<meta property="og:url" content="http://xiaozhazi.github.io/index.html">
<meta property="og:site_name" content="Frances Hu's Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Frances Hu's Blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://xiaozhazi.github.io/"/>

  <title> Frances Hu's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Frances Hu's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Born to be wild!</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/08/15/设计模式之美第二周/" itemprop="url">
                  设计模式之美第二周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-08-15T23:24:00+08:00" content="2020-08-15">
              2020-08-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="理论二：封装、抽象、继承、多态分别可以解决哪些编程问题？"><a href="#理论二：封装、抽象、继承、多态分别可以解决哪些编程问题？" class="headerlink" title="理论二：封装、抽象、继承、多态分别可以解决哪些编程问题？"></a>理论二：封装、抽象、继承、多态分别可以解决哪些编程问题？</h2><p>本文主要针对四大特性，结合实际代码，帮助我们了解每个特性存在的意义和目的，以及它们能解决哪些编程问题。</p>
<h3 id="封装-Encapsulation"><a href="#封装-Encapsulation" class="headerlink" title="封装 Encapsulation"></a>封装 Encapsulation</h3><p>封装也叫作信息藏匿或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式来访问内部信息或数据。</p>
<p>对于封装这个特性，我们需要编程语言本身提供一定的语法机制来支持。这个语法机制就是访问权限控制。</p>
<p>如果对类中属性访问不做限制，那任何代码都可以访问、修改类中的属性，虽然这样看起来更灵活，但是从另一方面来说过度灵活意味着不可控。除此之外，类仅仅通过有限的方法暴露必要的操作，也提高类的易用性。调用者不需要了解太多背后的业务细节，用错的概率也会减少。</p>
<h3 id="抽象-Abstraction"><a href="#抽象-Abstraction" class="headerlink" title="抽象 Abstraction"></a>抽象 Abstraction</h3><p>封装主要讲的是如何隐藏信息、保护数据，而抽象讲的是如何隐藏方法的具体实现，让调用者之需要关心方法提供了哪些功能，并不需要知道这些功能是如何实现的。</p>
<p>在面向对象编程中，我们常借助编程语言提供的接口类（Interface）或者抽象类（Abstract）这两种语法机制，来实现抽象。</p>
<p>抽象的意义，首先作为一种只关注功能不关注实现的设计思路，正好帮我们的大脑过滤掉许多非必要的信息。其次，抽象在代码设计中起到非常重要的指导作用。很多设计原则都体现了抽象这种设计思想，比如基于接口而非实现编程、开闭原则、代码解耦等。</p>
<p>我们在定义类的方法时，也要有抽象思维，不要在方法定义中暴露太多的实现细节，以保证在某个时间点需要修改方法的实现逻辑时不用去修改其定义。</p>
<h3 id="继承-Inheritance"><a href="#继承-Inheritance" class="headerlink" title="继承 Inheritance"></a>继承 Inheritance</h3><p>为了实现继承特性，编程语言需要提供特殊的语法机制来支持，如Java中使用extends来实现继承，C++使用冒号，Python使用paraentheses（）等。不过有些语言只支持单继承，如Java、PHP、C#、Ruby等，有些支持多重继承，如C++、python、Perl等。</p>
<p>继承存在的最大好处就是代码复用。不过过度使用继承层次过深过复杂，会导致代码可读性、可维护性变差。所以继承应该尽量少用，甚至不用。（多用组合少用继承）</p>
<h3 id="多态-Polymorphism"><a href="#多态-Polymorphism" class="headerlink" title="多态 Polymorphism"></a>多态 Polymorphism</h3><p>多态能提高代码的可扩展性和复用性。除此之外多态也是很多设计模式、设计原则、编程技巧的代码实现基础。比如策略模式、基于接口而非实现编程、依赖倒置原则、里氏替换原则、利用多态去掉冗长的if-else语句等。</p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>Java不支持继承的原因是多重继承存在副作用：钻石问题（菱形继承）</p>
<p>假设B和C都继承A，且都重写了A中同一方法，类D继承类B和类C，对于B、C重写的A中的方法，类D会继承哪一个会产生歧义。但是Java支持多接口实现，因为接口中的方法是抽象的，在实现接口时需要实现类自己实现，所以不会出现二义性问题。</p>
<h2 id="理论三：面向对象比面向过程有哪些优势？面向过程过时了吗？"><a href="#理论三：面向对象比面向过程有哪些优势？面向过程过时了吗？" class="headerlink" title="理论三：面向对象比面向过程有哪些优势？面向过程过时了吗？"></a>理论三：面向对象比面向过程有哪些优势？面向过程过时了吗？</h2><h3 id="什么是面向过程编程与面向过程编程语言？"><a href="#什么是面向过程编程与面向过程编程语言？" class="headerlink" title="什么是面向过程编程与面向过程编程语言？"></a>什么是面向过程编程与面向过程编程语言？</h3><p>面向过程编程也是一种编程范式/风格，它以过程作为组织代码的基本单元。以数据与方法相分离为最主要的特点。面向过程风格是一种流程化的编程风格，通过拼接一组顺序执行的方法来操作数据完成一项功能。<br>面向过程编程语言首先是一种编程语言。它最大的特点是不支持类和对象两个语法概念，不支持丰富的面向对象编程特性。</p>
<h3 id="面向对象编程相比面向过程编程有哪些优势？"><a href="#面向对象编程相比面向过程编程有哪些优势？" class="headerlink" title="面向对象编程相比面向过程编程有哪些优势？"></a>面向对象编程相比面向过程编程有哪些优势？</h3><h4 id="OOP更加能够应对大规模复杂程序的开发"><a href="#OOP更加能够应对大规模复杂程序的开发" class="headerlink" title="OOP更加能够应对大规模复杂程序的开发"></a>OOP更加能够应对大规模复杂程序的开发</h4><p>对于大规模复杂程序开发来说，整个程序的处理流程错综复杂，并非只有一条主线。如果用面向过程编程这种流程化、线性的思维方式，去翻译这个网状结构，去思考如何把程序拆解为一组顺序执行的方法会比较吃力。</p>
<p>面向对象编程以类为思考对象。在进行面向对象编程的时候，我们并不是一上来就去思考如何讲复杂的流程拆解为一个一个方法，而是采用曲线救国的策略。先去思考如何给业务建模，如何将需求翻译为类，如何给类之间建立交互关系，完成这些工作完全不需要考虑错综复杂的处理流程。</p>
<p>除此之外，面向对象编程还提供了一种更加清晰、更加模块化的代码组织方式。</p>
<p>实际上利用面向过程的编程语言那样，也可以写出面向对象风格的代码。只不过可能会比用面向对象编程语言付出的代价更高一些。两种编程风格并不是完全对立的。</p>
<h4 id="OOP代码更易复用、易扩展、易维护"><a href="#OOP代码更易复用、易扩展、易维护" class="headerlink" title="OOP代码更易复用、易扩展、易维护"></a>OOP代码更易复用、易扩展、易维护</h4><p>封装特性是两种编程风格最基本的区别，面向对象将数据和方法绑定在一起，通过访问权限控制，只允许外部调用者通过类暴露的有限方法访问数据，而不会像面向过程那样，数据可以被任意方法修改。因此面向对象提供的封装特性更有利于提高代码的易维护性。</p>
<p>函数本身就是一种抽象，它隐藏了具体实现。我们在使用函数时之需要了解函数具有什么功能，而不需要了解它是怎么实现的。从这一点上两种编程风格都支持抽象特性。只是面向对象还提供了其他抽象特性的实现方式，如基于接口实现的抽象。基于接口的抽象可以让我们在不改变原有实现的情况下，轻松替换新的实现逻辑，提高了代码的可扩展性。</p>
<p>继承特性是面向对象特有的，能避免代码重复写很多遍，提高了代码的复用性。</p>
<p>多态特性也是面向对象特有，在需要修改一个功能实现的时候，可以通过实现一个新的子类的方式，在子类重写原来的逻辑功能。用子类替换父类遵从了“对修改关闭，对扩展开放”的原则，提高代码的扩展性。除此之外，多态特性使得不同类对象可以传递相同的方法，执行不同的逻辑，提高代码复用性。</p>
<h4 id="OOP语言更加人性化、更高级智能"><a href="#OOP语言更加人性化、更高级智能" class="headerlink" title="OOP语言更加人性化、更高级智能"></a>OOP语言更加人性化、更高级智能</h4><p>面向对象时，开发者是在思考如何给业务建模、如何将真实世界映射为类或者对象，能更聚焦到业务本身，而不是思考如何和机器打交道。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ul>
<li>Unix/Linux这些复杂系统，也是基于C语言这种面向过程的编程语言开发的，怎么看待这种现象？<ul>
<li>操作系统是业务无关的，它更接近于底层计算机，因此更适合用面向过程的语言编写。并且和硬件打交道需要考虑到语言本身翻译成机器语言的成本和执行效率。</li>
<li>不过操作系统虽然是面向过程的C语言实现，但是其设计逻辑是面向对象的。它用结构体同样实现了信息的封装，内核源码中也不乏继承和多态思想的体现。面向对象思想并不局限于具体语言。</li>
</ul>
</li>
</ul>
<h2 id="理论四：-哪些代码设计看似面向对象，实际是面向过程的？"><a href="#理论四：-哪些代码设计看似面向对象，实际是面向过程的？" class="headerlink" title="理论四： 哪些代码设计看似面向对象，实际是面向过程的？"></a>理论四： 哪些代码设计看似面向对象，实际是面向过程的？</h2><h3 id="滥用getter、setter方法"><a href="#滥用getter、setter方法" class="headerlink" title="滥用getter、setter方法"></a>滥用getter、setter方法</h3><p>在项目开发中，有时定义完类的属性之后，就顺手将属性的getter、setter方法都定义上。IDE或者Lombok插件会自动生成所有属性的getter、setter方法。</p>
<p>这种方法是不推荐的，因为其违反了面向对象的封装特性，相当于将面向对象编程风格退化成了面向过程编程风格。 例如下面这段代码</p>
<pre><code>public class ShoppingCart {
    private int itemsCount;
    private double totalPrice;
    private List&lt;ShoppingCartItem&gt; items = new ArrayList&lt;&gt;();

    public int getItemsCount() {
        return this.itemsCount;
    }

    public void setItemCount(int itemsCount) {
        this.itemsCount = itemsCount;
    }

    public double getTotalPrice() {
        return this.totalPrice;
    }

    public void setTotalPrice(double totalPrice) {
        this.totalPrice = totalPrice;
    }

    public List&lt;ShoppingCartItem&gt; getItems() {
        return this.items;
    }

    public void addItems(ShoppingCartItem item) {
        items.add(item);
        itemCount++;
        totalPrice += item.getPrice();
    }
    ...
}
</code></pre><p>在这个代码中虽然我们将itemsCount和totalPrice定义为private，但是外部可以通过setter方法随意修改这两个属性的值。可能会导致和items属性的值不一致。暴露不该暴露的setter方法明显违反了面向对象的封装特性。数据没有任何访问权限，任何代码都可以随意修改，代码就退化成了面向过程编程风格了。</p>
<p>对于items我们没有设置setter方法，这样的设计看起来没有任何问题，而实际上并不是。items属性的getter方法返回的是一个List容器。外部调用者在拿到这个容器后，是可以操作容器内部数据的。比如obj.getItems().clear()会清空购物车，这样也会导致类属性中三个数据不一致。</p>
<p>正确的方法是应该专门在类中提供clear方法，并且修改getItems返回类型为Collections.undermidifiableList()。此时外部调用要修改就会抛出UnsupportedOperationException异常，避免容器中的数据被修改。（这里还存在一个问题，虽然items容器中数据不会被修改，但是容器中每个对象ShoppingCartItem的数据仍然可以修改）</p>
<h3 id="滥用全局变量和全局方法"><a href="#滥用全局变量和全局方法" class="headerlink" title="滥用全局变量和全局方法"></a>滥用全局变量和全局方法</h3><p>面向对象编程中，常见的全局变量有单例类对象、静态成员变量、常量等，常见的全局方法有静态方法。</p>
<ul>
<li>单例类对象在全局代码中只有一份，相当于全局变量</li>
<li>静态成员变量归属类上的数据，被所有实例化对象共享，也相当于一定程度上的全局变量</li>
<li>常量是非常常见的全局变量，放到一个Constant类中</li>
<li>静态方法一般用来操作静态变量或者外部数据。如各种Utils类，里面的方法一般都会定义成静态方法。静态方法将方法和数据分离，破坏了封装特性，是典型的面向过程风格。</li>
</ul>
<p>如：</p>
<pre><code>public class Constants {
    public static final String MYSQL_ADDR_KEY = &quot;mysql_addr&quot;;
    ...
}
</code></pre><p>我们会把程序中所有用到的常量都集中放到这个Constants类中，这并不是一个很好的设计思路。</p>
<ul>
<li>首先会影响代码的可维护性。开发同一项目的工程师很多，在开发过程中可能都要涉及修改这个类，查找修改可能比较费时，并且会增加提交代码冲突的概率。</li>
<li>其次，这样的设计会增加代码的编译时间。 依赖这个类的代码很多，每次修改Constants类都会导致依赖它的类重新编译。</li>
<li>最后，这样设计会影响代码的复用性。 如果我们在另一个项目中复用本项目的一个类，该类又依赖Constants类，即使只依赖其中的一部分我们仍然需要将整个Constants类也一起并入。引入许多无关的常量到新项目中。</li>
</ul>
<p>如何改进呢？</p>
<ul>
<li>将Constants类拆解为功能更加单一的多个类</li>
<li>另一种更好的思路是，并不单独地设计Constants常量类，而是哪个类用到了某个常量，就把这个常量定义到这个类中，提高了类设计的内聚性和代码的复用性</li>
</ul>
<p>对于Utils类，它的出现主要是解决了多个类需要用到一块相同的功能逻辑，为了避免代码重复。通常为了复用会通过继承特性，将相同的属性和方法提取出来，定义到父类中，子类复用父类中的属性和方法。但是有时候从业务含义上，这些类并不一定具有继承关系，仅仅为了代码复用生硬地抽象一个父类，会影响代码可读性。所以只包含静态方法Utils类就出现了，它实现了公用的方法但是不需要共享任何数据，因此不需要定义任何属性。同时也要注意不要实现大而全的Utils类，最好细化一下。</p>
<h3 id="定义数据和方法分离的类"><a href="#定义数据和方法分离的类" class="headerlink" title="定义数据和方法分离的类"></a>定义数据和方法分离的类</h3><p>传统的MVC分为Model层、Controller层、View层，在做前后端分离之后，三层结构在后端开发时会稍微有些调整，被分为Controller层、Service层、Repository层。Controller层负责暴露接口给前端调用，Service层负责核心业务逻辑，Repository层负责数据读写。在每一层中我们又会定义相应的VO（ViewObject）、BO（BusinessObject）、Entity。一般情况下VO、BO、Entity只会定义数据，不会定义方法，所有操作这些数据的业务逻辑都定义在对应的Controller类、Service类、Repository类中。这就是典型的面向过程的编程风格。</p>
<p>实际上这种开发模式叫做基于贫血模型的开发模式，也就是我们现在非常常用的一种Web项目开发模式。</p>
<h3 id="面向对象编程中，为什么容易写出面向过程风格的代码？"><a href="#面向对象编程中，为什么容易写出面向过程风格的代码？" class="headerlink" title="面向对象编程中，为什么容易写出面向过程风格的代码？"></a>面向对象编程中，为什么容易写出面向过程风格的代码？</h3><p>主要是面向过程符合人的流程化思维方式。面向对象则是一种自底向上的思考方式，不是先去思考执行流程来分解任务，而是将任务翻译成一个一个的小模块，设计类之间的交互，最后按照流程将类组装起来完成整个任务。这种思考路径比较适合复杂程序开发，不是特别符合人类的思考习惯。</p>
<p>除此之外，面向对象中类的设计挺需要技巧，需要一定设计经验，要去思考如何封装合适的数据和方法到一个类里，如何设计类之间的关系，类之间的交互等诸多问题。</p>
<p>不管使用面向过程还是面向对象，最终目的都是写出易维护、易读、易复用、易扩展的高质量代码。只要我们能避免面向过程编程风格的一些弊端，控制好它的副作用，在掌控范围内为我们所用，就大可不用避讳在面向对象编程中写面向过程风格的代码。</p>
<h2 id="理论五：-接口VS抽象类的区别？如何用普通的类模拟抽象类和接口？"><a href="#理论五：-接口VS抽象类的区别？如何用普通的类模拟抽象类和接口？" class="headerlink" title="理论五： 接口VS抽象类的区别？如何用普通的类模拟抽象类和接口？"></a>理论五： 接口VS抽象类的区别？如何用普通的类模拟抽象类和接口？</h2><h3 id="什么是抽象类和接口？区别在哪里？"><a href="#什么是抽象类和接口？区别在哪里？" class="headerlink" title="什么是抽象类和接口？区别在哪里？"></a>什么是抽象类和接口？区别在哪里？</h3><p>抽象类</p>
<ul>
<li>不允许被实例化，只能被继承。</li>
<li>抽象类可以包含属性和方法。方法既可以包含代码实现，也可以不包含代码实现。不包含代码实现的方法叫做抽象方法。</li>
<li>子类继承抽象类，必须实现抽象类中的所有抽象方法。</li>
</ul>
<p>接口</p>
<ul>
<li>接口不能包含属性（即成员变量）</li>
<li>接口只能声明方法，方法不能包含代码实现</li>
<li>类实现接口的时候，必须实现接口中声明的所有方法</li>
</ul>
<p>抽象类实际上就是类，只不过是一种不能被实例化的特殊类，只能被子类继承，is-a的关系。接口表示has-a的关系，表示具有某些功能。对于接口，有一个更加形象的叫法，就是协议。</p>
<h3 id="抽象类和接口能解决什么编程问题？"><a href="#抽象类和接口能解决什么编程问题？" class="headerlink" title="抽象类和接口能解决什么编程问题？"></a>抽象类和接口能解决什么编程问题？</h3><p>抽象类是为代码复用而生的，多个子类可以继承抽象类中定义的属性和方法，避免在子类中重复编写相同的代码。普通的类继承虽然也可以解决代码复用问题，但是无法使用多态特性，会增加类被无用的风险。虽然也可以通过设置私有的构造函数的方式来解决，不过显然没有抽象类优雅。</p>
<p>接口更侧重于解耦，是对行为的一种抽象，相当于一组协议或者契约，可以类比API接口。调用者只需要关注抽象的接口，不需要了解具体的实现，具体的实现代码对调用者透明。接口实现了约定和实现相分离，可以降低代码间的耦合性，提高代码的可扩展性。</p>
<h3 id="如何模拟抽象类和接口两个语法概念？"><a href="#如何模拟抽象类和接口两个语法概念？" class="headerlink" title="如何模拟抽象类和接口两个语法概念？"></a>如何模拟抽象类和接口两个语法概念？</h3><p>我们可以通过抽象类来模拟接口。首先接口的定义：接口中没有成员变量，只有方法声明，没有方法实现，实现接口的类必须实现接口中的所有方法。</p>
<pre><code>class Strategy {
    public:
        ~Strategy();
        virtual void algorithm() = 0;
    protected:
        Strategy();
}
</code></pre><p>上述C++代码中用抽象类模拟了一个接口，类中没有定义任何属性，并且所有方法都是virtual类型。</p>
<p>除了用抽象类来模拟接口，我们还可以用普通类来模拟接口。类中虽然包含方法不符合接口定义，但是我们可以让类中的方法抛出异常来模拟不包含实现的接口，并且能强迫子类在继承这个父类的时候都去主动实现父类的方法，否则就会在运行时抛出异常。为了避免该类被实例化，我们将类的构造函数声明为protected方法就可以了。</p>
<pre><code>public class MockInterface {
    protected MockInterface() {}
    public void funcA() {
        throw new MethodUnSupportedException();
    }
} 
</code></pre><h3 id="如何决定该用抽象类还是接口？"><a href="#如何决定该用抽象类还是接口？" class="headerlink" title="如何决定该用抽象类还是接口？"></a>如何决定该用抽象类还是接口？</h3><p>要表示一种is-a的关系，并且是为了解决代码复用的问题，就用抽象类。</p>
<p>要表示一种has-a的关系，并且是为了解决抽象而非代码复用的问题，就可以使用接口。</p>
<p>抽象类是一种自下而上的设计思路，现有子类的代码重复，然后再抽象成上层的父类。而接口正好相反，它是一种自上而下的设计思路，在编程的时候一般是先设计接口再去考虑具体的实现。</p>
<h2 id="理论六：为什么基于接口而非实现编程？有必要为每个类都定义接口吗？"><a href="#理论六：为什么基于接口而非实现编程？有必要为每个类都定义接口吗？" class="headerlink" title="理论六：为什么基于接口而非实现编程？有必要为每个类都定义接口吗？"></a>理论六：为什么基于接口而非实现编程？有必要为每个类都定义接口吗？</h2><p>Program to an interface, not an implematation。这句话最早出自1994年GoF的设计模式这本书，是一种比较抽象泛化的思想。此处的interface不要局限于编程语言中的接口。</p>
<p>如果落实到具体代码，这条原则中的接口可以理解为编程语言中的接口或者抽象类。</p>
<p>应用这条原则可以有效地提高代码质量，实现接口和实现相分离，封装不稳定的实现，暴露稳定的接口。当实现发生变化的时候，上游系统的代码基本上不需要做改动，以此来降低耦合性，提高扩展性。</p>
<ul>
<li>函数的命名不能暴露任何实现细节</li>
<li>封装具体的实现细节</li>
<li>为实现类定义抽象的接口，具体的实现类都依赖统一的接口定义，遵从一致的上传功能协议。使用者依赖接口，而不是具体的实现类来编程。</li>
</ul>
<p>总之，我们在做软件开发的时候，一定要有抽象意识、封装意识、接口意识。在定义接口的时候，不要暴露任何实现细节，接口的定义只表明做什么，而不是怎么做。而且在设计接口的时候，我们要多思考一下，这样的接口设计是否足够通用，能否做到在替换具体的接口实现的时候，不需要任何接口定义的改动。</p>
<h3 id="是否要为每个类定义接口？"><a href="#是否要为每个类定义接口？" class="headerlink" title="是否要为每个类定义接口？"></a>是否要为每个类定义接口？</h3><p>如果业务场景中某个功能只有一种实现方式，未来也不可能被其他实现方式替换，那么我们就没有必要为其设计接口，也没有必要基于接口编程，直接使用实现类就可以。</p>
<h2 id="理论七：为什么要多用组合少用继承？如何决定该用组合还是继承？"><a href="#理论七：为什么要多用组合少用继承？如何决定该用组合还是继承？" class="headerlink" title="理论七：为什么要多用组合少用继承？如何决定该用组合还是继承？"></a>理论七：为什么要多用组合少用继承？如何决定该用组合还是继承？</h2><h3 id="为什么不推荐用继承？"><a href="#为什么不推荐用继承？" class="headerlink" title="为什么不推荐用继承？"></a>为什么不推荐用继承？</h3><p>比如，我们设计一个关于鸟的类AbstractBird，所有细分的鸟都继承这个抽象类。 大部分鸟都可以飞，我们可不可以在抽象类中定义fly()方法呢？答案是不行，因为还有特例，如果我们对所有不会飞的鸟都重写fly方法并且抛出异常也可行，但是不够优雅。一方面增加了代码量另一方面也违背的Least Knowledge Principle（最小知识原则/迪米特法则），暴露了不该暴露的接口给外部，增加了类使用过程中被误用的概率。</p>
<p>此时再通过抽象类派生出两个细分的类，AbstractFlyableBird/AbstractUnFlyableBird()，这样继承关系就变成3层。如果此时再关注鸟会不会叫等等特点，继承关系就会越来越复杂，导致代码可读性变差。也破坏了类的封装特性，将父类的实现细节暴露给子类，子类的实现依赖父类的实现，两者高度耦合，一旦父类代码修改就会影响所有子类的逻辑。</p>
<h3 id="组合相比继承有哪些优势？"><a href="#组合相比继承有哪些优势？" class="headerlink" title="组合相比继承有哪些优势？"></a>组合相比继承有哪些优势？</h3><p>我们可以利用组合Composition、接口、委托delegation三个技术手段，一起解决刚刚继承存在的问题。</p>
<p>接口表示某种行为特性，针对会飞特性我们可以定义Flyable接口，只让会飞的鸟去实现这个接口，对于会叫、会下蛋这些行为特性，类似定义Tweetable接口、EggLayable接口。不过接口只声明方法，不定义实现，也就是所有会下蛋的类都要实现一遍LayEgg()方法，会导致代码重复的问题。</p>
<p>我们可以针对三个接口再定义三个实现类，通过组合和委托技术来消除代码重复。</p>
<pre><code>public interface Flyable() {
    void fly();
}
public class FlyAbility implements Flyable {
    @Override
    public void fly() {...}
}
...

public class Ostrich implements Tweetable, Egglayable {
    private TweetAbility tweetAbility = new TweetAbility();
    private EggLayAbility eggLayAbility = new EggLayAbility();

    @Overide 
    public void tweet() {
        tweetAbility.tweet();
    }

    @Overide
    public void layEgg() {
        eggLayAbility.layEgg();
    }
}
</code></pre><h3 id="如何判断该用组合还是继承？"><a href="#如何判断该用组合还是继承？" class="headerlink" title="如何判断该用组合还是继承？"></a>如何判断该用组合还是继承？</h3><p>如果类之间的继承结构稳定，继承层次比较浅（最多两层），继承关系不复杂我们就可以大胆使用继承。反之则尽量使用组合。</p>
<p>有一些设计模式会固定使用继承或组合：</p>
<ul>
<li>装饰者模式decorator pattern、策略模式Strategy pattern、组合模式Composite pattern都使用了组合关系</li>
<li>模版模式template pattern则使用了继承关系</li>
</ul>
<p>还有一些特殊场景必须使用继承。如果不能改变函数的入参类型，而入参又非接口，为了支持多态只能采用继承来实现。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/08/10/ES-tech_md/" itemprop="url">
                  ES相关知识
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-08-10T21:52:00+08:00" content="2020-08-10">
              2020-08-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ES常见知识点"><a href="#ES常见知识点" class="headerlink" title="ES常见知识点"></a>ES常见知识点</h2><h3 id="ES集群-一个node一般会分配几个分片"><a href="#ES集群-一个node一般会分配几个分片" class="headerlink" title="ES集群,一个node一般会分配几个分片?"></a>ES集群,一个node一般会分配几个分片?</h3><ul>
<li>ES中的数据组织成索引,每一个索引由一个或多个分片组成.每个分片是Luncene索引中的一个实例,可以把实例理解成自管理的搜索引擎,用于在ES集群中对一部分数据进行索引和处理查询. 分片是ES在集群周围分发数据的单位,ES在重新平衡数据时移动分片的速度取决于分片的大小,数量以及网络和磁盘性能.</li>
<li>避免非常大的分片,因为大的分片可能会对集群从故障中恢复的能力产生负面影响.分片大小为50GB通常被界定为适用于各种用例的限制.</li>
<li>在集群节点上保存的分片数量与可用的堆内存大小成正比,经验来说,每个节点的分片数量保持在低于每1GB堆内存对应集群的分片在20-25之间. 如30GB内存的堆内存节点最多可以有600-750个分片.</li>
<li>参考文章: <ul>
<li><a href="https://mp.weixin.qq.com/s/mKL2PJuNUJTl71Axv4-Rcw" target="_blank" rel="external">Elasticsearch究竟要设置多少分片数</a></li>
<li><a href="https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster" target="_blank" rel="external">How many shard should I have in my Elasticsearch cluster</a></li>
</ul>
</li>
</ul>
<h3 id="Elasticsearch如何实现master选举"><a href="#Elasticsearch如何实现master选举" class="headerlink" title="Elasticsearch如何实现master选举"></a>Elasticsearch如何实现master选举</h3><ul>
<li>Elasticsearch的任意一个节点都可以设置node.master和node.data属性<ul>
<li>master=true,data=true:  即是Master Eligible又是data节点</li>
<li>master=true,data=false: 单纯的Master Eligible节点</li>
<li>master=false,data=true: 单纯的data节点</li>
<li>master=false,data=false: 纯粹的Coordinating Node, 协调节点负责查询时的数据收集,合并,以及聚合操作. ES中所有节点都是协调节点</li>
</ul>
</li>
<li>ES针对当前集群的所有Master Eligible节点进行选举,为了避免split-brain现象,ES选取QUORUM思想,只有超过半数选票的节点才能成为master.(eligibleNodesNum/2 + 1)</li>
<li>当满足下列条件时就会触发一次master选举<ul>
<li>当前master eligible节点不是master</li>
<li>当前master eligible节点与其他节点通信无法发现master</li>
<li>集群中无法连接到master的master eligible节点达到minimum_master_nodes的值</li>
</ul>
</li>
<li>某个节点决定要选举时,会实现如下操作:<ul>
<li>寻找ClusterStateVersion比自己高的master eligible节点,向其发送选举投票</li>
<li>如果CLusterStateVersion相同,则计算自己能找到的master eligible节点(包含自己)中节点id最小的节点,向其发送选举投票</li>
<li>如果一个节点收到足够多的选票,并且向自己也投票了,则该节点成为master开始发布集群信息</li>
</ul>
</li>
<li>与其他选举方法对比<ul>
<li>Zookeeper: ES可以使用Zookeeper来进行选举,方法如下:<ul>
<li>所有master eligible尝试在ZK上创建指定路径</li>
<li>只有第一个节点创建成功,该节点成为master,其余节点watch此路径</li>
<li>一旦ZK失去master链接,该路径被删除,其他master eligible继续尝试创建路径, 重复上述操作</li>
</ul>
</li>
<li>Raft: 相比ES自身的选举算法,Raft是经过严格论证的一致性算法,ES早期版本时Raft还未提出,可能后续会参考改进.</li>
</ul>
</li>
</ul>
<h3 id="如何做到ES写入调优"><a href="#如何做到ES写入调优" class="headerlink" title="如何做到ES写入调优?"></a>如何做到ES写入调优?</h3><ul>
<li>客户端：　多线程批量写<ul>
<li>通过性能测试,确定最佳文档数量</li>
<li>多线程,观察HTTP429返回,实现retry以及线程数量的自动调节</li>
</ul>
</li>
<li>服务器端: <ul>
<li>降低IO操作<ul>
<li>使用ES自动生成的文档ID(避免Get操作)</li>
<li>调整Refresh interval等配置(降低搜索实时性)</li>
<li>调整translog选项,降低写磁盘频率(牺牲容灾能力)</li>
</ul>
</li>
<li>降低CPU和存储开销<ul>
<li>减少不必要的分词</li>
<li>避免不需要的doc_values</li>
<li>文档字段尽量保持相同顺序,提高文档的压缩率</li>
</ul>
</li>
<li>尽量做到写入和分片的负载均衡,实现水平扩展<ul>
<li>Shard Filtering</li>
<li>Write load balance</li>
</ul>
</li>
<li>调整Bulk线程池和队列<ul>
<li>客户端:<ul>
<li>单个bulk请求体的数据量不要太大,官方建议5-15mb</li>
<li>写入bulk请求超时时间需要足够长,建议60s以上</li>
<li>写入端尽量将数据轮询到不同节点</li>
</ul>
</li>
<li>客户端:<ul>
<li>索引创建属于计算密集型,应该使用固定大小的线程池,来不及处理的放入队列.线程数配置为CPU核数+1,避免过多上下文切换</li>
<li>队列大小可以适当增加,不要太大否则占用内存导致GC</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Index建模实践<ul>
<li>如果只需要聚合不需要搜索,index设置为false</li>
<li>如果不需要算分,Norms设为false</li>
<li>不对字符串使用默认的dynamic mapping,字段数量过多会对性能产生较大影响</li>
<li>Index_options控制在创建倒排索引时哪些内容会被添加到倒排索引中,节约CPU</li>
<li>关闭_source减少IO操作,适合指标数据</li>
</ul>
</li>
</ul>
<h3 id="如何避免split-brain"><a href="#如何避免split-brain" class="headerlink" title="如何避免split-brain?"></a>如何避免split-brain?</h3><p>当节点崩溃或者节点通讯故障时,如果一个子节点无法连接到主节点,那么它会发起选举从与之连接的master eligible节点中选举一个新的主节点.新主节点将接管开始工作,如果旧的主节点重新加入集群或恢复通信,那么新主节点将降级到子节点.这过程在大多数情况下是不存在冲突,高效无缝衔接的.</p>
<p>但是考虑集群中只有两个节点的极端情况,如果因为网络原因导致节点无响应,节点都相信对方已经挂掉.重新选举后会存在两个主节点,集群处于不一致状态,分片的两份数据分开了,如果不做全量的重索引很难进行重排序. </p>
<p>极端情况下,客户端无法感知到这种不一致状态,因为index请求无论发往哪个节点都会成功,只有在查询时才可能发现问题.</p>
<p>要避免脑裂,就是要关注discovery.zen.minimum_master_nodes的值,要设置为n/2+1,还有建议配置3节点以上的集群. 对于已经存在的两节点集群,可以添加一个新节点并将node.data设为false,放在一台便宜的服务器上.</p>
<h3 id="ES对于大数据量-上亿量级-的聚合如何实现"><a href="#ES对于大数据量-上亿量级-的聚合如何实现" class="headerlink" title="ES对于大数据量(上亿量级)的聚合如何实现?"></a>ES对于大数据量(上亿量级)的聚合如何实现?</h3><p>有些算法可以分布式执行,类似一些单次请求获得精确结果的聚合.它们无需额外代价,就能在多台机器上运行,如max度量:</p>
<ul>
<li>请求广播到所有分片</li>
<li>查看每个文档的price字段,如果price&gt;current_max就替换</li>
<li>返回所有分片的最大值给协调节点</li>
<li>找出所有分片返回的最大值即可<br>这种算法会随着机器数的线性增长而横向扩展,无需任何协调操作且内存消耗小. 但是不是所有算法都可以如此,一些复杂的算法需要在算法性能和内存使用上做出权衡.</li>
</ul>
<p>三角因子模型: 大数据,精确性,实时性</p>
<ul>
<li>精确+实时: 数据可以存入单台机器的内存中,结果会100%精确,响应会相对快速.</li>
<li>大数据+精确: 传统的hadoop,可以处理PB级数据并且未我们提供精确的答案,但它可能需要几周的时间才能为我们提供答案.</li>
<li>大数据+实时: 近似算法为我们提供准确但不精确的结果.</li>
</ul>
<p>ES目前支持两种近似算法(cardinality和percentiles),以牺牲一点小小的估算错误为代价换回高速的执行效率和极小的内存消耗.</p>
<h4 id="Cardinality"><a href="#Cardinality" class="headerlink" title="Cardinality"></a>Cardinality</h4><p>Elasticsearch提供的首个近似聚合是Cardinality度量.它提供一个字段的基数,即该字段的distinct或者unique值的数目.基于HyperLogLog++(HLL)算法,HLL会先对用户输入做哈希运算,然后根据哈希运算结果中的bits做概率估算从而得到基数. 算法特性如下:</p>
<ul>
<li>可配置的精度用来控制内存的使用(更精确=更多内存)</li>
<li>小的数据集精度非常高</li>
<li>可以通过配置参数,来设置去重需要的固定内存使用量.无论数千还是数十亿的唯一值,内存使用量只与配置的精度相关.</li>
</ul>
<p>要配置精度,必须指定percision_threshold参数值,这个阈值定义了在何种基数水平下我们希望得到一个近乎精确的结果. 接受0-40,000之间的数字,更大的值还是会被当做40,000来处理.</p>
<p>对于指定阈值,HLL的数据结构大概使用percision_threshold*8字节的内存,所以必须在牺牲内存和获得额外的准确度之间做平衡. 实际使用中,阈值=100时可以在唯一值为百万的情况下仍将误差维持在5%以内.</p>
<p>如果想要获得唯一值的数目,通常需要查询整个数据集合,所有基于所有数据的操作都必须迅速.HyperLogLog已经很快了,它只是简单的对数据做哈希以及一些位操作,但是仍然可以进一步优化.</p>
<p>HLL只需要字段内容的哈希值,因此我们可以在索引时就预先计算好,在查询时跳过哈希计算将哈希值直接从fielddata中加载出来. 在执行聚合时使用X.hash字段而非X字段,cardinality会读取预先计算的哈希值取代动态计算原始值的哈希. 单个文档节省时间非常少,但是如果聚合一亿数据,每个字段会多花费10ns时间,这样每次查询时都会额外增加1s. 如果我们在非常大量的数据里面使用cardinality需要权衡使用预计算的意义,是否需要提前计算hash,从而在查询中获得更好的性能.</p>
<h4 id="Percentiles"><a href="#Percentiles" class="headerlink" title="Percentiles"></a>Percentiles</h4><p>百分位数通常用来找出异常,比如监控网站的延时来判断响应是否能保证良好的用户体验. 和基数一样,计算百分位需要一个近似算法,percentiles中使用一个TDigest算法</p>
<ul>
<li>百分位的准确度和百分位的极端程度相关. 1和99的百分位要比50百分位要准确</li>
<li>数据集合小的情况,百分位非常准确</li>
<li>随着桶里数值的增长,算法会开始对百分位进行估算.有效在准确度和内存节省之间做出权衡.</li>
</ul>
<p>通过修改compression参数来控制内存和准确度之间的比值.TDigest算法用节点近似计算百分比,节点越多准确度越高.compression参数限制节点的最大数目为20*compression.</p>
<p>通过增加compression可以以消耗更多内存未代价提高百分位数准确性,更大的压缩比值会使得算法运行更慢,因为底层的树形结构的存储会增长,导致操作的代价更高.默认compression=100</p>
<p>一个节点大约使用32字节的内存,所以在最坏情况下,默认设置会生成一个64KB的TDigest.</p>
<h3 id="ES主分片数量可以在后期更改吗-为什么"><a href="#ES主分片数量可以在后期更改吗-为什么" class="headerlink" title="ES主分片数量可以在后期更改吗?为什么?"></a>ES主分片数量可以在后期更改吗?为什么?</h3><p>ES 2.X版本时不可以,因为ES对document的处理是通过路由算法来进行处理,更改主分片数量会导致路由被破坏,间接导致数据丢失.所以主分片数量不可以修改.</p>
<p>如果修改分片数量后重新分配数据,分片的切分成本和reindex成本差不多,所以官方直接使用reindex. 如果数据不重复,其实新的业务数据可以切换到新的索引上继续写,查询时查询新旧两个索引.</p>
<p>从ES6.1以后支持在线扩大shard的数量,但是操作期间需要对index锁写:</p>
<ul>
<li>创建一个新的目标索引,定义与源索引相同,但具有更多的主分片</li>
<li>将段从源索引硬链接到目标索引,如果文件系统不支持hard link,则将所有段都复制到新索引中(非常耗时)</li>
<li>创建低级文件后,再次对所有文档进行hash处理,删除属于不同分片的文档</li>
<li>恢复目标索引</li>
</ul>
<h3 id="ES更新文档-删除文档的执行流程"><a href="#ES更新文档-删除文档的执行流程" class="headerlink" title="ES更新文档/删除文档的执行流程"></a>ES更新文档/删除文档的执行流程</h3><h4 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h4><ul>
<li>在进行写操作时,ES根据传入的_routing参数按照公式计算出文档要分配的分片,从集群元数据中找出对应主分片的位置,将请求路由到该分片.</li>
<li>文档写入Lucene后不能被立即查询,ES提供refresh操作,定时的调用Lucene的reopen(OpenIfChanged)为内存中新写入的数据生成一个新的segment.此时文档可以被检索.</li>
<li>即使refresh后文档仍然在文件系统缓存中,如果服务器宕机这部分数据依旧会丢失. ES为此增加了translog,文档写入时会先将文档写入Lucene,然后写入一份到translog落盘.(如果可靠性要求不高可以设置异步落盘)translog是追加写入,性能比随机写入要好.先写Lucene后写translog是因为写入Lucene可能会失败,减少写入失败回滚的复杂度.</li>
<li>间隔30分钟或者translog大小到达阈值时触发flush操作,ES会先执行refresh操作将buffer生成segment,然后调用Lucene的commit方法将所有内存中的segment fsync到磁盘中.此时Lucene中数据完成持久化,清空translog中数据(6.X版本为了实现sequenceIDs,不删除translog)</li>
<li>由于refresh默认间隔1s,会产生大量的小segment,ES会运行一个任务检测当前磁盘中的segment,对符合条件的进行合并,减少Lucene中的segment个数,提高查询速度减少负载.</li>
</ul>
<p>Lucene仅支持文档整体更新,ES为了支持局部更新,在Lucene的Store索引中存储一个_source字段,key时文档ID内容是文档原文.更新时先从_source中获取原文,与更新部分合并,再调用Lucene API进行全量更新. 增加版本机制防止其他线程并发写.</p>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><ul>
<li>提交删除操作,先查询要删除文档所属的segment</li>
<li>commit中包含一个.del文件,记录哪些segment中的哪些文档被标记为deleted.</li>
<li>当.del文件中存储的文档足够多时,ES执行物理删除操作,清楚文档<ul>
<li>在删除中进行搜索操作: 依次查询所有segment,根据.del文件过滤掉标记为deleted的文档,然后返回搜索结果</li>
<li>在删除过程中更新: 将旧文档标记为deleted,将新文档写入新的segment中.执行查询时通过.del过滤掉旧版本文档</li>
</ul>
</li>
</ul>
<h3 id="ES-shard内部是由什么组成的"><a href="#ES-shard内部是由什么组成的" class="headerlink" title="ES shard内部是由什么组成的?"></a>ES shard内部是由什么组成的?</h3><p>Shard 实际上就是一个Lucene的实例(Lucene Index),单个Lucene实例中最多包含Integer.MAX_VALUE-128个documents</p>
<p>一个LuceneIndex在文件系统表现上来看就是存储了一系列文件的目录,由许多个独立的segments组成. segments包含了文档中的词汇字典,词汇字典的倒排索引,以及document的字段数据. 所有segments数据存储于_<segments_name>.cfs文件中</segments_name></p>
<h4 id="Segments"><a href="#Segments" class="headerlink" title="Segments"></a>Segments</h4><p>segments直接提供了搜索功能,ES中的一个shard由大量的segments文件组成,且每一次fresh都会产生一个新的segment文件,segment文件有大有小,相当碎片化. ES内部则会开启一个线程将小的segment合并减少碎片化,降低文件打开数提升IO性能.</p>
<p>segment文件是不可变更的,当一个document更新时,实际上是将旧的文档标记删除,索引一个新文档(在_<segment_name>.del标记某个文档删除,查询时会跳过).在Merge时会将旧文档删除掉(物理删除).</segment_name></p>
<h3 id="ES中分析器是什么"><a href="#ES中分析器是什么" class="headerlink" title="ES中分析器是什么?"></a>ES中分析器是什么?</h3><p>分析 包含以下过程:</p>
<ul>
<li>将文本分成适合于倒排索引的独立词条</li>
<li>将词条统一化为标准格式以提高可搜索性<br>分析器执行上述工作,实际上将三个功能封装到一个包中:</li>
<li><strong>字符过滤器:</strong> 分词前整理字符串</li>
<li><strong>分词器:</strong> 拆分字符串到单个词条</li>
<li><strong>Token过滤器:</strong> 词条按顺序通过每个token过滤器,该过程可能会改变词条(大小写),删除此条(无用词删除),或者增加词条(同义词).</li>
</ul>
<p>ES附带了可以直接使用的预包装的分析器:</p>
<ul>
<li><strong>标准分析器:</strong> ES默认使用的分析器, 分析各语言文本最常用的选择,根据Unicode定义的单词边界划分文本. 删除绝大部分标点并将词条小写.</li>
<li><strong>简单分析器:</strong> 在任何不是字母的地方分割文本,将词条小写</li>
<li><strong>空格分析器:</strong> 在空格的地方划文本</li>
<li><strong>语言分析器:</strong> 考虑指定语言的特点,如英语分析器删除无用的单词(the and…),并且提取英语单词的词干</li>
</ul>
<p>当我们索引一个文档,它的全文域被分析成词条以用来创建倒排索引.当全文域检索的时候,需要将查询字符串通过相同的分析过程,以保证搜索的词条格式和索引中词条格式一致.</p>
<h3 id="客户端和集群连接时-如果选择特定的节点执行请求"><a href="#客户端和集群连接时-如果选择特定的节点执行请求" class="headerlink" title="客户端和集群连接时,如果选择特定的节点执行请求?"></a>客户端和集群连接时,如果选择特定的节点执行请求?</h3><p>TransportClient利用transport模块远程连接一个elasticsearch集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的transport地址，并以 轮询 的方式与这些地址进行通信。</p>
<h3 id="ES中倒排索引是什么"><a href="#ES中倒排索引是什么" class="headerlink" title="ES中倒排索引是什么?"></a>ES中倒排索引是什么?</h3><p>Inverted Index也叫反向索引,通过value找key. 对比词典的话,Term就相当于词语,Term Dictionary相当于词典,Term Index相当于词典的目录索引, Posting List相当于词语在字典的页数集合.</p>
<ul>
<li>Term: 一段文本经过分析器分析后会输出一串单词,单词即是Term</li>
<li>Term Dictionary: 里面维护的Term,可以理解为Term集合. </li>
<li>Term Index: 为了更快的找到某个单词,我们为单词建立索引.B-Tree通过减少磁盘寻道次数来提高查询性能. ES也是采用同样思路,直接通过内存查找Term,不读磁盘. 如果term过多,Term dictionary会很大无法都放入内存,因此通过TermIndex(字典树). 这棵树不会包含所有的Term,包含的是一些Term的前缀,通过term index快速定位到Term dictionary的offset,然后从这个位置向后顺序查找. 再加上一些压缩基数,term index的尺寸可以只有所有Term尺寸的几十分之一,使得内存可以缓存整个term index.</li>
<li>PostingList(倒排列表):记录了出现过某个单词的所有文档的文档列表,以及该单词出现的位置信息,每条记录成为一个倒排项Posting.</li>
</ul>
<p>为什么Elasticsearch/Lucene检索可以比mysql快了? Mysql只有term dictionary这一层，是以b-tree排序的方式存储在磁盘上的。检索一个term需要若干次的random access的磁盘操作。而Lucene在term dictionary的基础上添加了term index来加速检索，term index以树的形式缓存在内存中。从term index查到对应的term dictionary的block位置之后，再去磁盘上找term，大大减少了磁盘的random access次数。</p>
<p>ES在建立倒排索引时，会对拆分的各个单词进行相应处理，以提升后面搜索的时候能够搜索到相关联的文档的概率，这就是标准化规则转换，主要包括：时态的转换（例如liked转换为like）、单复数的转换（hospitals转换为hospitals）、同义词的转换（small转换为little）、大小写的转换（默认转换为小写）</p>
<p>当利用ES进行查询时，查询结果都会返回一个对应词条的相关度分数（score）。相关度分数的计算基于TF/IDF算法（Term Frequence&amp;Inverse Doucument Frequency）</p>
<ul>
<li>Term Frequence ，TF(t in f)：我们查询的词条在文本中出现多少次，出现次数越多，相关度越高.</li>
<li>Inverse Doucument Frequency，IDF(t in all-f)：查询词条在所有文本中出现的次数，出现次数越高，相关度越低。</li>
<li>Field-length(字段长度规约)：字段的长度越长，相关度越低。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/08/02/设计模式之美第一周/" itemprop="url">
                  设计模式之美第一周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-08-02T16:55:00+08:00" content="2020-08-02">
              2020-08-02
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前也下决心看过《HeadFirst设计模式》和《HeadFirst面向对象分析与设计》，但是都没有坚持读完。看了下极客上的这个课程比较适合自己，希望能坚持下来✊</p>
<h2 id="码农要尽早学习并掌握设计模式相关知识"><a href="#码农要尽早学习并掌握设计模式相关知识" class="headerlink" title="码农要尽早学习并掌握设计模式相关知识"></a>码农要尽早学习并掌握设计模式相关知识</h2><p>设计模式能更直接地提高我们的开发能力，如同数据结构和算法教人如何写出高效代码，设计模式教我们如何写出可扩展、可读、可维护的高质量代码。</p>
<h3 id="为什么学习设计模式？"><a href="#为什么学习设计模式？" class="headerlink" title="为什么学习设计模式？"></a>为什么学习设计模式？</h3><ul>
<li>应对面试中的设计模式相关问题<ul>
<li>不管是前端、后端还是全栈工程师，在面试中设计模式问题总是被问得频率比较高。因此平时要多注意积累。</li>
</ul>
</li>
<li>被人吐槽代码烂<ul>
<li>Talk is cheap。 代码能力是码农最基础的能力，是展示程序员基础素养的最直接的衡量标准。这个专栏不仅讲解设计模式，还会通过实战教我们避免类似命名不规范、类设计不合理、分层不清晰、没有模块化概念、代码结构混乱、高度耦合等代码问题</li>
</ul>
</li>
<li>提高复杂代码的设计和开发能力</li>
<li>让读源码、学框架事半功倍<ul>
<li>优秀的开源项目、框架、中间件、代码量、类个数都会比较多，为了保证代码的扩展性、灵活性、可维护性，代码中会使用到很多设计模式或者设计思想。学习设计模式相关知识，可以让我们更轻松地读开源项目。</li>
</ul>
</li>
<li>为职场发展做铺垫</li>
</ul>
<p>投资要趁早，这样才能尽早享受复利。设计模式作为一门与编码、开发有着直接关系的基础知识，早点学习就可以在项目中早点实践锻炼。</p>
<h2 id="从哪些维度评判代码质量的好坏？如何具备写出高质量代码的能力？"><a href="#从哪些维度评判代码质量的好坏？如何具备写出高质量代码的能力？" class="headerlink" title="从哪些维度评判代码质量的好坏？如何具备写出高质量代码的能力？"></a>从哪些维度评判代码质量的好坏？如何具备写出高质量代码的能力？</h2><h3 id="如何评价代码质量的高低？"><a href="#如何评价代码质量的高低？" class="headerlink" title="如何评价代码质量的高低？"></a>如何评价代码质量的高低？</h3><ul>
<li><strong>可维护性 Maintainability</strong>，对于一个项目来说，维护代码的时间远远大于编写代码的时间。工程师大部分的时间可能都是花在了修bug，改老功能逻辑，添加新功能之类的工作上。如果代码分层清晰、模块化好、高内聚低耦合、遵从基于接口而非事先编程的设计原则，那么就可能意味着代码易维护。除此之外，还跟项目代码量的多少、业务复杂程度、利用到的技术复杂程序、文档是否全面、团队成员的开发水平等诸多因素有关。</li>
<li><strong>可读性 Readability</strong>，Any fool can write code that a computer can understand, Good programmer write code that humans can understand. 看代码是否符合编码规范、命名是否达意、注释是否详尽、函数是否长短合适、模块划分是否清晰、是否符合高内聚低耦合等。 Code revie是很好的检测代码可读性的手段。</li>
<li><strong>可扩展性 Extensibility</strong>，对修改关闭，对扩展开放设计准则</li>
<li><strong>灵活性 Flexibility</strong>， 代码易扩展、易复用、或者易用。</li>
<li><strong>简洁性 Simplicity</strong>， KISS原则， ”Keep It Simple， Stupid“，思从深而行从简，真正的高手能云淡风轻地用最简单的方法解决最复杂的问题。</li>
<li><strong>复用性 Reusability</strong>， DRY设计原则，Don’t Repeat Yourself</li>
<li><strong>可测试性 Testability</strong>， 易写单元测试。</li>
</ul>
<h3 id="如何写出高质量代码？"><a href="#如何写出高质量代码？" class="headerlink" title="如何写出高质量代码？"></a>如何写出高质量代码？</h3><p>需要掌握一些更加细化、更加能落地的编程方法论，包括面向对象设计思想、设计原则、设计模式、编码规范、重构技巧等。</p>
<h2 id="面向对象、设计原则、设计模式、编码规范重构，五者有何关系？"><a href="#面向对象、设计原则、设计模式、编码规范重构，五者有何关系？" class="headerlink" title="面向对象、设计原则、设计模式、编码规范重构，五者有何关系？"></a>面向对象、设计原则、设计模式、编码规范重构，五者有何关系？</h2><h3 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h3><ul>
<li>四大特性： 封装、抽象、继承、多态</li>
<li>面向对象编程和面向过程编程的区别和联系</li>
<li>面向对象分析、面向对象设计、面向对象编程</li>
<li>接口和抽象类的区别以及各自的应用场景</li>
<li>基于接口而非实现编程的设计思想</li>
<li>多用组合少用继承的设计思想</li>
<li>面向过程的贫血模型和面向对象的充血模型</li>
</ul>
<h3 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h3><ul>
<li>SOLID原则-SRP 单一职责原则</li>
<li>SOLID原则-OCP 开闭原则</li>
<li>SOLID原则-LSP 里式替换原则</li>
<li>SOLID原则-ISP 接口隔离原则</li>
<li>SOLID原则-DIP 依赖倒置原则</li>
<li>DRY原则、KISS原则、YAGNI原则、LOD法则</li>
</ul>
<h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>经典的设计模式有23种，随着编程语言的演进，一些设计模式Singleton随时过时，甚至成了反模式，一些则被内置在编程语言中（如Iterator），另外还有一些新模式诞生（如Monostate）。</p>
<p>23中经典的设计模式，可以分为三大类：创建型、结构型、行为型。</p>
<ul>
<li>创建型 <ul>
<li>常用的： 单例模式、工厂模式、建造者模式</li>
<li>不常用： 原型模式</li>
</ul>
</li>
<li>结构型<ul>
<li>常用的： 代理模式、桥接模式、装饰者模式、适配器模式</li>
<li>不常用： 门面模式、组合模式、享元模式</li>
</ul>
</li>
<li>行为型<ul>
<li>常用的： 观察者模式、模版模式、策略模式、指责链模式、迭代器模式、状态模式</li>
<li>不常用： 访问者模式、备忘录模式、命令模式、解释器模式、中介模式</li>
</ul>
</li>
</ul>
<h3 id="编程规范"><a href="#编程规范" class="headerlink" title="编程规范"></a>编程规范</h3><p>编程规范主要解决的是代码的可读性问题。编程规范相对于设计原则、设计模式，更加具体、更加偏重代码细节。 《重构》《代码大全》《代码整洁之道》书籍推荐</p>
<h3 id="代码重构"><a href="#代码重构" class="headerlink" title="代码重构"></a>代码重构</h3><p>重构是软件开发中非常重要的一个环节。持续重构是保持代码质量不下降的有效手段，能有效避免代码腐化到无可救药的地步。</p>
<p>重构的工具就是前面罗列的那些面向对象设计思想、设计原则、设计模式、编码规范。实际上，设计思想、设计原则、设计模式一个最重要的应用场景就是在重构的时候。虽然设计模式可以提高代码的可扩展性，但是过度不恰当地使用，会增加代码的复杂度，影响代码的可读性。在开发初期，除非特别必须，一定不要过度设计，应用复杂的设计模式。而是当代码出现问题的时候，再针对问题，应用原则和模式进行重构。这样就能有效避免前期的过度设计。</p>
<ul>
<li>重构的目的Why、对象What、时机When、方法How</li>
<li>保证重构不出错的技术手段：单元测试和代码的可测试性</li>
<li>两种不同规模的重构：大重构（大规模高层次）和小重构（小规模低层次）</li>
</ul>
<h3 id="五者之间的联系"><a href="#五者之间的联系" class="headerlink" title="五者之间的联系"></a>五者之间的联系</h3><ul>
<li>面向对象编程因其具有丰富的特性，可以实现很多复杂的设计思路，是很多设计原则设计模式等编码实现的基础。</li>
<li>设计原则指导我们代码设计的一些经验总结。</li>
<li>设计模式是针对软件开发中经常遇到的一些设计问题，总结出来的一套解决方案或者设计思路。相比设计原则更具体更加可执行。</li>
<li>编程规范主要解决代码的可读性问题，相比设计原则、设计模式更具体，更偏重代码细节、更加能落地。持续小重构依赖的理论基础就是编程规范。</li>
<li>重构作为保持代码质量不下降的有效手段，利用的就是面向对象、设计原则、设计模式、编程规范这些理论。</li>
</ul>
<h2 id="本周回顾"><a href="#本周回顾" class="headerlink" title="本周回顾"></a>本周回顾</h2><p>专栏中所涉及的知识点都在下图中。<br><img src="/2020/08/02/设计模式之美第一周/DesignModelAll.jpg" alt>  </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/07/25/TalkGo读书会第一期总结/" itemprop="url">
                  TalkGo读书会第一期总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-07-25T16:55:00+08:00" content="2020-07-25">
              2020-07-25
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h2><p>选择加入读书会首先是本身对性能优化这个Topic比较感兴趣，另外公司底层架构升级维护也需要关注性能问题。课程计划表是两个月，一天一篇的话也不会特别占时间，但是真正执行下来才深刻体会到惰性的强大(如果不是读书会的话应该很容易就弃掉)。平时工作的话回家看书的时间也不会太多，有两三次都是周末抓紧时间搞定，尤其是后面网络那部分平时不太会去关注的指标，可能以前遇到这种章节都会直接略过。</p>
<p>这个课程主要还是偏向原理，基础的性能指标，主要提供了一些遇到性能问题如何下手分析解决的思路。真正在环境中遇到问题分析出原因定位代码还是需要一些经验。特别是一些开源软件遇到性能问题时定位具体代码还是需要耗费一些精力。我们组这个月正好遇到了ES小版本升级后，xpack monitor导致的内存使用问题，定位代码借助了jstack和阿里的Arthas才最终找到原因。</p>
<p>总之，这个课程还是需要经常拿来回顾下，遇到性能问题多积攒经验才是王道。</p>
<p>接下来的看书计划的话，正好前两天收到了一本《架构修炼之道》，准备先翻看下。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/07/19/Linux性能优化实战第八周--套路篇/" itemprop="url">
                  《Linux 性能优化实战》第八周--套路篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-07-19T16:55:00+08:00" content="2020-07-19">
              2020-07-19
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>在实际的性能分析中，发生性能瓶颈后登陆服务器想要排查的时候发现瓶颈已经消失且很难复现。因此要事先搭建监控系统，把系统和应用程序的运行状况监控下来，并定义一系列的策略，在发生问题第一时间告警处理。</p>
<p>系统监控通过USE法则利用prometheus+grafana来监控系统资源。应用程序监控分为指标监控和日志监控两大块，在复杂业务场景中通常搭建全链路跟踪系统来定位应用瓶颈。</p>
<p>性能问题可以从系统资源瓶颈和应用程序瓶颈两个角度来梳理分析。系统时应用的运行环境，系统的瓶颈会导致应用的性能下降，而应用的不合理设计也会引发系统资源的瓶颈。做性能分析要结合应用程序和操作系统的原理，就出引发问题的真凶。</p>
<p>本周回顾了前几周学习到的常见性能优化方法。值得注意的是一定切记要避免过早优化，性能优化往往会提高复杂度，一方面降低了可维护性，另一方面也为适应复杂多变的新需求带来障碍。所以要逐步完善，首先保证能满足当前的性能要求。发现性能不满足要求或者出现性能瓶颈后，再根据性能分析的结果选择最重要的性能问题进行优化。</p>
<p>不能把性能工具当成性能分析和优化的全部，一方面性能分析和优化的核心是对系统和应用程序运行原理的掌握，性能工具只是辅助我们更快完成此过程的帮手；另一方面完善监控系统可以提供绝大部分性能分析所需的基准数据，从这些数据中很可能大致定位出性能瓶颈，也就不用再去手动执行各类工具了。</p>
<h3 id="捞评论"><a href="#捞评论" class="headerlink" title="捞评论"></a>捞评论</h3><ol>
<li>除了USE原则，还有RED原则。更偏重于应用，在很多微服务中会用到。<ul>
<li>Rate： 每秒请求数量</li>
<li>Errors： 失败的请求次数</li>
<li>Duration： 处理一条请求所需的时间</li>
</ul>
</li>
<li>想要学习eBPF来说，可以从BPF Compiler Collection（BCC）这个项目开始。BCC提供了很多短小的示例，可以快速了解eBPF的工作原理并熟悉eBPF程序的开发思路。了解这些基本的用法后，再去深入了解eBPF。</li>
</ol>
<h2 id="Lesson-53-套路篇：系统监控的综合思路"><a href="#Lesson-53-套路篇：系统监控的综合思路" class="headerlink" title="Lesson 53 套路篇：系统监控的综合思路"></a>Lesson 53 套路篇：系统监控的综合思路</h2><p>一个好的监控系统，不仅可以实时暴露系统的各种问题，更可以根据这些监控到的状态，自动分析和定位大致的瓶颈来源，从而更精确地把问题汇报给相关团队。</p>
<ul>
<li>从系统来说，监控系统要涵盖系统的整体资源使用情况。如CPU、内存、磁盘、文件系统和网络等</li>
<li>从应用程序来说，监控系统要涵盖应用程序内部的运行状态。即包括进程的CPU、磁盘I/O等整体运行状况，也包括接口调用耗时、执行中的错误、内部对象的内存使用等程序内部的运行状况</li>
</ul>
<h3 id="USE法"><a href="#USE法" class="headerlink" title="USE法"></a>USE法</h3><p>USE， Utilization Saturation and Errors</p>
<ul>
<li>使用率，资源用于服务的时间或容量百分比</li>
<li>饱和度，资源的繁忙程度，通常和队列长度相关</li>
<li>错误数，表示发生错误的事件个数</li>
</ul>
<p>常见指标分类如下图所示：</p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/use.png" alt></p>
<h3 id="监控系统"><a href="#监控系统" class="headerlink" title="监控系统"></a>监控系统</h3><p>一个完整的监控系统通常由数据采集、数据存储、数据查询和处理、告警以及可视化展示等多个模块组成。</p>
<p>常见的开源监控工具有Zabbix、Nagios、Prometheus等，下面介绍Prometheus的基本架构：</p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/prometheus.png" alt></p>
<h4 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h4><p>Prometheus targets就是数据采集的对象，Retrieval负责采集这些数据。Prometheus支持两种采集模式：</p>
<ul>
<li>Pull模式，服务端的采集模块触发采集。只要采集目标提供HTTP接口即可（常用的采集模式）</li>
<li>Push模式，各个采集目标主动向Push Gateway推送目标，再由服务器端从Gateway中拉取（移动应用中常用）</li>
</ul>
<p>Prometheus提供服务发现机制，自动根据配置的规则动态发现需要监控的对象。在K8S容器平台中非常有效。</p>
<h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h4><p>TSDB（Time Series Database），负责将采集到的数据持久化到SSD等磁盘设备中。 TSDB是专门为时序数据设计的数据库，以时间为索引、数据量大且以追加的方式写入。</p>
<h4 id="数据查询和处理"><a href="#数据查询和处理" class="headerlink" title="数据查询和处理"></a>数据查询和处理</h4><p>TSDB在存储数据的同时，还提供了数据查询和基本的数据处理功能。即PromQL语言，提供了简介的查询、过滤功能。</p>
<h4 id="告警模块"><a href="#告警模块" class="headerlink" title="告警模块"></a>告警模块</h4><p>AlertManager提供了告警功能，基于PromQL语言的触发条件、告警规则的配置管理以及告警的发送。还支持通过分组、抑制或者静默等多种方式来聚合同类告警。</p>
<h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h4><p>Prometheus WebUI提供了简单的可视化界面。通常配合Grafana来构建强大的图形界面。</p>
<h2 id="Lesson-54-套路篇：应用监控的一般思路"><a href="#Lesson-54-套路篇：应用监控的一般思路" class="headerlink" title="Lesson 54 套路篇：应用监控的一般思路"></a>Lesson 54 套路篇：应用监控的一般思路</h2><h3 id="指标监控"><a href="#指标监控" class="headerlink" title="指标监控"></a>指标监控</h3><p><strong>应用程序的核心指标，不再是资源的使用情况，而是请求数、错误率和响应时间。</strong><br>除了上述三个指标外，下面几种指标也是应用程序监控必不可少的，可以帮助我们快速定位性能瓶颈。</p>
<ul>
<li><strong>应用进程的资源使用情况</strong>，比如进程占用的CPU、内存、磁盘I/O、网络等。</li>
<li><strong>应用程序之间的调用情况</strong>，如调用频率、错误数、延时等<ul>
<li>可以迅速分析出一个请求处理的调用链中哪个组件导致性能问题</li>
</ul>
</li>
<li><strong>应用程序内部核心逻辑的运行情况</strong>，比如关键环节的耗时以及执行过程中的错误等<ul>
<li>直接进入应用程序内部，定位到底是哪个处理环节的函数导致性能问题</li>
</ul>
</li>
</ul>
<p>由于业务系统通常会涉及到一连串的多个服务，形成一个复杂的分布式调用链。为了迅速定位这类跨应用的性能瓶颈，可以使用Zipkin、Jaeger、Pinpoint等各类开源工具来构建全链路跟踪系统。</p>
<h3 id="日志监控"><a href="#日志监控" class="headerlink" title="日志监控"></a>日志监控</h3><ul>
<li>指标是特定时间段的数值型测量数据，通常以时间序列的方式处理，适合于实时监控</li>
<li>日志都是某个时间点的字符串消息，通常需要对搜索引擎进行索引后，才能进行查询和汇总分析</li>
</ul>
<p>日志监控经典的方法是ELK技术栈，Elasticsearch、Logstash、Kibana三个组件的组合。</p>
<ul>
<li>Logstash负责从各个日志源采集日志，进行预处理，最后再把初步处理过的日志发送给Elasticsearch进行索引</li>
<li>Elasticsearch负责对日志进行检索，并提供一个完整的全文搜索引擎</li>
<li>Kibana负责对日志进行可视化分析，包括日志搜索、处理以及绚丽的仪表板展示</li>
</ul>
<p>ELK中logstash资源消耗较大，在资源紧张时往往使用Fluentd来替代，即EFK技术栈。采集端还可以使用filebeat，架构拓展为filebeat-kafka（zookeeper）-logstash或sparkstreaming-es，除了日志查询外可以做业务关联等。</p>
<h2 id="Lesson-55-套路篇：分析性能问题的一般步骤"><a href="#Lesson-55-套路篇：分析性能问题的一般步骤" class="headerlink" title="Lesson 55 套路篇：分析性能问题的一般步骤"></a>Lesson 55 套路篇：分析性能问题的一般步骤</h2><p>在收到监控系统的告警，发现系统戏院或者应用程序出现性能瓶颈，如何进一步分析根源？</p>
<h3 id="系统资源瓶颈"><a href="#系统资源瓶颈" class="headerlink" title="系统资源瓶颈"></a>系统资源瓶颈</h3><p>系统资源的瓶颈通过USE法，即使用率、饱和度和错误数三类指标来衡量。系统的资源可以分为硬件资源和软件资源两大类：</p>
<ul>
<li>CPU、内存、磁盘和文件系统以及网络等，都是常见的硬件资源</li>
<li>文件描述符、连接跟踪数、套接字缓冲区大小等都是典型的软件资源。</li>
</ul>
<p>在收到监控系统告警后就可以对照这些资源列表，再根据指标的不同来定位。</p>
<h4 id="CPU性能分析"><a href="#CPU性能分析" class="headerlink" title="CPU性能分析"></a>CPU性能分析</h4><p>利用top、vmstat、pidstat、strace以及perf等几个常见的工具，获得CPU性能指标后，再结合进程与CPU的工作原理，就可以迅速定位CPU性能瓶颈的来源。</p>
<p>top、vmstat、pidstat等工具所汇报的CPU性能指标都来源于/proc文件系统，这些指标都应该通过监控系统监控起来。 </p>
<p>比如当收到CPU使用率告警时，从监控系统中直接查询导致CPU使用率过高的进程，然后登陆到服务器分析该进程行为。可以使用strace查看进程的系统调用汇总，也可以使用perf找出热点函数，甚至可以使用动态追踪的方法来观察进程的当前执行过程，直到确定瓶颈个根源。</p>
<h4 id="内存性能分析"><a href="#内存性能分析" class="headerlink" title="内存性能分析"></a>内存性能分析</h4><p>通过free和vmstat输出的性能指标确认内存瓶颈，然后根据内存问题的类型，进一步分析内存的使用、分配、泄漏以及缓存等，最后找出问题的根源。</p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/mem_analysis.png" alt></p>
<p>内存的性能指标也来源于/proc文件系统，它们也都应该通过监控系统监控起来。如当收到内存不足的告警时，可以从监控系统中找出占用内存最多的几个进程。然后根据这些进程的内存占用历史，观察是否存在内存泄漏问题。确定可疑进程后，再登陆服务器分析该进程的内存空间或内存分配，查明原因。</p>
<h4 id="磁盘和文件系统I-O性能分析"><a href="#磁盘和文件系统I-O性能分析" class="headerlink" title="磁盘和文件系统I/O性能分析"></a>磁盘和文件系统I/O性能分析</h4><p>当使用iostat发现磁盘I/O存在性能瓶颈后，可以再通过pidstat、vmstat等确认I/O的来源。再根据来源的不同进一步分析文件系统和磁盘的使用率、缓存以及进程的I/O等，从而找出I/O问题所在。</p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/io.png" alt></p>
<p>磁盘和文件系统的性能指标也来源于/proc和/sys文件系统，也应该通过监控系统监控起来。</p>
<p>如果发现某块磁盘的I/O使用率为100%时，首先可以从监控系统中，找出I/O最多的进程。然后登陆服务器借助strace、lsof、perf等工具，分析该进程的I/O行为。最后再结合应用程序的原理找出大量I/O的原因。</p>
<h4 id="网络性能分析"><a href="#网络性能分析" class="headerlink" title="网络性能分析"></a>网络性能分析</h4><p>网络性能其实包含两类资源，网络接口和内核资源。网络分析要从Linux网络协议栈的原理来切入。</p>
<ul>
<li>链路层，从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分析；</li>
<li>网络层，从路由、分片、叠加网络等角度分析</li>
<li>传输层，从TCP、UDP的协议原理出发，从连接数、吞吐量、延迟重传等角度分析</li>
<li>应用层，从应用层协议、请求数、套接字缓存等角度进行分析</li>
</ul>
<p>网络的性能指标也都来源于内核，包括/proc文件系统、网络接口以及conntrack等内核模块，这些指标同样需要被监控系统监控。</p>
<p>例如，当收到网络不同的告警时，就可以从监控系统中查找各个协议层的丢包指标，确认丢包所在的协议层。然后从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等软硬件，是否存在性能瓶颈。最后再登录到服务器中，借助netstat、tcpdump、bcc等工具分析网络的收发数据，并且结合内核中的网络选项以及TCP等网络协议的原理找出问题所在。</p>
<h3 id="应用程序瓶颈"><a href="#应用程序瓶颈" class="headerlink" title="应用程序瓶颈"></a>应用程序瓶颈</h3><p>应用程序瓶颈本质来源有三种：<strong>资源瓶颈、依赖服务瓶颈、应用自身瓶颈</strong>。</p>
<p>资源瓶颈可以用前面的方法来分析。</p>
<p>依赖服务的瓶颈，也就是诸如数据库、分布式缓存、中间件等应用程序，直接或间接调用的服务出现了性能问题从而导致应用程序的响应变慢或者错误率升高。使用全链路跟踪系统可以帮助快速定位这类问题的根源。</p>
<p>应用程序自身的性能问题，包括了多线程处理不当、死锁、业务的复杂度过高等，这类问题可以通过应用程序指标监控以及日志监控中，观察关键环节的耗时和内部执行过程中的错误，帮助缩小问题范围。</p>
<p>不过应用程序内部的状态，外部通常不能直接获取详细的性能数据，需要应用程序在设计和开发时提供这些指标。</p>
<p>如果上述手段还是无法找出瓶颈，可以通过系统资源模块提供的各类进程分析工具来定位分析。 比如：</p>
<ul>
<li>strace观察系统调用</li>
<li>perf和火焰图分析热点函数</li>
<li>动态追踪技术分析进程的执行状态</li>
</ul>
<h2 id="Lesson-56-套路篇：优化性能问题的一般方法"><a href="#Lesson-56-套路篇：优化性能问题的一般方法" class="headerlink" title="Lesson 56 套路篇：优化性能问题的一般方法"></a>Lesson 56 套路篇：优化性能问题的一般方法</h2><h3 id="系统优化"><a href="#系统优化" class="headerlink" title="系统优化"></a>系统优化</h3><h4 id="CPU优化"><a href="#CPU优化" class="headerlink" title="CPU优化"></a>CPU优化</h4><p><strong>CPU性能优化的核心，在于排除所有不必要的工作、充分利用CPU缓存并减少进程程度对性能的影响。</strong></p>
<ul>
<li>把进程绑定到一个或者多个CPU上，充分利用CPU缓存的本地性，并减少进程间的相互影响。</li>
<li>为中断处理程序开启多CPU负载均衡，以便在发生大量中断时可以充分利用多CPU的优势分摊负载</li>
<li>使用cgroups等方法为进程设置资源限制，避免个别进程消耗过多的CPU。同时为核心应用程序设置更高的优先级，减少低优先级任务的影响</li>
</ul>
<h4 id="内存优化"><a href="#内存优化" class="headerlink" title="内存优化"></a>内存优化</h4><ul>
<li>除非有必要，Swap应该禁止掉。避免Swap的额外I/O，带来内存访问变慢的问题</li>
<li>使用Cgroups方法为进程设置内存限制。对于核心应用还应该降低oom_score，避免被OOM杀死</li>
<li>使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常</li>
</ul>
<h4 id="磁盘和文件系统I-O优化"><a href="#磁盘和文件系统I-O优化" class="headerlink" title="磁盘和文件系统I/O优化"></a>磁盘和文件系统I/O优化</h4><ul>
<li>通过SSD替代HDD、或者用RAID方法来提升I/O性能。</li>
<li>针对磁盘和应用程序I/O模式的特征，选择最合适的I/O调度算法。比如，SSD和虚拟机中的磁盘，通常用的是noop调度算法；数据库应用更推荐使用deadline算法</li>
<li>优化文件系统和磁盘的缓存、缓冲区，比如优化藏也的刷新频率、脏页限额，以及内核回收目录项缓存和索引节点缓存的倾向等</li>
</ul>
<h4 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h4><p>从内核资源和网络协议的角度：</p>
<ul>
<li>增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范围等内核资源配额</li>
<li>减少TIMEOUT超时时间、SYN+ACK重传数、Keepalive探测时间等异常参数处理</li>
<li>还可以开启端口复用、反向地址校验，并调整MTU大小等降低内核的负担</li>
</ul>
<p>从网络接口的角度：</p>
<ul>
<li>将原来CPU上执行的工作，卸载到网卡中执行，即开启网卡的GRO、GSO、RSS、VXLAN等卸载功能；</li>
<li>也可以开启网络接口的多队列功能，这样每个队列就可以用不用的中断号，调度到不同CPU上执行</li>
<li>增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量</li>
</ul>
<p>在极限性能情况下，内核的网络协议栈可能是最主要的性能瓶颈，所以一般考虑绕过内核协议栈。</p>
<ul>
<li>DPDK技术跳过内核协议栈，直接由用户态进程用轮询的方式，来处理网络请求。同时再结合大页、CPU绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率</li>
<li>内核自带的XDP技术，在网络包进入内核协议栈前，就对其进行处理。</li>
</ul>
<h4 id="应用程序优化"><a href="#应用程序优化" class="headerlink" title="应用程序优化"></a>应用程序优化</h4><p><strong>性能优化的最佳位置，还是应用程序内部</strong></p>
<ul>
<li>从CPU的角度来说，简化代码、优化算法、异步处理以及编译器优化等</li>
<li>从数据访问的角度，使用缓存、写时复制、增加I/O尺寸等都是常用的减少磁盘I/O的方法</li>
<li>从内存管理的角度，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能</li>
<li>从网络的角度，使用I/O多路复用，长连接代替短连接、DNS缓存等，可以优化网络I/O并减少网络请求数，从而减少网络延时带来的性能问题</li>
<li>从进程的工作模型来说，异步处理、多线程或多进程可以充分利用每一个CPU的处理能力，从而提高应用程序的吞吐能力</li>
</ul>
<p>还可以使用消息队列，CDN、负载均衡等各种方法来优化应用程序的架构，将原来单机要承担的任务调度到多台服务器中并行处理。</p>
<h2 id="Lesson-57-套路篇：Linux性能工具速查"><a href="#Lesson-57-套路篇：Linux性能工具速查" class="headerlink" title="Lesson 57 套路篇：Linux性能工具速查"></a>Lesson 57 套路篇：Linux性能工具速查</h2><h3 id="CPU性能工具"><a href="#CPU性能工具" class="headerlink" title="CPU性能工具"></a>CPU性能工具</h3><p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/cpu_metric.png" alt></p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/cpu_tools.png" alt></p>
<h3 id="内存性能工具"><a href="#内存性能工具" class="headerlink" title="内存性能工具"></a>内存性能工具</h3><p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/mem_metric.png" alt></p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/mem_tools.png" alt></p>
<h3 id="磁盘I-O性能工具"><a href="#磁盘I-O性能工具" class="headerlink" title="磁盘I/O性能工具"></a>磁盘I/O性能工具</h3><p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/io_metric.png" alt></p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/io_tools.png" alt></p>
<h3 id="网络性能工具"><a href="#网络性能工具" class="headerlink" title="网络性能工具"></a>网络性能工具</h3><p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/net_metric.png" alt></p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/net_tools.png" alt></p>
<h3 id="基准测试工具"><a href="#基准测试工具" class="headerlink" title="基准测试工具"></a>基准测试工具</h3><ul>
<li>在文件系统和磁盘I/O模块中，使用fio工具</li>
<li>在网络模块，使用iperf、pktgen等</li>
<li>在基于Nginx的案例中，使用ab、wrk等</li>
</ul>
<p>现在重新回看Brendan Gregg的这张Linux基准测试工具图谱，收获良多。</p>
<p><img src="/2020/07/19/Linux性能优化实战第八周--套路篇/performance_tool.png" alt></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/07/10/Linux性能优化实战第七周--综合实战篇/" itemprop="url">
                  《Linux 性能优化实战》第七周--综合实战篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-07-10T21:55:00+08:00" content="2020-07-10">
              2020-07-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>本周主要通过一些案例，对之前所学的知识进行复习和深化。首先容器化的应用程序性能分析，依旧可以使用之前的方法来分析和定位，不过要结合命名空间、cgroups、iptables等来综合分析。比如：</p>
<ul>
<li>cgroups影响容器应用的运行</li>
<li>iptables中的NAT会影响容器的网络性能</li>
<li>叠加文件系统，会影响应用的I/O性能</li>
</ul>
<p>对于网络丢包问题分析，要从Linux网络收发的流程入手，结合TCP/IP协议栈的原理来逐层分析。当碰到内核线程的资源使用异常时，很多常用的进程级性能工具并不能直接用到内核线程上。此时可以使用内核自带的perf来观察它们的行为，并找出热点函数，进一步定位性能瓶颈。不过perf汇总报告并不直观，可以通过火焰图来协助排查。</p>
<p>perf对系统内核线程进行分析时，内核线程依然还在正常运行，这种方法也被称为动态追踪技术。动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮助分析、定位想要排查的问题。在Linux系统中，常用的动态追踪方法包括ftrace、perf、eBPF/BCC以及SystemTap等。</p>
<ul>
<li>使用perf配合火焰图寻找热点函数，是一个比较通用的性能定位方法，在很多场景中都可以使用</li>
<li>如果仍然满足不了需求的话，在新版的内核中eBPF和BCC是最灵活的动态追踪方法</li>
<li>而在旧版本内核，特别是在RHEL系统中，由于eBPF支持受限，SystemTap和ftrace往往是更好的选择</li>
</ul>
<hr>
<p>接下来是本周读书笔记</p>
<hr>
<h2 id="Lesson-46-案例篇：为什么应用容器化后，启动慢了很多？"><a href="#Lesson-46-案例篇：为什么应用容器化后，启动慢了很多？" class="headerlink" title="Lesson 46 案例篇：为什么应用容器化后，启动慢了很多？"></a>Lesson 46 案例篇：为什么应用容器化后，启动慢了很多？</h2><p>本课主要学习如何分析应用程序容器化后的性能问题。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><pre><code>sudo docker run --name tomcat --cpus 0.1 -m 512M -p 8080:8080 -itd feisky/tomcat:8
curl localhost:8080
</code></pre><p>容器内核心应用逻辑比较简单，申请一个256M的内存然后输出“HelloWorld”。等待容器启动后运行curl命令给出了结果“HelloWorld”，但是随后出现Empty reply from server一直connection refused。</p>
<p>查看tomcat log并没有发现问题并且容器状态为Exited，此时利用docker inspect查看容器信息，发现State信息中OOMKilled为true，说明容器是被OOM杀死。但是我们已经指定了-m为512M正常不会遇到OOM问题。</p>
<p>此时利用dmesg命令查看系统日志，定位OOM相关日志，可以看到输出显示：</p>
<ul>
<li>mem_cgroup_out_of_memory，超出了cgroup内存限制</li>
<li>java进程在容器内运行，容器内存的使用和限制都是512M，当前使用量已经超过该限制</li>
<li>被杀死的进程，虚拟内存为4.3G，匿名内存页为505M，页内存为19M。</li>
</ul>
<p>分析可知，Tomcat容器的内存主要用在了匿名内存，其实就是主动申请分配的堆内存。Tomcat是基于JAVA开发，自然想到JVM堆内存配置问题。</p>
<pre><code>重新启动容器，执行下列命令查看JVM堆配置
sudo docker exec tomcat java -XX:+PrintFlagsFinal -version | grep HeapSize
sudo docker exec tomcat free -m #容器内部看到的仍然是主机内存
</code></pre><p>看到初始堆内存大小InitialHeapSize为126MB，最大堆内存大小为1.95GB，比容器限制要大。因为容器看不到该配置，虽然在启动容器时设置了内存限制，但是并不影响JVM使用。</p>
<p><strong>解决方法：</strong>在运行容器时加上 -e JAVA_OPT=“-Xmx512m -Xms512m”限制JVM的初始内存和最大内存即可</p>
<p>重新启动容器后，查看tomcat log发现能正常启动，但是启动时间需要22s。</p>
<p>再次重启容器并使用top来观察输出，发现机器中CPU使用率并不高且内存也非常充足，再看进程上Java进程的CPU使用率为10%，内存使用率0.9%。 其他进程使用率几乎可以忽略。</p>
<p>继续重启容器，top拿到JAVA进程PID之后，再用pidstat分析该进程，发现虽然CPU使用率很低，只有10%，但是wait%却非常高达到了87%，说明线程大部分时间都在等待调度，没有真正运行。 </p>
<p>再看我们运行容器时限制了–cpus 0.1,限制了CPU使用。将该值增加大1再重启此时只需要2s即可完成。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在容器平台中最常见的一个问题就是刚开始图省事不进行资源限制，当容器数量增长之后就会经常出现各种异常问题，最终查下来可能就是某个应用资源使用过高，导致整台机器短时间无法响应。因此使用Docker运行Java应用时一定要确保设置容器资源限制的同时，配置好JVM选项。 也可以升级JAVA版本到JAVA10，即可自动解决类似问题。</p>
<h2 id="Lesson-47-48-案例篇：服务器总是时不时丢包怎么办？"><a href="#Lesson-47-48-案例篇：服务器总是时不时丢包怎么办？" class="headerlink" title="Lesson 47/48 案例篇：服务器总是时不时丢包怎么办？"></a>Lesson 47/48 案例篇：服务器总是时不时丢包怎么办？</h2><p><strong>丢包率是网络性能中最核心的指标之一</strong></p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>本次实验案例是一个Nginx应用，hping3和curl是Nginx的客户端。</p>
<pre><code>sudo docker run --name nginx --hostname nginx --privileged -p 80:80 -itd feisky/nginx:drop
hping3 -c 10 -S -p 80 XXX.XXX.XXX.XXX
</code></pre><p>因为Nginx使用的是TCP协议，而ping是基于ICMP协议的，因此我们用hping3来测试。此时输出显示10个请求包值收到5个回复，每个请求的RTT波动较大，小的只有3ms大的则有3s左右。可以猜测3s左右的RTT是丢包重传导致。</p>
<p>从图中可以看出可能发生丢包的位置，实际上贯穿了整个网络协议栈：</p>
<ul>
<li>在两台VM连接之间，可能会发生传输失败的错误。如网络阻塞、线路错误等</li>
<li>网卡收包后，环形缓冲区因溢出而丢包</li>
<li>链路层，会因为网络帧校验失败、QoS等丢包</li>
<li>IP层，会因为路由失败、组包大小超过MTU等而丢包</li>
<li>传输层，因为端口未监听、资源占用超过内核限制丢包</li>
<li>套接字层，因为套接字缓冲区溢出而丢包</li>
<li>应用层，应用程序异常而丢包</li>
<li>此外如果配置了iptables规则，可能因为过滤规则而丢包</li>
</ul>
<p>因为VM2只是一个hping3命令，为了简化排查同时假设VM1的网络和内核配置也没问题。因此可能发生问题的地方就是容器内部了。进入容器内部逐层排查丢包原因。</p>
<h4 id="链路层分析"><a href="#链路层分析" class="headerlink" title="链路层分析"></a>链路层分析</h4><p>首先查看<strong>链路层</strong>，通过ethtool/netstat查看网卡的丢包记录,从输出中没有发现任何错误，说明容器的虚拟网卡没有丢包。（<strong>注意：如果tc等工具配置了QoS，tc规则导致额丢包不会包含在网卡的统计信息中</strong>）接下来检查eth0是否配置了tc规则，并查看有无丢包。</p>
<pre><code>sudo docker exec -ti nginx /bin/bash
netstat -f

tc -s qdisc show dev eth0
qdisc netem 800d: root refcnt 2 limit 1000 loss 30%
 Sent 432 bytes 8 pkt (dropped 4, overlimits 0 requeues 0)
</code></pre><p>此时tc规则中看到，eth0上面配置了一个网络模拟排队规则qdisc netem，并且配置了丢包率为30%。后面统计信息显示发送了8个包，但是丢了4个。</p>
<p>发现这点问题之后，直接删掉netem模块即可。</p>
<pre><code>tc qdisc del dev eth0 root netem loss 30%
</code></pre><p>此时再次执行hping3命令，发现还是50%的丢包，RTT的波动也很大，从3ms到1s。</p>
<h4 id="网络层和传输层"><a href="#网络层和传输层" class="headerlink" title="网络层和传输层"></a>网络层和传输层</h4><p>在容器内部继续执行netstat -s命令，可以看到各协议的收发汇总，以及错误信息。 输出表明只有TCP协议发生了丢包和重传。TCP协议有多次超时和失败重试，并且主要错误是半连接充值。即主要失败都是三次握手失败。</p>
<ul>
<li>11 failed connection attempts</li>
<li>4 sgements retransmitted</li>
<li>11 resets received for embryonic SYN_RECV sockets</li>
<li>4 TCPSynRetrans</li>
<li>7 TCPTimeouts</li>
</ul>
<h4 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h4><p>因为iptables和内核的连接跟踪机制也可能会导致丢包，因此也需要进行排查。要确定是不是说连接跟踪导致的问题，只需要对比当前的连接跟踪数和最大连接跟踪数即可。由于连接跟踪在内核中是全局的，因此需要在主机中查看。</p>
<pre><code>sysctl net.netfilter.nf_conntrack_max
</code></pre><p>此时当前连接跟踪数远小于最大连接跟踪数，因此丢包不可能是连接跟踪导致。</p>
<p>接下来回到容器内部查看iptables的过滤规则，发现有两条DROP规则的统计数值不是0，分别是在INPUT和OUTPUT链中。这两条规则是一样的，指的是使用statistic模块进行随机30%的丢包。删除这两条规则即可。</p>
<pre><code>iptables -t filter -nvL
    Chain INPUT
    pkts  bytes  target  prot  opt  in  out  source      destination
    6     240    DROP    all   --   *    *   0.0.0.0/0   0.0.0.0/0   statistic mode random probability 0.299999999981

    Chain FORWARD
   pkts  bytes  target  prot  opt  in  out  source      destination

    Chain OUTPUT
    pkts  bytes  target  prot  opt  in  out  source      destination
    6     264    DROP    all   --    *   *   0.0.0.0/0   0.0.0.0/0   statistic mode random probability 0.299999999981

iptables -t filter -D INPUT -m statistic --mode random --probability 0.30 -j DROP
iptables -t filter -D OUTPUT -m statistic --mode random --probability 0.30 -j DROP  
</code></pre><p>再用hping3验证此时80端口接发包正常。下面用curl命令检查Nginx对HTTP请求的响应：</p>
<pre><code>curl --max-time 3 http://XXX.XXX.XXX.XXX
    curl:(28) Operation timed out after 3000 milliseconds with 0 bytes received
</code></pre><p>这时候可以tcpdump转包来分析：</p>
<pre><code>tcpdump -i eth0 -nn port 80
</code></pre><p>从结果中可以看出，前三个包是正常的TCP三次握手，但是第四个包确实在3s之后，并且还是客户端VM2发送来的FIN包，说明客户端的连接关闭了。因为curl命令设置了3s超时选项，所以这种情况是因为curl命令超时后退出。</p>
<p>重新执行netstat -i命令查看网卡有没有丢包问题，输出显示RX-DRP是344，即网卡接收时丢包了。但是之前hping3不丢包，现在换成curl GET却丢包，我们需要对比下这两个工具。</p>
<ul>
<li>hping3只发送SYN包</li>
<li>curl在发送SYN包之后，还会发送HTTP GET 请求</li>
</ul>
<p>HTTP GET本质上也是一个TCP包，但是和SYN包相比，它还携带了HTTP GET的数据。这时候就容易想到时MTU配置错误导致。查看eth0的MTU设置只有199，将其改为以太网默认值1500即可。</p>
<pre><code>ifconfig eth0 mtu 1500
</code></pre><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>遇到网络丢包问题，要从Linux网络收发的流程入手，结合TCP/IP协议栈的原理来逐层分析。</p>
<h2 id="Lesson-49-案例篇：内核线程CPU利用率太高怎么办？"><a href="#Lesson-49-案例篇：内核线程CPU利用率太高怎么办？" class="headerlink" title="Lesson 49 案例篇：内核线程CPU利用率太高怎么办？"></a>Lesson 49 案例篇：内核线程CPU利用率太高怎么办？</h2><p>CPU使用率较高的内核线程，如果用之前的分析方法，一般需要借助于其他性能工具进行辅助分析。本节提供了一种直接观察内核线程的行为，更快定位瓶颈的方法。</p>
<h3 id="内核线程"><a href="#内核线程" class="headerlink" title="内核线程"></a>内核线程</h3><p>Linux中用户态进程的”祖先“都是PID为1的init进程，即systemd进程。但是systemd只管理用户态进程，那么内核态线程是有谁来管理呢？</p>
<ul>
<li>0号进程为idle进程，系统创建的第一个进程，它在初始化1号和2号进程后，演变为空闲任务。</li>
<li>1号进程为init进程，即systemd进程，在用户态运行，用来管理其他用户态进程</li>
<li>2号进程为kthreadd进程，在内核态运行用来管理内核线程。</li>
</ul>
<p>所以要查找内核线程，只需要从2号进程开始，查找它的子孙进程即可</p>
<pre><code>ps -f --ppid 2 -p 2 
#可以看出内核线程的名称都在中括号内，因此更简单的方法是直接查找名称中包含中括号的进程

ps -ef | grep &apos;\[.*\]&apos;
</code></pre><ul>
<li><strong>ksoftirqd </strong>软中断</li>
<li><strong>kswapd0</strong>， 用于内存回收</li>
<li><strong>kworker</strong>，用于执行内核工作队列，分为绑定CPU和未绑定CPU两大类</li>
<li><strong>migration</strong>，用于负载均衡中，把进程迁移到CPU上，每个CPU都有一个migration内核线程</li>
<li><strong>jbd2/sda1-8</strong>， Journaling Block Device，用来为文件系统提供日志功能，以保证数据的完整性</li>
<li><strong>pdflush</strong>，用于将内存中的脏页写入磁盘</li>
</ul>
<h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>运行一个nginx容器，通过curl命令验证nginx服务正常开启。用hping3命令模拟Nginx客户端请求，此时回到第一个终端，发现系统响应变慢。</p>
<p>用top观察发现2个CPU上软中断使用率都超过了30%，正好是软中断内核线程ksoftirqd/0和ksoftirqd/1。对于内核线程我们用stace、pstack、lsof无法查看详细的调用栈情况，此时可以用内核提供的工具来分析。</p>
<pre><code>perf record -a -g -p $pid --sleep 30
perf report
</code></pre><p>后续利用火焰图来协助排查分析定位热点函数，找出潜在的性能问题。</p>
<h2 id="Lesson-50-51-案例篇：动态追踪怎么用？"><a href="#Lesson-50-51-案例篇：动态追踪怎么用？" class="headerlink" title="Lesson 50/51 案例篇：动态追踪怎么用？"></a>Lesson 50/51 案例篇：动态追踪怎么用？</h2><p><strong>动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。</strong></p>
<p>Dtrace的工作原理：它的运行常驻在内核中，用户可以用dtrace命令，把D语言编写的追踪脚本，提交到内核中的运行时来执行。Dtrace可以跟踪用户态和内核态的所有事件，并通过一些列的优化措施，保证最小的性能开销。</p>
<p>Dtrace本身依然无法在Linux中运行，很多工程师都尝试过把Dtrace移植到Linux中，其中最著名的就是RedHat主推的SystemTap。</p>
<p>SystemTap也定义了一种类似的脚本语言，方便用户根据需要自由扩展。不过SystemTap没有常驻内核运行时，需要先把脚本编译为内核模块，然后再插入到内核中执行。因此systemTap启动比较慢，并且依赖于完整的调试符号表。</p>
<p>总的来说，为了追踪内核或用户空间的事件，Dtrace和SystemTap都会把用户传入的追踪处理函数，关联到被称为探针的检测点上。这些探针实际上也就是各种动态追踪技术所依赖的事件源。</p>
<p>根据事件类型不同，动态追踪所使用的事件源，可以分为<strong>静态探针、动态探针以及硬件事件</strong>三类。</p>
<ul>
<li><strong>硬件事件</strong>通常由性能监控计数器PMC产生，包含了各种硬件的性能情况，比如CPU的缓存、指令周期、分支预测等；</li>
<li><strong>静态探针</strong>，是指实现在代码中定义好，并编译到应用程序或者内核中的探针。这些探针只有在开启探测功能时才会被执行到。<ul>
<li>跟踪点 tracepoints，实际上就是在源码中插入的一些带有控制条件的探测点，这些探测点允许时候再添加处理函数。如内核中的printk</li>
<li>USDT探针，全称时用户级静态定义跟踪，需要在源码中插入DTRACE_PROBE()代码，并编译到应用程序中。MYSQL/PostgreSQL也内置了USDT探针</li>
</ul>
</li>
<li><p><strong>动态探针</strong>，指没有事先在代码中定义，但却可以在运行时动态添加的探针。常见的动态探针都两种：</p>
<ul>
<li>kprobes用来跟踪内核态的函数，包括用于函数调用的kprobe和用于函数返回的kretprobe</li>
<li><p>uprobes用来跟踪用户态的函数，包括用于函数调用的uprobe和用于函数返回的uretprobe</p>
<p>  kprobes需要内核编译时开启CONFIG_KPROBE_EVENTS，uprobes需要内核编译中开启CONFIG_UPROBE_EVENTS</p>
</li>
</ul>
</li>
</ul>
<h3 id="动态追踪机制"><a href="#动态追踪机制" class="headerlink" title="动态追踪机制"></a>动态追踪机制</h3><p>在探针基础上，Linux也提供了一系列的动态追踪机制，比如ftrace、perf、eBPF等</p>
<ul>
<li>ftrace最早用于函数跟踪，后来又扩展支持了各种事件跟踪功能。</li>
<li>perf是一种最简单的静态跟踪机制，也可以通过perf来定义动态事件，只关注真正感兴趣的事件</li>
<li>eBPF是在BPF（Berkeley Packet Filter）的基础上扩展来的，不仅支持事件跟踪机制，还可以通过自定义的BPF代码</li>
</ul>
<p>除此之外，还有很多内核外的工具，也提供了丰富的动态追踪功能，最常见的就是SystemTap和BCC，以及常用于容器性能分析的sysdig</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/07/04/Linux性能优化实战第六周--网络性能篇实战/" itemprop="url">
                  《Linux 性能优化实战》第六周--网络性能实战篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-07-04T17:26:55+08:00" content="2020-07-04">
              2020-07-04
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>本周主要从案例分析来学习Linux网络问题如何分析与解决，这也是这个专栏四个基础模块的最后一小部分。对于网络性能评估，一般情况需要从上到下对每个协议层进行性能测试，然后根据性能测试结果结合Linux网络协议栈原理，找出导致性能瓶颈的根源。</p>
<p>在优化网络的性能时，可以结合Linux系统的网络协议栈和网络收发流程，从应用程序、套接字、传输层、网络层再到链路层等，对每个层进行逐层优化。</p>
<ul>
<li>应用程序中，主要优化I/O模型、工作模型以及应用层的网络协议</li>
<li>套接字层，主要优化套接字的缓冲区大小</li>
<li>传输层，主要优化TCP和UDP协议</li>
<li>网络层，主要优化路由、转发、分片以及ICMP协议</li>
<li>链路层，主要优化网络包的收发、网络功能卸载以及网卡选项</li>
</ul>
<p>对于DDoS攻击，由于其分布式、大流量、难追踪等特点，目前还无法完全御防，只能设法缓解DDoS带来的影响。在实际应用中通常让Linux服务器配合专业的流量清洗以及网络防火墙设备一起来缓解该问题。</p>
<h3 id="新工具GET"><a href="#新工具GET" class="headerlink" title="新工具GET"></a>新工具GET</h3><pre><code>网络流量分析：
确认网络包的收发是否正常
tcpdump Wireshark
确认单次请求和并发请求时的网络延迟是否正常
hping3，wrk
确认路由是否正确，并查看路由中每一跳网关的延迟
traceroute
观察应用程序对网络套接字的调用情况是否正常
strace
Linux动态追踪框架
SystemTap
</code></pre><hr>
<p>接下来是本周读书笔记</p>
<hr>
<h2 id="Lesson-37-案例篇：DNS解析时快时慢应该怎么办？"><a href="#Lesson-37-案例篇：DNS解析时快时慢应该怎么办？" class="headerlink" title="Lesson 37 案例篇：DNS解析时快时慢应该怎么办？"></a>Lesson 37 案例篇：DNS解析时快时慢应该怎么办？</h2><h3 id="域名与DNS解析"><a href="#域名与DNS解析" class="headerlink" title="域名与DNS解析"></a>域名与DNS解析</h3><pre><code>cat /etc/resolv.conf
nameserver 8.8.8.8
</code></pre><p>除了nslookup之外，另一个常用的DNS解析工具dig，还提供了trace功能，可以展示递归查询的整个过程。</p>
<pre><code>dig +trace +ndnssec xiaozhazi.github.io #nodnssec表示禁止DNS安全扩展
</code></pre><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="DNS解析失败"><a href="#DNS解析失败" class="headerlink" title="DNS解析失败"></a>DNS解析失败</h4><pre><code>sudo docker run -it --rm -v $(mktemp):/etc/resolv.conf feisky/dnsutils bash 
nslookup xiaozhazi.github.io  
#connection timeout, no servers could be reached
ping -c3 8.8.8.8 # works normally

nslookup -debug xiaozhazi.github.io 
</code></pre><p>发现并没有连接DNS服务器而是连接环回地址，此时猜测可能容器内部没有配置DNS服务器。 在resolv.conf文件中添加即可。</p>
<h4 id="DNS解析不稳定"><a href="#DNS解析不稳定" class="headerlink" title="DNS解析不稳定"></a>DNS解析不稳定</h4><pre><code>sudo docker run -it --rm --cap-add=NET_ADMIN --dns 8.8.8.8 feisky/dnsutils bash 
time nslookup time.geekbang.org  # real time=10s
ping -c3 8.8.8.8  #latency=140ms
ping -c3 114.114.114.114 #latency=31ms， change dnsserver
#rerun nslookup now timecost=64ms
#此时重复执行仍会出现1s延时的情况，说明容器内没有使用DNS缓存

/etc/init.d/dnsmasq start 
然后修改resolv.conf文件，将DNS服务器改为dnsmasq的监听地址
此时再重复执行nslookup除第一次运行外，都只需10ms左右
</code></pre><p>DNS解析结果不稳定，可能存在以下几种情况：</p>
<ul>
<li>DNS服务器本身有问题，响应慢且不稳定</li>
<li>客户端到DNS服务器的网络延迟较大</li>
<li>DNS请求或响应包，在某些情况下被链路中的网络设备弄丢了</li>
</ul>
<p>几种常见的DNS优化方法：</p>
<ul>
<li>对DNS解析的结果进行缓存</li>
<li>对DNS解析的结果进行预取</li>
<li>使用HTTPDNS取代常规的DNS解析，使用HTTP协议栈绕过链路中的DNS服务器，可以避免域名被劫持的问题</li>
<li>基于DNS的全局负载均衡GSLB，根据用户的位置返回距离最近的IP地址</li>
</ul>
<h2 id="Lesson-38-案例篇：怎么使用tcpdump和Wireshark分析网络流量"><a href="#Lesson-38-案例篇：怎么使用tcpdump和Wireshark分析网络流量" class="headerlink" title="Lesson 38 案例篇：怎么使用tcpdump和Wireshark分析网络流量"></a>Lesson 38 案例篇：怎么使用tcpdump和Wireshark分析网络流量</h2><p>我们通常使用ping来测试服务延迟，不过有时候ping本身也会出现意想不到的问题，此时就需要我们抓取ping命令执行时收发的网络包，然后分析这些网络包，进而找出问题根源。</p>
<ul>
<li>tcmdump仅支持命令行格式使用，常用在服务器中抓取和分析网络包</li>
<li>Wireshark还提供了图形界面和汇总分析工具，在分析复杂的网络场景是比较实用</li>
</ul>
<h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><pre><code>#禁止接收从DNS服务器中发送过来并包含googleusercontent的包
iptables -I INPUT -p udp --sport 53 -m string --string googleusercontent --algo bm -j DROP
ping -c3 geektime.org

#此时三次请求都得到了响应，每次延迟30ms左右，没有丢包。但是总时间却超过了11s
#是不是DNS解析慢的原因呢？发现ping的输出中三次都是用的IP地址，说明ping只需要在最开始运行时解析一次得到IP
#用nslookup验证了下不存在域名解析慢的问题

tcpdump -nn udp port 53 or host XXX(geektime.org ip)
#另一个终端执行ping指令 
</code></pre><p>逐条分析tcpdump输出，发现有两条反向地址解析PTR请求，只看到了请求包没有应答包。而且每条记录都执行了5s才出现下一个网络包。</p>
<p>因此这里的ping缓慢是因为两次PTR请求超时导致的， 在ping执行时禁掉PTR即可</p>
<pre><code>ping -n -c3 geektime.org
</code></pre><h3 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a>tcpdump</h3><p>tcpdump 基于libpcap，利用内核中的AF_PACKET套接字，抓取网络接口中传输的网络包，并提供了请打的过滤规则，从大量的网络包中挑出最想关注的信息。</p>
<pre><code>-i   #tcpdump -i eth0  指定网络接口
-nn  #tcpdump -nn   不解析IP地址和端口号的名称 
-c   #tcpdump -c5   限制要抓取网络包的个数
-A   #tcpdump -A    以ASSCII格式显示网络包内容
-w   #tcpdump -w file.pcap  保存到文件中，通常以pcap作为后缀
-e   #tcpdump -e    输出链路层的头部信息
</code></pre><h2 id="Lesson-39-案例篇：怎么缓解DDoS攻击带来的性能下降问题？"><a href="#Lesson-39-案例篇：怎么缓解DDoS攻击带来的性能下降问题？" class="headerlink" title="Lesson 39 案例篇：怎么缓解DDoS攻击带来的性能下降问题？"></a>Lesson 39 案例篇：怎么缓解DDoS攻击带来的性能下降问题？</h2><h3 id="DDoS简介"><a href="#DDoS简介" class="headerlink" title="DDoS简介"></a>DDoS简介</h3><p>DDoS， Distributed Denial of Service。 前身是DoS，即拒绝服务攻击，指利用大量的合理请求，来占用过多的目标资源，从而使得目标服务无法响应正常请求。</p>
<p>DDoS则是采用的分布式架构，利用多台主机同时攻击目标主机。这样，即使目标服务部署了网络防御设备，面对大量网络请求时还是无力应对。目前已知的最大流量攻击正是Github遭受的DDoS攻击，峰值流量达到了1.35Tbps，PPS更是超过了1.2亿。</p>
<p>从攻击原理来看，DDoS分为：</p>
<ul>
<li>耗尽带宽。无论是服务器还是路由器、交换机等网络设备，带宽都有固定的上限。带宽耗尽后就会发生网络拥堵，无法传输其他正常的网络报文。</li>
<li>耗尽操作系统资源。例如CPU、内存等物理资源，以及连接表等软件资源。</li>
<li>消耗应用程序的运行资源。应用程序的运行，通常要和其他的资源或系统交互，如果程序一直忙于处理无效请求，也会导致正常请求的处理变慢甚至无法响应</li>
</ul>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>通过hping3命令模拟DoS攻击：</p>
<pre><code>hping3 -S -p 80 -i u10 XXX.XXX.XXX.XXX
#-S表示设置TCP协议的SYN
</code></pre><p>用Sar来观察，可以看到网络接收的PPS（每秒收发的报文数）已经达到了2w多，但是BPS（每秒收发的字节数）只有1174KB，即每个包只有54B。全是小包 </p>
<pre><code>sar -n DEV 1
</code></pre><p>继续通过tcpdump抓取eth0网卡的包</p>
<pre><code>tcpdump -i eth0 -n tcp port 80
</code></pre><p>Flag[S]表示是SYN包，大量的SYN包表明，这是一个SYN Flood攻击。通过Wireshark可以更直观的输出SYN Flood的过程。</p>
<p>其原理是：</p>
<ul>
<li>客户端构造大量的SYN包，请求建立TCP连接</li>
<li>服务器收到包后，向源IP发送SYN+ACK报文，并等待三次握手的最后一次ACK报文，直到超时</li>
</ul>
<p>这种等待状态的TCP连接，通常称为半开连接。由于连接表的大小有限，大量的半开连接就会导致连接表迅速占满，从而无法建立新的TCP连接。</p>
<pre><code>netstat -n -p | grep SYN_REC #定位半开连接的IP
iptables -I INPUT -s XXX.XXX.XXX.XXX -p tcp -j REJECT
</code></pre><p>如果遇到多台服务器同时发送SYN Flood攻击，这种方法可能就无效了。因为很可能无法SSH到机器上，因此提前要对系统做一些TCP，限制半开连接的数量/减少连接失败内核重启次数。</p>
<pre><code># /etc/sysctl.conf
sysctl -w net.ipv4.tcp_max_syn_backlog=1024
sysctl -w net.ipv4.tcp_synack_retries=1
</code></pre><p>还可以启用TCP SYN Cookies来防御SYN Flood攻击。</p>
<pre><code>sysctl -w net.ipv4.tcp_syncookies=1
</code></pre><h3 id="DDoS到底该如何防御"><a href="#DDoS到底该如何防御" class="headerlink" title="DDoS到底该如何防御"></a>DDoS到底该如何防御</h3><ul>
<li>可以用XDP或者DPDK构建DDoS方案，在内核网络协议栈前，或者跳过内核协议栈来识别并丢弃DDoS报文</li>
<li>对于流量型的DDoS，当服务器的带宽被耗尽时，服务器内部处理就无能为力了。此时只能在服务器外部的网络设备中增加专业的入侵检测和防御设备，配置流量清洗设备阻断恶意流量等。</li>
<li>对于慢速请求，响应流量很大时使得应用程序会耗费大量的资源处理，此时需要应用程序考虑识别，并尽早拒绝掉这些恶意流量。比如合理利用缓存，增加WAF(Web Application Firewall),使用CDN等。</li>
</ul>
<h2 id="Lesson-40-案例篇：网络请求延迟变大了，该怎么办？"><a href="#Lesson-40-案例篇：网络请求延迟变大了，该怎么办？" class="headerlink" title="Lesson 40 案例篇：网络请求延迟变大了，该怎么办？"></a>Lesson 40 案例篇：网络请求延迟变大了，该怎么办？</h2><h3 id="网络延迟"><a href="#网络延迟" class="headerlink" title="网络延迟"></a>网络延迟</h3><p><strong>网络延迟</strong> 网络数据传输所用的时间，这个时间可能单向也可以指双向的。 双向的往返通道延迟，RTT Round-Trip Time</p>
<p><strong>应用程序延迟</strong> 从应用程序接收请求到发回响应全程所用的时间。</p>
<p>通常用ping来测试网络延迟，但是ping基于ICMP通过计算ICMP回显响应报文和回显请求报文的时间差，来获得延时。这个过程不需要特殊认证，通常会被很多网络攻击利用。为了避免被攻击，很多网络服务会把ICMP禁掉。此时可以借助traceroute和hping3工具。</p>
<pre><code>hping3 -c 3 -S -p 80 baidu.com
# -c表示发送3次，-S设置TCPSYN，-p端口号
traceroute --tcp -p 80 -n baidu.com
# -n表示不对结果中的IP地址执行反向域名解析
</code></pre><h3 id="实验分析-1"><a href="#实验分析-1" class="headerlink" title="实验分析"></a>实验分析</h3><p>设计了对比实验，在80端口运行官方Nginx容器，在8080端口运行案例Nginx容器</p>
<pre><code>sudo docker run --network=host --name=good -itd nginx
sudo docker run --network=host --name=nginx -itd feisky/nginx:latency
</code></pre><p>通过hping3命令分别测试其延迟时，发现差不多都是7ms。</p>
<p>再用wrk测试并发请求下的延迟，分别测试机器并发100时两个端口的性能</p>
<pre><code>wrk --latency -c 100 -t 2 --timeout 2 http://IP:Port
</code></pre><p>此时官方Nginx的延迟在9ms左右，而案例应用则是44ms左右。 此时我们首先想到的是通过tcpdump抓取8080端口的网络包，并保存文件到nginx.pcap</p>
<pre><code>tcpdump -nn tcp port 8080 -w nginx.pcap
wrk --latency -c 100 -t 2 --timeout 2 htto://IP:Port 
</code></pre><p>再把抓取到的nginx.pcap文件复制到装有Wireshark的机器中进行分析，此时只过滤除TCP Stream的。 通过输出界面可以看出三次握手和第一次请求和响应都挺快，但是第二次请求就比较慢，40ms之后才发送ACK响应。</p>
<p>而TCP延迟确认（Delayed ACK）的最小超时时间就是40ms。 <strong>延迟确认</strong>是针对TCP ACK机制的一种优化，不用每次请求都发送一个ACK，而是等一会看看有没有其他包需要发送，捎带着ACK一起发送过去。如果等不到就在超时后单独发送ACK。</p>
<p>man TCP，发现TCP可以设置TCP_QUICKACK开启快速确认模式，否则默认采用延迟确认机制。</p>
<p>为了验证猜想，用strace观察wrk为套接字设置了哪些TCP选项,证明确实没有TCP_QUICKACK</p>
<pre><code>strace -f wrk --latency -c 100 -t 2 --timeout 2 http://IP:Port
&apos;&apos;&apos;
setsockopt(52,SOL_TCP,TCP_NODELAY,[1],4)=0
&apos;&apos;&apos;
</code></pre><p>但是这只是客户端的行为，按理说Nginx服务器不应该受此影响，再回过去分析网络包，重新观察Wireshark输出。发现第二个分组是等到客户端第一个分组的ACK后才发送的，有点类似延迟确认，不过此时不是ACK包，而是发送数据。</p>
<p>此时考虑<strong>Nagle算法</strong>，纳格算法，是TCP协议中用于减少小包发送数量的一种优化算法，目的是为了提高实际带宽利用率。算法规定一个TCP连接上，最多只能有一个未确认的未完成分组，在收到这个分组的ACK之前，不发送其他分组。这些小分组会被组合起来，并在收到ACK后，用同一个分组发送出去。</p>
<p>Nagle算法和Linux默认的延迟确认机制一起使用后，网络延迟会非常明显。 TCP可以设置TCP_NODELAY来禁用掉Nagle算法。</p>
<pre><code>sudo docker exec nginx cat /etc/nginx/nginx.conf | grep tcp_nodelay
    tcp_nodelay off;
</code></pre><p>将其设置为on即可。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>遇到网络延迟增大问题时，可以通过以下工具来定位网络中的潜在问题：</p>
<ul>
<li>hping3，wrk确认单次请求和并发请求时的网络延迟是否正常</li>
<li>traceroute 确认路由是否正确，并查看路由中每一跳网关的延迟</li>
<li>tcpdump和Wireshark 确认网络包的收发是否正常</li>
<li>strace观察应用程序对网络套接字的调用情况是否正常</li>
</ul>
<h2 id="Lesson-41-42-如何优化NAT性能？"><a href="#Lesson-41-42-如何优化NAT性能？" class="headerlink" title="Lesson 41/42 如何优化NAT性能？"></a>Lesson 41/42 如何优化NAT性能？</h2><h3 id="NAT原理"><a href="#NAT原理" class="headerlink" title="NAT原理"></a>NAT原理</h3><p>NAT，Network Address Tranlation，可以重写IP数据包的源IP或者目的IP，被普遍用来解决公网IP地址短缺的问题。原理是，网络中的多台主机通过共享一个公网IP地址，来访问外网资源。</p>
<ul>
<li>静态NAT，内网IP和公网IP一对一永久映射关系</li>
<li>动态NAT，内网IP从公网IP池中动态选择一个进行映射</li>
<li>网络地址端口转换NAPT，Network Address and Port Translation，即把内网IP映射到公网IP的不同端口上，让多个内网IP可以共享同一个公网地址</li>
</ul>
<p>NAPT是目前最流行的NAT类型，根据转换方式又分为三类：</p>
<ul>
<li>源地址转换SNAT，目的地址不变只替换源IP或者源端口</li>
<li>目的地址转换DNAT，源IP保持不变只替换目的IP或目的端口</li>
<li>双向地址转换，当接收网络包时执行DNAT，将目的地址转换为内网IP，发送网络包时执行SNAT，把源IP替换为外部IP</li>
</ul>
<p>比如，本地服务器IP为192.168.0.2，NAT网关IP为100.100.100，目的服务器baidu.com地址为123.125.115.110</p>
<ul>
<li>服务器访问baidu.com时，NAT地址会把源地址从本地服务器IP替换为网关IP，然后才发送给baidu.com</li>
<li>baidu.com发回响应包时，NAT网关又把目的地址替换为本地服务器IP，然后发送给目的服务器</li>
</ul>
<h3 id="iptables与NAT"><a href="#iptables与NAT" class="headerlink" title="iptables与NAT"></a>iptables与NAT</h3><p>Linux内核提供的Netfiler框架，允许对网络数据包进行修改和过滤。 以及iptables、ip6tables、ebtables等工具。</p>
<p>NAT表中内置了三个链：</p>
<ul>
<li>PREROUTING，路由判断前所执行的规则，比如，对接收到的数据包进行DNAT</li>
<li>POSTROUTING，路由判断后所执行的规则，比如，对发送或转发的数据包进行SNAT或MASQUERADE</li>
<li>OUTPUT，类似于PREROUTING，但只处理从本机发送出去的包</li>
</ul>
<p>SNAT配置需要在NAT表中的POSTROUTING链中配置： </p>
<ul>
<li><ol>
<li>为一个子网统一配置SNAT，并由Linux选择默认的出口IP，即MASQUERAGE      <em>iptables -t nat -A POSTROUTING -s 192.168.0.0/16 -j MASQUERADE</em></li>
</ol>
</li>
<li><ol>
<li>为具体的IP地址配置SNAT，并制定转换后的源地址<br> <em>iptables -t nat -A POSTROUTING -s 192.168.0.2 -j SNAT –to-source 100.100.100.100</em>    </li>
</ol>
</li>
</ul>
<p>DNAT配置需要在NAT表中的PREROUTING或OUTPUT链中配置，其中前者更常用<br><em>iptables -t nat -A PREROUTING -d 100.100.100.100 -j DNAT –to-destination 192.168.0.2</em></p>
<p>在使用iptables配置NAT规则后，Linux需要转发来自其他IP的网络包，要确保开启Linux的IP转发功能</p>
<pre><code>sysctl -w net.ipv4.ip_forward=1
</code></pre><h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>主要使用了SystemTap工具，Linux的一种动态追踪框架，把用户提供的脚本转换为内核模块来执行，用来监测和跟踪内核的行为。</p>
<p>先运行一个不用NAT的Nginx服务，用ab测试其性能作为基准性能。然后运行使用DNAT的Nginx容器</p>
<pre><code>sudo docker run --name nginx --priviledged -p 8080:8080 -itd feisky/nginx:nat
iptables -nL -t nat  #ensure DNAT rules are created
</code></pre><p>再用ab测试时发现连接超时错误，将超时时间延长后减少总测试次数发现延迟比基准值相差太多。</p>
<p>因为我们已经知道根源时NAT，因此不需要tcpdump再抓包分析来源。此时用SystemTap工具来测试，先写一个dropwatch.stp脚本</p>
<pre><code>#! /usr/bin/env stop
global locations

probe begin {printf &quot;Monitoring for dropped packets\n&quot;}
probe end {printf &quot;Stopping dropped packet monitor \n&quot;}

probe kernel.trace(&quot;kfree_skb&quot;) { locations[$location] &lt;&lt;&lt; 1 }

probe timer.sec(5)
{
  printf(&quot;\n&quot;)
  foreach ( l in locations-) {
    printf(&quot;%d packets dropped at %s\n&quot;,@count(locations[l]),sysname(l))
  }
  delete locations
}
---------
stap --all-modules dropwatch.stp
</code></pre><p>当probebegin输出后执行ab测试，观察stap命令输出，发现大量丢包发生在nf_hook_slow位置。再用perf report来查看nf_hook_slow的调用位置，主要来自于三个地方。分别是ipv4——conntrack_in，br_nf_pre_routing以及iptable_nat_ipv4_in。即nf_hook_slow主要在执行三个动作：</p>
<ul>
<li>接收网络包时，在连接跟踪表中查找连接，并为新连接分配跟踪对象</li>
<li>Linux网桥中转发包，因为实验中容器网络通过网桥实现</li>
<li>接收网络包时，执行DNAT将8080端口的包转发给容器</li>
</ul>
<p>此时要优化只有从内核着手，DNAT的基础时conntrack，因此主要针对其参数进行优化</p>
<pre><code>sysctl -a | grep conntrack

net.netfilter.nf_conntrack_count
net.netfilter.nf_conntrack_max 
net.netfilter.nf_conntrack_buckets
net.netfilter.nf_conntrack_tcp_timout_syn_recv
net.netfilter.nf_conntrack_tcp_timeout_syn_sent
net.netfilter.nf_conntrack_tcp_timeout_time_wait
</code></pre><h2 id="Lesson-44-44-套路篇：网络性能优化的几个思路"><a href="#Lesson-44-44-套路篇：网络性能优化的几个思路" class="headerlink" title="Lesson 44/44 套路篇：网络性能优化的几个思路"></a>Lesson 44/44 套路篇：网络性能优化的几个思路</h2><p>网络性能优化首先要获得网络基准测试报告，然后通过相关性能工具，定位出网络性能瓶颈，再进行优化。可以从应用程序、套接字、传输层、网络层以及链路层分别来看</p>
<h3 id="应用程序"><a href="#应用程序" class="headerlink" title="应用程序"></a>应用程序</h3><p>应用程序通过套接字接口进行网络操作，主要对网络I/O和进程自身的工作模型进行优化。<br>除了之前C10K的多路复用技术之外，应用层也有一些网络协议优化可以考虑：</p>
<ul>
<li>长连接代替短连接，显著降低TCP建立连接的成本</li>
<li>使用内存方式来缓存不常变化的数据，降低网络I/O次数，同时加快应用程序的响应速度</li>
<li>使用Protocol Buffer等序列化的方式，压缩网络I/O的数据了，可以提高应用程序的吞吐</li>
<li>使用DNS缓存、预取、HTTPDNS等方式，减少DNS解析的延迟，也可以提升网络IO的整体速度</li>
</ul>
<h3 id="套接字"><a href="#套接字" class="headerlink" title="套接字"></a>套接字</h3><p>每个套接字都有一个读写缓冲区，为了提高网络吞吐量，通常需要调整这些缓冲区的大小</p>
<ul>
<li>读缓冲区，缓存了远端发来的数据。</li>
<li><p>写缓冲区，缓存了要发出去的数据。</p>
<pre><code>net.core.optmem_max
net.core.rmem_max net.core.wmem_max
net.ipv4.tcp_rmem net.ipv4.tcp_wmem
</code></pre></li>
</ul>
<h3 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h3><h4 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h4><ul>
<li>请求数大的场景下，大量处于TIME_WAIT状态的连接，会占用大量内存和端口资源。这种场景下可以优化与TIME_WAIRT相关的内核选项。<ul>
<li>增加处于TIME_WAIT状态的连接数量net.ipv4.tcp_max_tw_buckets，并增大连接跟踪表的大小net,netfilter.nf_conntrack_max;</li>
<li>减少net.ipv4.tcp_fin_timeout,net.netfilter.nf_conntrack_tcp_timeout_time_wait,让系统尽快释放它们占用的资源</li>
<li>开启端口复用net.ipv4.tcp_tw_reuse，这样被TIME_WAIT状态占用的端口还能用于新建的连接中</li>
<li>增大本地端口的范围net.ipv4.ip_local_port_range，支持更多的连接，提高整体的并发能力</li>
<li>增加最大文件描述符的数量 </li>
</ul>
</li>
<li>缓解SYN FLOOD等攻击，可以优化与SYN状态相关的内核选项<ul>
<li>增大TCP半连接的最大数量，或者开启TCP SYN Cookies来绕开半开连接数量限制</li>
<li>减少SYN_RECV的重传SYN+ACK次数</li>
</ul>
</li>
<li>在长连接场景中，通常使用Keepalive来检测TCP连接的状态，以便对端连接断开后可以自动回收。系统默认的Keepalive探测间隔和重试次数一般都无法满足应用程序的性能要求。考虑优化与Keepalive相关的内核选项。<ul>
<li>缩短最后一次数据包到Keepalive的探测包间隔时间</li>
<li>缩短发送Keepalive探测包的间隔时间</li>
<li>减少探测失败后一直到通知应用程序前的重试次数</li>
</ul>
</li>
</ul>
<h4 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h4><ul>
<li>增大套接字缓冲区大小以及UDP缓冲区范围</li>
<li>增大本地端口号的范围</li>
<li>根据MTU大小，调整UDP数据包的大小，减少或避免分片的发生</li>
</ul>
<p>###网络层<br>网络层主要对路由、IP分片以及ICMP等进行优化。</p>
<ul>
<li>从路由和转发的角度出发<ul>
<li>在需要转发的服务器中，开启IP转发。 net.ipv4.ip_forward=1</li>
<li>调整数据包的生存周期TTL net.ipv4.ip_default_ttl</li>
<li>开启数据包的反向地址校验，防止IP欺骗，减少伪造IP带来的DDoS问题， net.ipv4.conf.eth0.rp_filter=1</li>
</ul>
</li>
<li>从分片的角度出发，调整MTU的大小</li>
<li>从ICMP的角度出发，为了避免ICMP主机探测，ICMP Flood等问题，限制ICMP的行为<ul>
<li>禁用ICMP</li>
<li>禁止广播ICMP</li>
</ul>
</li>
</ul>
<p>###链路层</p>
<p>网卡收包后调用的中断处理程序，需要消耗大量的CPU，可以将这些中断处理程序调度到不同的CPU上执行，提高网络吞吐量。</p>
<ul>
<li>为网卡硬中断配置CPU亲和性，或者开启irqbalance服务</li>
<li>开启RPS(Receive Packet Steering)和RFS(Receive Flow Steering)，将应用程序和软中断的处理调度到相同的CPU上。增加CPU缓存命中率，减少网络延迟</li>
<li>将原来在内核中通过软件处理的功能，卸载到网卡中通过硬件执行<ul>
<li>TSO (TCP Segmentation Offload), UFO (UDP Fragmentation Offload) </li>
<li>GSO (Generic Segmentation Offload)</li>
<li>LRO (Large Receive Offload)</li>
<li>GRO (Generic Receive Offload)</li>
<li>RSS (Receive Side Scaling)</li>
<li>VXLAN卸载</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/06/27/Linux性能优化实战第五周--网络性能篇/" itemprop="url">
                  《Linux 性能优化实战》第五周--网络性能篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-06-27T15:26:55+08:00" content="2020-06-27">
              2020-06-27
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>本周的学习首先了解了Linux网络的工作原理，<strong>OSI七层网络模型</strong>，<strong>TCP/IP模型</strong>以及<strong>网络包的收发流程</strong>。应用程序通过Socket接口发送数据包时先要在网络协议栈从上到下逐层处理最终到网卡上发送，接收也要经过网络协议栈从下到上逐层解析，最后送到应用程序。以及网络传输相关性能指标和响应的查看工具。</p>
<p>并且学习了经典的C10K问题，以及延伸的C1000K和C10M问题。这个印象中研究生毕业面试B家的时候被问到过。C10K问题的根源一方面在于系统有限的资源，另一方面，同步阻塞I/O模型以及轮询的套接字接口限制了网络事件的处理效率。目前高性能网络方法都基于epoll。从10K到100K，增加物理资源就能解决，但是到1000K时就需要多方面的优化工作，从硬件中断处理和网络功能卸载、到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列等内核的优化，再到应用程序的工作模型优化，都需要考虑。</p>
<p>再进一步实现10M，就需要用XDP的方式，在内核协议栈之前处理网络包；或者用DPDK直接跳过网络协议栈在用户空间通过轮询的方式直接处理网络包。其中DPDK时目前最主流的高性能网络解决方案，但是需要能支持DPDK的网卡配合使用。</p>
<h3 id="新工具GET"><a href="#新工具GET" class="headerlink" title="新工具GET"></a>新工具GET</h3><pre><code>查看网络配置
ifconfig/ip
套接字信息/协议栈统计信息
netstat/ss
网络吞吐量和PPS
sar
带宽
ethtool
连通性和延时
ping
应用层性能
wrk/jmeter
传输层性能
iperf
转发性能
pktgen
</code></pre><h3 id="捞评论GET"><a href="#捞评论GET" class="headerlink" title="捞评论GET"></a>捞评论GET</h3><p>1、客户端的网络环境复杂，出现网络抖动如何分析解决？</p>
<p>在第一个网络出入口记录每次收发消息的内容和具体时间戳(精确到ms)，遇到玩家反馈时根据id及发生的大致时间在日志中查找响应记录，看是服务器响应慢还是客户端到服务器的线路慢。可以考虑更多的接入点、专线、CDN等优化公网的链路延迟问题。</p>
<hr>
<p>接下来是本周读书笔记</p>
<hr>
<h2 id="Lesson-33-34-关于网络，你必须知道这些"><a href="#Lesson-33-34-关于网络，你必须知道这些" class="headerlink" title="Lesson 33/34 关于网络，你必须知道这些"></a>Lesson 33/34 关于网络，你必须知道这些</h2><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p>开放式系统互联通信参考模型（Open System Interconnection Reference Model），简称<strong>OSI网络模型</strong></p>
<p>为了解决网络互联中异构设备的兼容性，并解耦复杂的网络包处理流程，OSI模型把网络互联的框架分为应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层等七层。</p>
<ul>
<li>应用层，负责为应用程序提供统一的接口</li>
<li>表示层，负责把数据转换成兼容接收系统的格式</li>
<li>会话层，负责维护计算机之间的通信连接</li>
<li>传输层，负责为数据加上传输表头，形成数据包</li>
<li>网络层，负责数据的路由和转发</li>
<li>数据链路层，负责MAC寻址，错误侦测和改错</li>
<li>物理层，负责在物理网络中传输数据帧</li>
</ul>
<p>在Linux中我们实际上使用的是一个更实用的四层模型，即TCP/IP网络模型。</p>
<p>TCP/IP模型把网络互联的框架分为应用层、传输层、网络层、网络接口层等四层。</p>
<ul>
<li>应用层，负责向用户提供一组应用程序，如HTTP，FTP，DNS等</li>
<li>传输层，负责端到端的通信，如TCP，UDP</li>
<li>网络层，负责网络包的封装、寻址和路由，如IP，ICMP</li>
<li>网络接口层，负责网络包在物理网络中的传输，比如MAC寻址、错误侦测以及通过网卡传输网络帧等</li>
</ul>
<h3 id="Linux网络栈"><a href="#Linux网络栈" class="headerlink" title="Linux网络栈"></a>Linux网络栈</h3><ul>
<li>传输层在应用数据前面增加了TCP头</li>
<li>网络层在TCP数据包前增加了IP头</li>
<li>网络接口层在IP数据包前后分别增加了帧头和帧尾</li>
</ul>
<p>网络接口配置的最大传输单元MTU规定了最大的IP包大小，以太网中MTU默认时1500</p>
<h3 id="Linux网络收发流程"><a href="#Linux网络收发流程" class="headerlink" title="Linux网络收发流程"></a>Linux网络收发流程</h3><h4 id="网络包的接收流程"><a href="#网络包的接收流程" class="headerlink" title="网络包的接收流程"></a>网络包的接收流程</h4><ul>
<li>当一个网络帧到达网卡后，网卡通过DMA方式，把网络包放到收包队列中；然后通过硬中断告诉中断处理程序已经接收到了网络包。</li>
<li>网卡中断处理程序为网络帧分配内核数据结构sk_buff，并将其拷贝到sk_buff缓冲区中；再通过软中断通知内核接收到了新的网络帧。</li>
<li>内核协议栈从缓冲区中取出网络帧，并通过网络协议，从上到下处理这个网络帧<ul>
<li>链路层检查报文合法性，找出上层协议的类型（IPv4 or IPv6），再去掉帧头和帧尾，交给网络层；</li>
<li>网络层取出IP头，判断网络包的下一步走向，比如是交给上层处理还是转发。当网络层确认这个包发送本机后，取出上层协议的类型（TCP or UDP），去掉IP头再交给传输层处理。</li>
<li>传输层取出TCP/UDP头之后根据&lt;源IP，源端口，目的IP，目的端口&gt;四元组作为标识，找出对应的Socket，并把数据拷贝到Socket接收缓存中</li>
</ul>
</li>
<li>应用程序使用Socket接口读取到新收到的数据</li>
</ul>
<h4 id="网络包的发送流程"><a href="#网络包的发送流程" class="headerlink" title="网络包的发送流程"></a>网络包的发送流程</h4><ul>
<li>应用程序通过调用Socket API发送网络包</li>
<li>由于这是系统调用，会陷入内核态的套接字层中。套接字层把数据包放到Socket发送缓冲区中</li>
<li>网络协议栈从Socket发送缓冲区中取出数据包，再按照TCP/IP栈，从上到下逐层处理</li>
<li>分片后的网络包再发送到网络接口层，进行物理地址寻址，找到下一跳的MAC地址，然后添加帧头和帧尾，放到发包队列中。这一切完成后会有软中断通知驱动程序</li>
<li>驱动程序通过DMA从发包队列中读出网络帧，并通过物理网卡发送出去</li>
</ul>
<h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><ul>
<li>带宽， 表示链路的最大传输速率，单位b/s</li>
<li>吞吐量，表示单位时间内成功传输的数据量，单位b/s或者B/s 吞吐量/带宽=网络使用率</li>
<li>延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。<ul>
<li>建立连接需要的时间， TCP握手延时</li>
<li>一个数据包往返所需的时间，RTT</li>
</ul>
</li>
<li>PPS，Packet Per Second，表示以网络包为单位的传输速率。通常用来评估网络的转发能力</li>
</ul>
<p>此外，<strong>网络的可用性、并发连接数、丢包率、重传率</strong>也是常用的性能指标。</p>
<h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><pre><code>ifconfig eth0
ip -s addr show dev eth0
</code></pre><ul>
<li>网络接口的状态标志， ifconfig输出中的RUNNING，ip输出中的LOWER_UP，都表示物理网络是联通的</li>
<li>MTU大小</li>
<li>网络接口的IP地址、子网、以及MAC地址</li>
<li>网络收发的字节数、包数、错误数以及丢包情况，特别是TX和RX部分的errors、dropped、overruns、carrier以及collisions等指标不为0时，通常表示出现了网络I/O问题<ul>
<li><strong>errors</strong>表示发生错误的数据包数，比如校验错误、帧同步错误等</li>
<li><strong>dropped</strong>表示丢弃的数据包数，即数据包已经收到了Ring Buffer，但是因为内存不足等原因丢包</li>
<li><strong>overruns</strong>表示超限数据包数，即网络I/O速度过快，导致RingBuffer中的数据包来不及处理导致的丢包</li>
<li><strong>carrier</strong>表示发生carrier错误的数据包数，比如双工模式不匹配、物理电缆出问题</li>
<li><strong>collisions</strong>表示碰撞数据包数</li>
</ul>
</li>
</ul>
<h3 id="套接字信息"><a href="#套接字信息" class="headerlink" title="套接字信息"></a>套接字信息</h3><pre><code>netstat -nlp | head -n 3
ss -ltnp | head -n 3
</code></pre><p>netstat和ss用来查看套接字、网络栈、网络接口以及路由表的信息。其中Recv-Q和Send-Q信息需要特别关注，如果不是0的话说明有网络包的堆积发生。</p>
<p>当Socket处于Established时，Recv-Q表示套接字缓冲中还没有被应用取走的字节数，Send-Q表示还没有被远端主机确认的字节数。<br>当Socket处于Listening时，Recv-Q表示全连接队列的长度，Send-Q表示全连接队列的最大长度。</p>
<h3 id="协议栈统计信息"><a href="#协议栈统计信息" class="headerlink" title="协议栈统计信息"></a>协议栈统计信息</h3><pre><code>netstat -s 
ss -s
</code></pre><h3 id="吞吐量和PPS"><a href="#吞吐量和PPS" class="headerlink" title="吞吐量和PPS"></a>吞吐量和PPS</h3><pre><code>sar -n DEV 1 
</code></pre><p>带宽可以用ethtool来查询</p>
<pre><code>ethtool eth0 | grep Speed
</code></pre><h3 id="连通性和延时"><a href="#连通性和延时" class="headerlink" title="连通性和延时"></a>连通性和延时</h3><pre><code>ping -c3 XXX.XXX.XXX.XXX
</code></pre><h2 id="Lesson-35-基础篇：C10K和C1000K回顾"><a href="#Lesson-35-基础篇：C10K和C1000K回顾" class="headerlink" title="Lesson 35 基础篇：C10K和C1000K回顾"></a>Lesson 35 基础篇：C10K和C1000K回顾</h2><p>C10K问题最早由Dan Kegel在1999年提出，那是服务器还是32位系统，运行Linux2.2版本，只配置的很少的内存(2G)和千兆网卡。怎样在这样的系统中支持并发1万的请求？</p>
<p>从资源上说，2G内存和千兆网卡服务器，同时处理1w请求，只要每个请求处理占用不超200KB内存和100Kbit的网络带宽就可以。所以物理资源充足，接下来时软件的问题。</p>
<p>如果每个请求分配一个进程/线程，1w个请求会涉及1w个进程/线程的调度、上下文切换乃至它们占用的内存都会成为瓶颈。</p>
<ul>
<li>怎样在一个线程内处理多个请求？非阻塞I/O或者异步I/O？</li>
<li>怎么更节省资源地处理用户请求？用最少的线程来服务这些请求？</li>
</ul>
<h3 id="I-O模型优化"><a href="#I-O模型优化" class="headerlink" title="I/O模型优化"></a>I/O模型优化</h3><p>I/O事件的通知方式：</p>
<ul>
<li><strong>水平触发</strong>：只要文件描述符可以非阻塞的执行I/O，就会触发通知 </li>
<li><strong>边缘出发</strong>：只有在文件描述符发生改变时(I/O请求到达时)，才发送一次通知。</li>
</ul>
<p>I/O多路复用的方法：</p>
<ul>
<li><strong>使用非阻塞I/O和水平触发通知，比如使用select和poll</strong><ul>
<li>select和 poll从文件描述符列表中，找出哪些可以执行IO，然后进行真正的网络I/O读写。由于I/O是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，达到了单线程处理多请求的目的。 </li>
<li>优点：对程序友好，API简单。</li>
<li>缺点：需要对文件描述符列表轮询，请求多时较为耗时，且select和poll还有一些限制。以及应用程序每次调用select和poll时还需要把文件描述符的集合从用户空间传入内核空间，由内核修改后再传回用户空间。增加了处理成本。</li>
</ul>
</li>
<li><strong>使用非阻塞I/O和边缘触发通知，如epoll</strong><ul>
<li>epoll使用红黑树在内核中管理文件描述符的集合，使用事件驱动的机制，只关注有I/O事件发生的文件描述符，不需要轮询整个集合</li>
<li>epoll在Linux2.6之后提供，由于边缘触发只在文件描述符可读或可写事件发生时才通知，应用程序需要尽可能多地执行I/O并要处理更多的异常事件</li>
</ul>
</li>
<li><strong>使用异步I/O</strong><ul>
<li>异步I/O也是在Linux2.6后提供，和直观逻辑不太一样，使用时要小心设计，难度较高</li>
</ul>
</li>
</ul>
<h3 id="工作模型优化"><a href="#工作模型优化" class="headerlink" title="工作模型优化"></a>工作模型优化</h3><ul>
<li>主进程+多个worker子进程<ul>
<li>主进程执行bind()+listen()后创建多个子进程</li>
<li>每个子进程中都通过accept()和epoll_wait（）来处理相同的套接字</li>
<li>Nginx就是采取这种模式，主进程用来初始化套接字并管理子进程的生命周期，worker进程用来负责实际的请求处理</li>
<li>accept和epoll_wait调用存在一个惊群问题，当网络I/O事件发生时多个进程被同时唤醒，但实际上只有一个进程来响应事件，其他被唤醒的进程都会重新休眠。<ul>
<li>accept惊群问题在Linux2.6中解决了</li>
<li>epoll_wait到Linux4.5才通过EPOLLEXCLUSIVE解决</li>
<li>nginx通过在worker进程中增加一个全局锁来解决，worker进程首先要竞争到锁，然后才加入到epoll中，确保只有一个worker子进程被唤醒</li>
</ul>
</li>
</ul>
</li>
<li>监听相同端口的多进程模型<ul>
<li>所有进程都监听相同的接口，并且开启SO_REUSEPORT选项，由内核将请求负载均衡到这些监听进程中</li>
<li>不会存在惊群问题，Nginx1.9.1中支持该模式，SO_REUSEPORT选项在Linux3.9以上版本才有</li>
</ul>
</li>
</ul>
<h3 id="C1000K"><a href="#C1000K" class="headerlink" title="C1000K"></a>C1000K</h3><p>基于I/O多路复用和请求处理的优化，C10K问题很容易解决，那么C1000K呢？</p>
<p>100万个请求需要大量的系统资源</p>
<ul>
<li>假设一个请求16KB，需要15GB内存</li>
<li>带宽上来看，假设只有20%的活跃连接，即使每个连接只需要1KB/s的吞吐量，总共也需要1.6Gb/s的吞吐量。因此还需要配置万兆网卡，或者基于多网卡bonding承载更大的吞吐量。</li>
</ul>
<p>C1000K的解决方法，本质上还是构建在epoll的非阻塞I/O模型上，只不过除了I/O模型外还需要从应用程序到Linux内核，再到CPU、内存和网络各个层次的深度优化，特别是需要借助硬件来卸载哪些通过软件处理的大量功能。</p>
<h3 id="C10M"><a href="#C10M" class="headerlink" title="C10M"></a>C10M</h3><p>同时处理1000w条请求呢？在C1000K时各种软件硬件的优化可能已经做到极致了，此时无论怎么优化应用程序和内核中各种网络参数，想实现1000万请求的并发都是及其困难的。</p>
<p>究其根本，还是Linux内核协议栈做了太多太多繁重的工作，从网卡中断带来的硬中断处理程序开始到软中断中的各层网络协议处理，最后再到应用程序，这个路径太长导致网络包的处理优化到一定程度后就无法再进一步。</p>
<p>要解决这个问题，就要跳过内核协议栈的冗长路径，把网络包直接发送到要处理的应用程序那里。</p>
<ul>
<li>DPDK，用户态网络的标准，跳过内核协议栈直接由用户进程通过轮询方式处理网络接收。 还通过大页、CPU绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。</li>
<li>XDP，Linux内核提供的一种高性能网络数据路径。它允许网络包在进入内核协议栈之前就进行处理，也可以带来更高的性能。XDP底层也是基于Linux内核的eBPF机制实现的。</li>
</ul>
<h2 id="Lesson-36-套路篇：怎么评估系统的网络性能？"><a href="#Lesson-36-套路篇：怎么评估系统的网络性能？" class="headerlink" title="Lesson 36 套路篇：怎么评估系统的网络性能？"></a>Lesson 36 套路篇：怎么评估系统的网络性能？</h2><h3 id="各协议层的性能测试"><a href="#各协议层的性能测试" class="headerlink" title="各协议层的性能测试"></a>各协议层的性能测试</h3><h4 id="转发性能"><a href="#转发性能" class="headerlink" title="转发性能"></a>转发性能</h4><p>网络接口和网络层，主要负责网络包的封装、寻址、路由以及发送和接收。在这两个网络协议中，每秒可处理的网络包数PPS就是最重要的性能指标。特别是64B小包的处理能力，值得我们特别关注。如何来测试网络包的处理能力呢？</p>
<p>Linux内核自带的高性能网络测试工具pktgen,但是并不能直接找到pktgen命令，需要加载pktgen内核模块后，再通过/proc文件系统来交互</p>
<pre><code>modprobe pktgen
ps -ef | grep pktgen | grep -v grep
ls /proc/net/pktgen/
</code></pre><h3 id="TCP-UDP性能"><a href="#TCP-UDP性能" class="headerlink" title="TCP/UDP性能"></a>TCP/UDP性能</h3><pre><code>iperf
netperf
</code></pre><h3 id="HTTP性能"><a href="#HTTP性能" class="headerlink" title="HTTP性能"></a>HTTP性能</h3><p>在应用层，有的应用程序会直接基于TCP或UDP构建服务，也有大量的应用基于应用层的协议来构建服务。HTTP就是一个最常用的应用层协议，要测试HTTP性能可以通过ab、webbench等。</p>
<pre><code>ab
webbench
</code></pre><h3 id="应用负载性能"><a href="#应用负载性能" class="headerlink" title="应用负载性能"></a>应用负载性能</h3><pre><code>wrk
TCPCopy
Jmeter
</code></pre><hr>
<h2 id="Lesson-36-套路篇：怎么评估系统的网络性能"><a href="#Lesson-36-套路篇：怎么评估系统的网络性能" class="headerlink" title="Lesson 36 套路篇：怎么评估系统的网络性能"></a>Lesson 36 套路篇：怎么评估系统的网络性能</h2><p>上节课学习了C10M的解决方案，不过在大多数场景下，我们并不需要单机并发1000万请求。通过调整系统架构，把请求分发到多台服务器中并行处理，才是更简单、扩展性更好的方案。</p>
<p>这就需要我们评估系统的网络性能，以便考察系统的处理能力，并为容量规划提供基准数据。</p>
<h3 id="性能指标回顾"><a href="#性能指标回顾" class="headerlink" title="性能指标回顾"></a>性能指标回顾</h3><p><strong>带宽、吞吐量、延时、PPS</strong>，这四个性能指标中带宽跟物理网卡配置直接关联；Linux服务器的网络吞吐量一般会比带宽小，交换机等专门的网络设备吞吐量一般接近带宽；PPS以网络包为单位的网络传输速率，通常用在需要大量转发的场景中；对于TCP或者Web服务来说通常会用并发连接数和每秒请求数QPS等指标。</p>
<h3 id="网络基准测试"><a href="#网络基准测试" class="headerlink" title="网络基准测试"></a>网络基准测试</h3><p>在测试之前需要弄清楚需要测试的应用程序基于协议栈的哪一层？</p>
<ul>
<li>基于HTTP和HTTPS的Web应用程序，属于应用层，需要测试HTTP/HTTPS的性能；</li>
<li>大多数游戏服务器，为了支持更大的在线人数。通常会基于TCP/UDP与客户端交互，需要测试TCP/UDP性能</li>
<li>还有一些场景将Linux作为一个软交换机或者路由器来使用，此时要更关注网络包的处理能力。即PPS，关注网络层的转发能力。</li>
</ul>
<h3 id="各协议层性能测试"><a href="#各协议层性能测试" class="headerlink" title="各协议层性能测试"></a>各协议层性能测试</h3><h4 id="转发性能-1"><a href="#转发性能-1" class="headerlink" title="转发性能"></a>转发性能</h4><p>网络接口层和网络层，主要负责网络包的封装、寻址、路由以及发送和接收。这里最重要的性能指标就是PPS每秒可处理的网络包数。</p>
<p>可以用hping3或者pktgen来测试网络包处理能力。其中pktgen作为一个Linux内核自带的高性能网络测试工具，需要加载pktgen内核模块后再通过/proc文件系统交互。</p>
<pre><code>modprobe pktgen
ls /proc/net/pktgen
</code></pre><p>在测试时，需要先给每个内核线程kpktgend_X以及测试网卡，配置pktgen选项。再通过pgctrl启动测试。</p>
<p>假设发包及其使用网卡eth0，目标机器的IP为192.168.0.30， MAC地址为11:11:11:11:11:11</p>
<pre><code>#define function for test options
function pgset() {
    local result
    echo $1 &gt; $PGDEV
    result=`cat $PGDEV | fgrep &quot;Result: OK:&quot;`

    if [ &quot;$result&quot; = &quot;&quot; ]; then 
        cat $PGDEV | fgrep Result:
    fi
} 

#bind eth0 for thread 0
PGDEV=/proc/net/pktgen/eht0
pgset &quot;count 1000000&quot; #total packages
pgset &quot;delay 5000&quot;
pgset &quot;clone_skb 0&quot;
pgset &quot;pkt_size 64&quot;
pgset &quot;dst 192.168.0.30&quot;
pgset &quot;dst_mac 11.11.11.11.11.11&quot;

#Start test
PGDEV=/proc/net/pktgen/pgctrl
pgset &quot;start&quot;

#Check result
cat /proc/net/pktgen/eth0
</code></pre><h4 id="TCP-UDP性能-1"><a href="#TCP-UDP性能-1" class="headerlink" title="TCP/UDP性能"></a>TCP/UDP性能</h4><pre><code>iperf
netperf
</code></pre><h4 id="HTTP性能-1"><a href="#HTTP性能-1" class="headerlink" title="HTTP性能"></a>HTTP性能</h4><pre><code>ab -c 1000 -n 10000 http://www.baidu.com
webbench
</code></pre><h4 id="应用负载性能-1"><a href="#应用负载性能-1" class="headerlink" title="应用负载性能"></a>应用负载性能</h4><pre><code>wrk TCPCopy Jmeter LoadRunner
工作中用过Jmeter进行测试
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/06/21/Linux性能优化实战第四周--IO性能篇/" itemprop="url">
                  《Linux 性能优化实战》第四周--IO性能篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-06-21T10:57:55+08:00" content="2020-06-21">
              2020-06-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>本周主要学习Linux I/O相关的基础知识以及遇到I/O异常问题如何分析解决。<strong>Linux一切皆文件</strong>。为了支持不同的文件系统，首先Linux在用户进程和文件系统之间实现了一层虚拟文件系统。用户进程和内核中的其他子系统只需要跟VFS提供的统一接口进行交互。其次，为了降低慢速磁盘对性能的影响，文件系统又通过页缓存、目录项缓存以及索引节点缓存来减少对应用程序性能的影响。</p>
<p>文件系统层、通用块层和块设备层组成了Linux存储系统I/O栈。其中通用块层是磁盘I/O的核心，向上为文件系统和应用程序提供访问块设备的标准接口，向下把各种异构磁盘抽象为统一的块设备，并对文件系统和应用程序发来的I/O请求进行重新排序、请求合并等。</p>
<p>通过实验学习了遇到IO瓶颈进一步导致CPU使用率高的问题如何分析和解决。一般通过iostat确认是否存在I/O性能瓶颈，再用strace和lsof定位应用程序以及它正在写入的日志文件路径。最后通过调整日志打印级别来解决。如果strace无法跟踪到write系统调用时，可以用filetop和opensnoop来定位具体的线程和读写文件目录；也可以加-p选项开启线程跟踪。</p>
<p>MYSQL的MyISAM引擎主要依赖系统缓存加速磁盘IO的访问。如果系统中还有其他应用同时运行，MyISAM引擎很难充分利用系统缓存。缓存可能会被其他应用程序占用，甚至被清理掉。因此最好不要将应用程序的性能优化完全建立在系统缓存上，最好能在应用程序内部分配内存，构建完全自主的缓存；或者利用第三方缓存应用，如Memcached，redis等。</p>
<p>对于磁盘IO瓶颈可以通过在内存充足时将数据放在更快的内存中来进行优化。也可以进一步利用Trie树等各种算法来进一步优化处理效率。</p>
<h3 id="新工具GET"><a href="#新工具GET" class="headerlink" title="新工具GET"></a>新工具GET</h3><p>查看目录项和各种文件系统索引节点的缓存情况： </p>
<pre><code>cat /proc/slabinfo | slabtop
</code></pre><p>磁盘IO观察</p>
<pre><code>iostat -d -x 1      # -d -x 表示显示所有磁盘I/O的指标
</code></pre><p>进程IO观察</p>
<pre><code>pidstat -d 1

iotop #可以按照I/O大小对进程排序找到I/O较大的进程
</code></pre><p>当strace无法跟踪到文件IO痕迹时</p>
<pre><code>filetop 查看文件名以及使用情况
opensnoop 查看具体的文件目录
</code></pre><p>TCP网络连接可以用过nsenter工具来查看详细信息</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><pre><code>find / -name XXX 会不会导致系统的缓存升高？
会，导致inode_cache/dentry/proc_inode_cache/xfs_inode缓存升高
</code></pre><p>可以通过实验进行验证，先清除系统缓存，然后执行命令观察缓存使用情况。</p>
<pre><code>iostat/pidstat已经证明了IO瓶颈是由哪个进程导致，为什么strace跟踪没有发现痕迹？
</code></pre><p>写文件是由子线程来进行处理的，默认strace是不开启线程跟踪的。在strace命令加上-fp选项既可以跟踪进程也可以跟踪线程。</p>
<hr>
<p>接下来是本周读书笔记</p>
<hr>
<h2 id="Lesson-23-Linux文件系统是怎么工作的"><a href="#Lesson-23-Linux文件系统是怎么工作的" class="headerlink" title="Lesson 23 Linux文件系统是怎么工作的"></a>Lesson 23 Linux文件系统是怎么工作的</h2><ul>
<li>磁盘为文件系统提供了最基本的持久化存储</li>
<li>文件系统在磁盘的基础上，提供了一个用来管理文件的树状结构</li>
</ul>
<h3 id="索引节点和目录项"><a href="#索引节点和目录项" class="headerlink" title="索引节点和目录项"></a>索引节点和目录项</h3><p>Linux中一切皆文件。为了方便管理，Linux文件系统为每个文件都分配两个数据结构：</p>
<ul>
<li>索引节点： index node，记录文件的元数据(如inode编号，文件大小，访问权限，修改日期，数据位置等)。索引节点会持久化存储到磁盘中，同样占用磁盘空间。</li>
<li>目录项： directory entry，记录文件的名字，索引节点指针以及与其他目录项的关联关系。目录项是由内核维护的一个内存数据结构，也叫目录项缓存。</li>
</ul>
<p>索引节点是每个文件唯一标志，目录项维护文件系统的树状结构。目录项和索引节点关系是多对一。</p>
<p>磁盘在执行文件系统格式化时，会被分成三个存储区域：</p>
<ul>
<li>超级块，存储整个文件系统的状态</li>
<li>索引节点区，用来存储索引节点</li>
<li>数据块区，用来存储文件数据</li>
</ul>
<h3 id="虚拟文件系统"><a href="#虚拟文件系统" class="headerlink" title="虚拟文件系统"></a>虚拟文件系统</h3><p>目录项、虚拟节点、逻辑块以及超级块构成了Linux文件系统的四大基本要素，为了支持各种不同的文件系统，内核在用户进程和文件系统之间引入了一个虚拟文件系统VFS抽象层。</p>
<p>VFS定义了一组所有文件系统都支持的数据结构和标准接口。这样用户进程和内核中的其他子系统只需要跟VFS提供的统一接口进行交互即可。</p>
<h3 id="文件系统I-O"><a href="#文件系统I-O" class="headerlink" title="文件系统I/O"></a>文件系统I/O</h3><p>根据是否利用标准库缓存，可以分为：</p>
<ul>
<li>缓冲I/O，利用标准库缓存来加速文件的访问，标准库内部再通过系统调度访问文件</li>
<li>非缓冲I/O，直接通过系统调用来访问文件，不再经过标准库缓存</li>
</ul>
<p>根据是否利用系统的页缓存，分为：</p>
<ul>
<li>直接I/O，跳过操作系统的页缓存，直接跟文件系统交互来访问文件  （O_DIRECT）</li>
<li>非直接I/O，文件读写时，先要经过系统的页缓存然后再由内核或额外的系统调用，真正写入磁盘</li>
</ul>
<p>根据应用程序是否阻塞自身运行，分为：</p>
<ul>
<li>阻塞I/O，应用程序执行I/O操作后如果没有获得响应，就会阻塞当前线程</li>
<li>非阻塞I/O，应用程序执行I/O操作后，不会阻塞当前的线程，可以继续执行其他的任务。然后再通过轮询或者事件通知的形式获取调用结果                   （O_NONBLOCK）</li>
</ul>
<p>根据是否响应结果，分为：</p>
<ul>
<li>所谓同步I/O，应用程序执行I/O操作后，要一直等到整个I/O完成后才能获得I/O响应 （O_SYNC/O_DSYNC）</li>
<li>所谓异步I/O，应用程序执行I/O操作后，不用等待完成和完成后的响应，而是继续执行就可以。等这次I/O完成后，响应会用事件通知的方式告诉应用程序 （O_ASYNC）</li>
</ul>
<h3 id="性能观测"><a href="#性能观测" class="headerlink" title="性能观测"></a>性能观测</h3><pre><code>cat /proc/slabinfo | grep -E &apos;^#|dentry|inode&apos;

slabtop
</code></pre><hr>
<h2 id="Lesson-24-25-Linux磁盘I-O时怎么工作的？"><a href="#Lesson-24-25-Linux磁盘I-O时怎么工作的？" class="headerlink" title="Lesson 24/25 Linux磁盘I/O时怎么工作的？"></a>Lesson 24/25 Linux磁盘I/O时怎么工作的？</h2><p>###磁盘<br>磁盘是可以持久化的设备，根据存储介质不同，分为：</p>
<ul>
<li>机械磁盘(Hard Disk Driver)，主要由盘片和读写磁头组成，数据存储在盘片的环状磁道中。读写数据时移动磁头，定位到数据所在的磁道中，然后才能访问。最小读写单位是扇区，一般512byte</li>
<li>固态磁盘(Solid State Disk)，由固态电子元器件组成，不需要磁道寻址。无论连续I/O还是随机I/O都比前者要好。最小读写单位是页，一般4KB，8KB等</li>
</ul>
<p>两种磁盘随机I/O都要比连续I/O慢得多：</p>
<ul>
<li>机械磁盘随机I/O需要更多的磁头寻道和盘片旋转</li>
<li>固态磁盘同样存在“先擦除再写入的限制”，随机读写会导致大量的垃圾回收</li>
<li>连续I/O可以通过预读的方式来减少I/O请求的次数</li>
</ul>
<p>###通用块层</p>
<p>通用块层，是处在文件系统和磁盘驱动中间的一个块设备抽象层。主要有以下功能：</p>
<ul>
<li>与VFS类似，向上为文件系统和应用程序提供块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。</li>
<li>给文件系统和应用程序发来的I/O请求排队，并通过重新排序、请求合并等来提升磁盘读写的能力。</li>
</ul>
<p><strong>I/O调度算法</strong>：</p>
<ul>
<li><strong>NONE</strong>，不使用任何I/O调度，常用在虚拟机中</li>
<li><strong>NOOP</strong>，先入先出队列，只进行最基本的请求合并，常用与SSD磁盘</li>
<li><strong>CFQ（Completely Fair Schedule）</strong>，完全公平调度器，为每个进程维护一个I/O调度队列，并按照时间片来均匀分布每个进程的I/O请求。 类似进程CPU调度，CFQ还支持进程I/O的优先级调度，适用于大量进程的系统</li>
<li><strong>Deadline</strong>，分别为读写请求创建不同的I/O队列，提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理。多用于IO压力较重的场景，如数据库等</li>
</ul>
<h3 id="IO栈"><a href="#IO栈" class="headerlink" title="IO栈"></a>IO栈</h3><p>Linux存储系统的I/O栈由上到下分为 <strong>文件系统层、通用块层、设备层</strong>。存储系统的IO通常是整个系统最慢的一环，所以Linux系统通过多种缓存机制来优化I/O效率。</p>
<ul>
<li>优化文件访问性能： 页缓存、索引节点缓存、目录项缓存等减少对下层设备的直接调用</li>
<li>优化块设备访问性能：使用缓冲区来缓存块设备的数据</li>
</ul>
<h3 id="磁盘性能指标"><a href="#磁盘性能指标" class="headerlink" title="磁盘性能指标"></a>磁盘性能指标</h3><ul>
<li>使用率， 磁盘处理I/O的时间百分比</li>
<li>饱和度， 磁盘处理I/O的繁忙程度</li>
<li>IOPS， 每秒的I/O请求数</li>
<li>吞吐量， 每秒I/O请求大小</li>
<li>响应时间， I/O请求从发出到收到响应的间隔时间</li>
</ul>
<h3 id="磁盘I-O观测"><a href="#磁盘I-O观测" class="headerlink" title="磁盘I/O观测"></a>磁盘I/O观测</h3><pre><code>iostat -d -x 1      # -d -x 表示显示所有磁盘I/O的指标
</code></pre><h3 id="进程I-O观测"><a href="#进程I-O观测" class="headerlink" title="进程I/O观测"></a>进程I/O观测</h3><pre><code>pidstat -d 1

iotop #可以按照I/O大小对进程排序找到I/O较大的进程
</code></pre><hr>
<h2 id="Lesson-26-案例篇：如何找出狂打日志的内鬼"><a href="#Lesson-26-案例篇：如何找出狂打日志的内鬼" class="headerlink" title="Lesson 26 案例篇：如何找出狂打日志的内鬼"></a>Lesson 26 案例篇：如何找出狂打日志的内鬼</h2><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>首先运行目标应用</p>
<pre><code>sudo docker run -v /tmp:/tmp --name=app -itd feisky/logapp
ps -ef | grep /app.py #确保程序启动
</code></pre><p>我们先用top来观察CPU和内存的使用情况，然后再用iostat来观察磁盘使用情况</p>
<pre><code>top 
#观察发现CPU0使用率高且iowait超过了90%，说明cpu0上正在运行IO密集型程序
#进程方面pythonCPU使用率较高，记录其pid号
#内存使用方面，总内存8G剩余700+M，Buffer/Cache占用较高
</code></pre><p>基本可以判断出CPU使用率中的iowait是一个潜在瓶颈，而内存中的缓存占比较大。<br>再用iostat查看I/O使用情况</p>
<pre><code>iostat -x -d 1 
#发现sda的I/O使用率很高，很可能已经接近饱和
#查看前面指标，每秒写磁盘请求数是64，写大小是32MB，写请求响应时间7s，而请求队列长度则达到了1000+
#超慢的响应时间和请求队列过长，进一步验证了IO已经饱和
</code></pre><p>接下来分析I/O性能瓶颈的根源</p>
<pre><code>pidstat -d 1 
#此时python进程的写比较大，且每秒数据超过了45M，说明python进程导致了IO瓶颈

strace -p XXXX 
#在write()系统调用上，可以看出进程向文件描述符编号为3的文件中写入了300M数据
#再观察后面的stat调用，可以看到它正在获取/tmp/logtest.txt.1的状态，这种格式的文件在日志回滚中常见

losf -p XXXX
#查看进程打开了哪些文件，/tmp/logtest.txt
</code></pre><p>综上说明进程以每次300MB的速度在疯狂的写日志，其中日志文件目录为/tmp/logtest.txt。此时查看案例源码发现其默认记录INFO级别以上的所有日志。此时将默认级别调高到WARNING级别，日志问题即可解决。</p>
<hr>
<h2 id="Lesson-27-案例篇：为什么我的磁盘IO延迟很高？"><a href="#Lesson-27-案例篇：为什么我的磁盘IO延迟很高？" class="headerlink" title="Lesson 27 案例篇：为什么我的磁盘IO延迟很高？"></a>Lesson 27 案例篇：为什么我的磁盘IO延迟很高？</h2><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>本实验需要两台虚拟机，一台案例分析的目标机器运行Flask应用，另一台作为客户端请求单词的热度。</p>
<pre><code>sudo docker run --name=app -p 10000:80 -itd feisky/word-app
</code></pre><p>然后在第二天机器 curl */popularity/word 发现一直没响应</p>
<p>回第一台机器来分析，首先执行df命令查看文件系统使用情况，发现也要等好久才输出。此时df显示系统还有足够多的磁盘空间。此时同样可以先用top来观察CPU和内存使用情况，再用iostat来观察磁盘的IO情况。</p>
<p>为了避免curl请求结束，在终端2循环执行curl，并用time观察每次执行时间。</p>
<pre><code>while true； do 
    time curl */popularity/word
    sleep 1
done
</code></pre><p>top输出发现两个CPU的iowait都非常高，进程部分python进程的CPU使用率稍高，可能和iowait相关。 记录其pid</p>
<pre><code>ps -aux | grep app.py #正好CPU使用率高的进程是我们的案例应用

iostat -x -d 1 #发现磁盘sda的I/O使用率已经达到98%，写响应时间18s，每秒32MB显然已经达到了IO瓶颈

pidstat -d 1 #再次看到了案例应用pid导致的io瓶颈

strace -p XXX
</code></pre><p>类似上节的套路，此时strace中可以看到大量的stat系统调用，却没有任何write调用。文件写明明应该有响应的write系统调用，现有工具却找不到痕迹。此时就该考虑换工具了，filetop基于eBPF机制，主要跟踪内核中文件的读写情况，并输出线程ID、读写大小、读写类型以及文件名称。</p>
<pre><code>filetop -C #发现每隔一段时间线程号为XXX的python应用会写入大量的txt文件，再大量读。
ps -efT | grep XXX #该线程确实属于我们的应用进程
</code></pre><p>filetop只给出文件名，并没有给出文件路径。此时opensnoop工具登场</p>
<pre><code>opensnoop #可以看到这些txt文件位于/tmp目录下，文件从0.txt到1000.txt
</code></pre><p>结合filetop和opensnoop我们可以猜测案例应用应该是写入1000个txt文件后，又将这些文件内容读取到内存中进行处理。在打断ls检查路径中文件时发现内容为空。此时再次运行opensnoop发现目录变化了，说明这些目录都是应用程序动态生成的，用后就删除了。</p>
<p>接下来查看程序源码发现该案例应用，在每个请求的处理过程中都会生成一批临时文件，然后读入内存处理，最后再删除整个目录。这是一种常见的利用磁盘空间处理大量数据的技巧，不过本次案例中的IO请求太重导致磁盘I/O利用率过高。</p>
<p>通过算法优化，在内存充足时将所有数据放到内存中处理，这样就能避免IO性能问题。</p>
<hr>
<h2 id="Lesson-28-案例篇：一个SQL查询要15s是怎么回事？"><a href="#Lesson-28-案例篇：一个SQL查询要15s是怎么回事？" class="headerlink" title="Lesson 28 案例篇：一个SQL查询要15s是怎么回事？"></a>Lesson 28 案例篇：一个SQL查询要15s是怎么回事？</h2><h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>案例由3个容器组成，一个mysql数据库应用，一个商品搜索应用，一个数据处理的应用。在执行搜索命令时遇到了返回数据为空且处理时间超过15s的问题。同样通过循环持续发送请求来进行问题问题，为了避免系统压力过大sleep 5s再开始新请求。</p>
<p>同样的套路，top iostat pidstat定位IO瓶颈问题以及mysqld进程。慢查询现象大多是CPU使用率高，但这里看到的却是IO问题，说明这并不是单纯的慢查询问题。</p>
<p>接下来通过strace发现线程XXX正在读取大量数据，且读取文件的描述符编号为38。再用lsof尝试查找对应的文件，此时发现lsof没有任何输出。</p>
<pre><code>echo $? #查找上一条指令退出时返回值，结果为1说明lsof命令执行失败。
</code></pre><p>因为-p需要指定进程号，而我们传入线程号所以执行失败。<br>切换回进程号重新执行lsof命令，从输出可以看出确实mysqld进程打开了大量的文件，根据文件描述符找到对应的文件路径为/var/lib/mysql/test/products.MYD文件。</p>
<p>MYSQL中MYD文件时MyISAM引擎用来存储表数据的文件，文件名就是数据表的名字，父目录即为数据库的名字。 即改文件告诉我们mysqld正在读取test数据库中的products表。</p>
<p>如何确定这些文件是不是mysqld正在使用的数据库文件呢？有没有可能是不再使用的旧数据？我们通过查看mysqld配置的数据路径即可。</p>
<pre><code>sudo docker exec -ti mysql mysql -e &apos;show global variable like &quot;%datadir%&quot;;&apos;
</code></pre><p>可以看到/var/lib/mysql确实是mysqld正在使用的数据存储目录。<br>即然找出了数据库和表，下一步就是弄清楚数据库中正在执行什么样的SQL。</p>
<p>在SQL命令界面执行</p>
<pre><code>show full processlist #可以看到select * from products where productName=‘geektime’这条执行时间比较长
</code></pre><p>一般SQL慢查询问题，很可能是没有利用好索引导致，如何判断这条是不是这样？</p>
<pre><code>explain  select * from products where productName-‘geektime’
</code></pre><p>其中pissible_keys和key都为NULL，type为ALL全表查询，这条查询语句根本没有使用索引，所以查询时会扫描整个表。</p>
<p>因此给productName建立索引即可， 优化后查询时间从15s缩短到了3ms。</p>
<p>该案例中测试时启动了一个DataService应用，其实停止该应用查询时间也能缩短到0.1s。这种情况下是否还存在IO瓶颈呢？通过vmstat来查看IO变化，发现磁盘读和iowait刚开始挺大，但是没过多久就变成了0，说明IO瓶颈消失。为什么呢？</p>
<p>通过查看DataService源码可以看到其读取文件前先将 /proc/sys/vm/drop_caches改为1。即释放文件缓存，而mysql读取的数据就是文件缓存，dataService不断释放缓存导致mysql直接访问磁盘。因此产生IO瓶颈。</p>
<hr>
<h2 id="Lesson-29-案例篇：Redis响应验证延迟，如何解决？"><a href="#Lesson-29-案例篇：Redis响应验证延迟，如何解决？" class="headerlink" title="Lesson 29 案例篇：Redis响应验证延迟，如何解决？"></a>Lesson 29 案例篇：Redis响应验证延迟，如何解决？</h2><h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><p>本实验由python应用和redis两部分组成。Python应用是一个基于Flask的应用，会利用Redis来管理应用程序的缓存。</p>
<p>实验中在访问应用程序的缓存接口时，发现10s的长响应时间，接下来定位瓶颈。</p>
<p>同样为了避免分析过程中请求结束，通过loop循环来执行curl命令。</p>
<p>继续先通过top和iostat先分析是否存在IO瓶颈。 结果发现CPU的iowait比较高，但是磁盘每秒写数据为2.5M，IO使用率为0，没有IO瓶颈。</p>
<p>但是案例中测试时从Redis缓存中读取数据，对应应该时磁盘的读操作，iostat结果却显示时写操作。所以我们就要知道是什么进程在具体写磁盘。</p>
<p>运行pidstat -d查看发现是redis-server进程在写磁盘。接下来用strace+lsof查看到底在写什么。从系统调用看epoll_wait、read、write、fdatasync这些系统调用都比较频繁，刚才观察的写操作应该是write和fdatasync导致。lsof找出这些系统调用的操作对象，发现只有7号普通文件会产生磁盘写，其操作路径为/data/appendonly.aof。</p>
<p>在Redis中这对应着持久化配置中的appendonly和appendfsync选项，可能是由于它们配置不合理导致磁盘写较多。为了验证这个猜测，通过redis命令行查这两个选项的配置。</p>
<pre><code>sudo docker exec -ti redis redis-cli config get &apos;append*&apos;
</code></pre><p>发现appendfsync配置为always，appendonly配置为yes。</p>
<p>Redis提供了两种数据持久化方式：</p>
<ul>
<li><strong>快照方式</strong>，按照指定的时间间隔生成数据的快照，并且保存在磁盘文件中。为避免阻塞主进程，Redis会fork一个子进程来进行快照的保存。 无论备份恢复都比追加文件性能好，缺点是在数据量大时fork子进程会用到比较大的内存，保存数据比较耗时。</li>
<li><strong>追加文件</strong>，在文件末尾追加记录的方式对redis写入数据进行持久化。提供appendfsync选项设置fsync策略：<ul>
<li>always， 每个操作都会执行一次fsync，最安全</li>
<li>everysec，每秒钟调用一次fsync，即使最坏情况也只会丢失1s数据</li>
<li>no， 交给操作系统来处理</li>
</ul>
</li>
</ul>
<p>回头看上述测试，因为配置为always导致每次写数据都会调用一次fsync，从而造成比较大的磁盘IO压力。</p>
<p>但是为什么查询会有磁盘写呢，我们再次审视strace和lsof的输出，发现编号为8的TCP socket正好对应TCP读写，是一个标准的“请求-相应”格式。从socket中GET uuid：X后响应good，再从socket中读取SADD good X后响应1。对Redis来说SADD是一个写操作，所以Redis会将其持久化到appendonly.aof文件中。因此产生大量的磁盘读写。</p>
<p>接下来我们确认8号TCPsocket对应的Redis客户端是否为我们的案例应用。 通过lsof -i 找出TCP socket对应的TCP连接信息，进入容器网络命名空间内部看到完成的TCP连接。</p>
<pre><code>PID=$(sudo docker inspect --format {{.State.Pid}} app)
nsenter --target $PID --net -- lsof -i
</code></pre><p>综合分析可知，首先redis配置always不太合理，本案例不需要这么高频的同步写，改为1s时间间隔足够；其次python应用在查询接口中会调用Redis的SADD命令，这很可能是不合理使用缓存导致。</p>
<p>修改配置后请求时间降低到0.9s，接着通过分析源码解决第二个问题。代码中Python应用将Redis当成临时空间，用来存储查询过程中找到的数据。优化将其放在内存中，再次查看响应时间已经降低到了0.2s。</p>
<hr>
<h2 id="Lesson-30-套路篇：如何迅速分析出系统IO瓶颈"><a href="#Lesson-30-套路篇：如何迅速分析出系统IO瓶颈" class="headerlink" title="Lesson 30 套路篇：如何迅速分析出系统IO瓶颈"></a>Lesson 30 套路篇：如何迅速分析出系统IO瓶颈</h2><h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><h4 id="文件系统IO性能指标"><a href="#文件系统IO性能指标" class="headerlink" title="文件系统IO性能指标"></a>文件系统IO性能指标</h4><ul>
<li>存储空间的使用情况，容量、使用量以及剩余空间等<ul>
<li>文件系统向外展示的空间使用，而非磁盘空间的真实用量</li>
<li>索引节点的使用情况，包括容量、使用量以及剩余量 （如果文件系统中存储过多的小文件，就能碰到索引节点容量已满的问题）</li>
</ul>
</li>
<li>缓存使用情况，页缓存、目录项缓存、索引节点缓存以及各个具体文件系统的缓存</li>
<li>文件系统IO， IOPS、响应延迟时间、以及吞吐量</li>
</ul>
<p>Linux文件系统并没有提供直接查看这些指标的方法，只能通过系统调用、动态跟踪或者基准测试的方法来间接观察评估。</p>
<h4 id="磁盘IO性能指标"><a href="#磁盘IO性能指标" class="headerlink" title="磁盘IO性能指标"></a>磁盘IO性能指标</h4><ul>
<li>使用率</li>
<li>IOPS</li>
<li>吞吐量</li>
<li>响应时间</li>
<li>Buffer也常出现在内存和磁盘问题的分析中</li>
</ul>
<h3 id="性能工具"><a href="#性能工具" class="headerlink" title="性能工具"></a>性能工具</h3><ul>
<li><strong>df</strong>，既可以查看文件系统数据的空间容量，也可以查看索引节点的容量</li>
<li><strong>/proc/meminfo，/proc/slabinfo及slaptop</strong>，观察页缓存、目录项缓存、索引节点缓存以及具体的文件系统的缓存</li>
<li><strong>iostat，pidstat</strong>观察磁盘和进程的IO情况<ul>
<li><strong>iostat</strong>查看磁盘的IO使用率、吞吐量、响应时间以及IOPS性能指标</li>
<li><strong>pidstat</strong>查看进程的IO吞吐量以及块设备的IO延迟     </li>
</ul>
</li>
<li><strong>strace+lsof</strong>定位问题进程正在读写的文件</li>
<li><strong>filetop+opensnoop</strong>，从内核中跟踪系统调用，最终找出瓶颈来源</li>
</ul>
<h3 id="性能指标和工具的联系"><a href="#性能指标和工具的联系" class="headerlink" title="性能指标和工具的联系"></a>性能指标和工具的联系</h3><h4 id="根据指标找工具"><a href="#根据指标找工具" class="headerlink" title="根据指标找工具"></a>根据指标找工具</h4><p><img src="/2020/06/21/Linux性能优化实战第四周--IO性能篇/metrictool.png" alt>  </p>
<h4 id="根据工具查指标"><a href="#根据工具查指标" class="headerlink" title="根据工具查指标"></a>根据工具查指标</h4><p><img src="/2020/06/21/Linux性能优化实战第四周--IO性能篇/toolmetric.png" alt>  </p>
<h3 id="如何迅速分析I-O的性能瓶颈"><a href="#如何迅速分析I-O的性能瓶颈" class="headerlink" title="如何迅速分析I/O的性能瓶颈"></a>如何迅速分析I/O的性能瓶颈</h3><ul>
<li>先用iostat发现磁盘IO性能瓶颈</li>
<li>再借助pidstat定位出导致瓶颈的进程</li>
<li>随后分析进程的IO行为</li>
<li>最后结合应用程序的原理，分析这些IO的来源</li>
</ul>
<p>为了缩小排查范围，通常先运行几个支持指标较多的工具，如iostat、vmstat、pidstat等，然后再根据观察到的现象，结合系统和应用程序的原理，寻找下一步的分析方向。</p>
<p>例如MYSQL和Redis案例中，通过iostat确认磁盘出现IO性能瓶颈，然后用pidstat找出I/O最大的进程，接着借助strace找出该进程正在读写的文件，最后结合应用程序的原理找出大量IO的原因。</p>
<p>当用iostat发现磁盘IO性能瓶颈后，再用pidstat和vmstat检查，可能会发现IO来自内核线程。如Swap使用大量升高。这种情况下，就得进行内存分析，先找出占用大量内存的进程，再设法减少内存的使用。</p>
<p><img src="/2020/06/21/Linux性能优化实战第四周--IO性能篇/analysis.png" alt>  </p>
<hr>
<h2 id="Lesson-31-套路篇：-磁盘I-O性能优化的几个思路"><a href="#Lesson-31-套路篇：-磁盘I-O性能优化的几个思路" class="headerlink" title="Lesson 31 套路篇： 磁盘I/O性能优化的几个思路"></a>Lesson 31 套路篇： 磁盘I/O性能优化的几个思路</h2><h3 id="IO基准测试"><a href="#IO基准测试" class="headerlink" title="IO基准测试"></a>IO基准测试</h3><p>为了更客观的评估优化效果，首先应该对磁盘和文件系统进行基准测试，得到文件系统或磁盘IO的极限性能。<br>fio（Flexible I/O Tester）是最常用的基准测试工具</p>
<pre><code># 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
#顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
#顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
</code></pre><ul>
<li>direct表示是否跳过系统缓存，1表示跳过</li>
<li>iodepth表示使用异步I/O（asynchronous I/O）时同时发出的IO请求上限</li>
<li>rw表示I/O模式</li>
<li>ioengine表示I/O引擎，支持同步sync，异步libaio，内存映射mmap，网络net等</li>
<li>bs表示I/O的大小</li>
</ul>
<p>结果报告中</p>
<ul>
<li>slat表示从I/O提交到实际执行I/O的时长。 submission latency</li>
<li>clat表示从I/O提交到I/O完成的时长。 completion latency</li>
<li>lat表示从fio创建IO到IO完成的总时长</li>
</ul>
<p>fio支持I/O的重放，先用blktrace记录磁盘设备的I/O访问情况，然后使用fio重放blktrace的记录。</p>
<pre><code>blktrace /dev/sdb #跟踪磁盘IO
ls #查看blktrace记录的结果
blkparse sdb -d sdb.bin #将结果转化为二进制文件
fio --name=reply --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin #使用fio重放日志
</code></pre><h3 id="I-O性能优化"><a href="#I-O性能优化" class="headerlink" title="I/O性能优化"></a>I/O性能优化</h3><h4 id="应用程序优化"><a href="#应用程序优化" class="headerlink" title="应用程序优化"></a>应用程序优化</h4><ul>
<li>可以用追加写代替随机写，减少寻址开销，加快I/O写的速度</li>
<li>可以借助缓存I/O，充分利用系统缓存，降低实际I/O的次数 </li>
<li>在应用程序内部构建自己的缓存，或者用Redis等外部缓存。一方面能在应用程序内部控制缓存的数据和生命周期，另一方面可以降低其他应用程序使用缓存对自身的影响</li>
<li>需要频繁读写同一块磁盘空间时，可以用mmap代替read/write，减少内存的拷贝次数</li>
<li>在需要写同步的场景中，尽量将写请求合并，即可以用fsync()取代O_SYNC</li>
<li>在多个应用程序共享磁盘时，为了保证I/O不被某个应用完全占用，推荐使用cgroups的I/O子系统来限制进程/进程组的IOPS以及吞吐量</li>
<li>在使用CFQ调度器时，可以用ionice来调整进程的调度优先级，提高核心应用的I/O优先级。</li>
</ul>
<h4 id="文件系统优化"><a href="#文件系统优化" class="headerlink" title="文件系统优化"></a>文件系统优化</h4><ul>
<li>根据实际负载场景不同选择最适合的文件系统</li>
<li>选好文件系统后进一步优化文件系统的配置选项，包括文件系统的特性、日志模式、挂载选项等</li>
<li>优化文件系统的缓存</li>
<li>不需要持久化时可以用内存文件系统tmpfs来获取更好的IO性能。</li>
</ul>
<h4 id="磁盘优化"><a href="#磁盘优化" class="headerlink" title="磁盘优化"></a>磁盘优化</h4><ul>
<li>换用性能更好的磁盘， 如SSD替换HDD</li>
<li>使用RAID将多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。既可以提高数据的可靠性，又可以提升数据的访问性能</li>
<li>针对磁盘和应用程序IO模式特征，选择最合适的IO调度算法<ul>
<li>SSD和虚拟机中的磁盘，用noop调度算法</li>
<li>数据库应用，用deadline算法</li>
</ul>
</li>
<li>对应用程序的数据进行磁盘级别的隔离。 为日志、数据库等I/O压力大的应用配置单独的磁盘</li>
<li>顺序读多的场景增大磁盘的预读数据<ul>
<li>调整内核选项/sys/block/sdb/queue/read_ahead_kb， 默认为128KB</li>
<li>blockdev工具设置， blockdev –setra 8192 /dev/sdb， 这里单位为512B</li>
</ul>
</li>
<li>优化块设备的I/O选项<ul>
<li>调整磁盘队列的长度，/sys/block/sdb/queue/nr_requests</li>
</ul>
</li>
</ul>
<p>最后要注意磁盘本身是否存在硬件错误。 可以查看dmesg中是否有硬件I/O故障的日志。还可以用badblocks、smartctl等工具检测磁盘的硬件问题，或者用e2fsck来检测文件系统的错误。 如果发现问题可以用fsck来修复。</p>
<hr>
<h2 id="Lesson-32-答疑-（略）"><a href="#Lesson-32-答疑-（略）" class="headerlink" title="Lesson 32 答疑 （略）"></a>Lesson 32 答疑 （略）</h2><p>捞评论学习：</p>
<ol>
<li>数据写ES，运行一段时间后发现写入很慢，查IO时发现读IO很高写IO很少。用iotop定位es一些写的线程，将线程id转成16进制，用jstack打印出ES的堆栈信息，查处16进程的线程号的堆栈。发现原来时ES会根据doc id查数据，然后选择更新或新插入。ES数据量大时，会占用很多的读IO。 解决方法：写ES时不传入id，让es自动生成来解决。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2020/06/14/Linux性能优化实战第三周--内存性能篇/" itemprop="url">
                  《Linux 性能优化实战》第三周--内存性能篇
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2020-06-14T10:57:55+08:00" content="2020-06-14">
              2020-06-14
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>本周主要学习内存性能方面的检测与优化。首先在概念上更加系统的了解了Linux内存的工作原理。进程看到的内存是内核提供的虚拟内存，通过页表映射到实际的物理内存。进程通过malloc申请内存根据页面大小有两种不同的方式，并且内存并不是立即分配而是在首次访问时通过缺页异常在内核态进行分配并更新页表。</p>
<p>通过阅读文档以及实验了解了Buffer和Cache的区别。前者是对磁盘数据的缓存，后者是对文件数据的缓存，且<strong>两者均作用于读写操作</strong>。并掌握cachestat/cachetop/pcstat等工具如何检测系统缓存命中指标，在实验中掌握如何处理缓存异常的场景。</p>
<p>在内存资源紧张时，Linux通过直接回收和定期扫描的方式来释放文件页和匿名页。其中资源是否紧张可以通过内存的三个阈值来判断。另外我们可以手动调整内存资源配置，例如修改 /proc/sys/vm/min_free_kbytes来调整内存阈值，/proc/sys/vm/swappiness来调整文件页和匿名页回收倾向。在NUMA架构下还可以设置/proc/sys/vm/zone_reclaim_node来调整本地内存的回收策略。</p>
<p>当Swap变高时，可以用sar，/proc/zoneinfo，/proc/pid/status等方法查看系统or进程的内存使用情况，进而找到Swap升高的根源和受影响的进程。不过通常我们禁止Swap的使用来提升系统的整体性能：</p>
<ul>
<li>内存足够大时，禁用Swap</li>
<li>实在需要Swap时，可以尝试降低swapiness的值，减少回收时Swap的使用倾向</li>
<li>响应延迟敏感的应用，可以用mlock/mlockall来锁定内存，禁止内存换出</li>
</ul>
<p>之前在搭建组内K8S环境时按照教程都是先关闭Swap，不明所以。现在通过这周的学习才真正了解到缘由。</p>
<p>本周对内存使用情况监测所用的主要工具有：</p>
<ul>
<li>常用性能工具： free/top/ps，vmstat观察内存变化情况</li>
<li>查看缓存命中情况： bcc包中的cachestat和cachetop，基于Linux内核的eBPF(extend Berkeley Packet Filters)来跟踪内核中管理的缓存<ul>
<li>cachestat 查看整个操作系统缓存的读写命中情况</li>
<li>cachetop 提供了每个进程的缓存命中情况 </li>
</ul>
</li>
<li>跟踪内存分配/释放： memleak    </li>
<li>查看内存各个指标变化： sar</li>
</ul>
<p>对于系统内存问题的分析与定位，通常先运行几个覆盖面比较大的性能工具，如free，top，vmstat，pidstat等</p>
<ul>
<li>先用free和top查看系统整体内存使用情况</li>
<li>再用vmstat和pidstat，查看一段时间的趋势，从而判断内存问题的类型</li>
<li>最后进行详细分析，比如内存分配分析，缓存/缓冲区分析，具体进程的内存使用分析等 </li>
</ul>
<p><img src="/2020/06/14/Linux性能优化实战第三周--内存性能篇/analysis.png" alt> </p>
<p>以及一些常见的优化思路：</p>
<ul>
<li>最好禁止Swap，若必须开启则尽量降低swappiness的值</li>
<li>减少内存的动态分配，如可以用内存池，HugePage等</li>
<li>尽量使用缓存和缓冲区来访问数据。如用堆栈明确声明内存空间来存储需要缓存的数据，或者用Redis外部缓存组件来优化数据的访问</li>
<li>cgroups等方式来限制进程的内存使用情况，确保系统内存不被异常进程耗尽</li>
<li>/proc/pid/oom_adj调整核心应用的oom_score，保证即使内存紧张核心应用也不会被OOM杀死 </li>
</ul>
<p>另外，在探索问题的过程中由于性能指标较多，我们不可能记住所有指标的详细含义，网上搜索有时并不能得到真正准确的答案，因此养成查文档的爱好非常重要。</p>
<hr>
<p>接下来是本周读书笔记</p>
<hr>
<h2 id="Lesson-15-Linux内存是怎么工作的"><a href="#Lesson-15-Linux内存是怎么工作的" class="headerlink" title="Lesson 15 Linux内存是怎么工作的"></a>Lesson 15 Linux内存是怎么工作的</h2><h3 id="内存映射"><a href="#内存映射" class="headerlink" title="内存映射"></a>内存映射</h3><p>大多数计算机用的主存都是动态随机访问内存(DRAM)，只有内核才可以直接访问物理内存。Linux内核给每个进程提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样进程就可以很方便的访问内存(虚拟内存)。</p>
<p>虚拟地址空间的内部分为内核空间和用户空间两部分，不同字长的处理器地址空间的范围不同。32位系统内核空间占用1G，用户空间占3G。 64位系统内核空间和用户空间都是128T，分别占内存空间的最高和最低处，中间部分为未定义。</p>
<p>并不是所有的虚拟内存都会分配物理内存，只有实际使用的才会。分配后的物理内存通过内存映射管理。为了完成内存映射，内核为每个进程都维护了一个页表，记录虚拟地址和物理地址的映射关系。页表实际存储在CPU的内存管理单元MMU中，处理器可以直接通过硬件找出要访问的内存。</p>
<p>当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存，更新进程页表，再返回用户空间恢复进程的运行。</p>
<p>MMU以页为单位管理内存，页大小4KB。为了解决页表项过多问题Linux提供了<strong>多级页表</strong>和<strong>HugePage</strong>的机制。</p>
<h3 id="虚拟内存空间分布"><a href="#虚拟内存空间分布" class="headerlink" title="虚拟内存空间分布"></a>虚拟内存空间分布</h3><p>从图中可以看出用户空间内存从低到高是五种不同的内存段：</p>
<ul>
<li><strong>只读段</strong> 代码和常量等</li>
<li><strong>数据段</strong> 全局变量等</li>
<li><strong>堆</strong> 动态分配的内存，从低地址开始向上增长</li>
<li><strong>文件映射</strong> 动态库、共享内存等，从高地址开始向下增长</li>
<li><strong>栈</strong> 包括局部变量和函数调用的上下文等，栈的大小是固定的。一般8MB</li>
</ul>
<h3 id="内存分配与回收"><a href="#内存分配与回收" class="headerlink" title="内存分配与回收"></a>内存分配与回收</h3><h4 id="分配"><a href="#分配" class="headerlink" title="分配"></a>分配</h4><p>malloc对应到系统调用上有两种实现方式：</p>
<ul>
<li><strong>brk()</strong> 针对小块内存(&lt;128K)，通过移动堆顶位置来分配。内存释放后不立即归还内存，而是被缓存起来。</li>
<li><strong>mmap()</strong>针对大块内存(&gt;128K)，直接用内存映射来分配，即在文件映射段找一块空闲内存分配。</li>
</ul>
<p>前者的缓存可以减少缺页异常的发生，提高内存访问效率。但是由于内存没有归还系统，在内存工作繁忙时，频繁的内存分配/释放会造成内存碎片。</p>
<p>后者在释放时直接归还系统，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁内存分配会导致大量缺页异常，使内核管理负担增加。</p>
<p>上述两种调用并没有真正分配内存，这些内存只有在首次访问时，才通过缺页异常进入内核中，由内核来分配。</p>
<h4 id="回收"><a href="#回收" class="headerlink" title="回收"></a>回收</h4><p>内存紧张时，系统通过以下方式来回收内存：</p>
<ul>
<li>回收缓存： LRU算法回收最近最少使用的内存页面；</li>
<li>回收不常访问内存： 把不常用的内存通过交换分区写入磁盘</li>
<li><p>杀死进程： OOM内核保护机制 （进程消耗内存越大oom_score越大，占用CPU越多oom_score越小，可以通过/proc手动调整oom_adj） </p>
<pre><code>echo -16 &gt; /proc/$(pidof XXX)/oom_adj
</code></pre></li>
</ul>
<h3 id="如何查看内存使用情况"><a href="#如何查看内存使用情况" class="headerlink" title="如何查看内存使用情况"></a>如何查看内存使用情况</h3><p>free来查看整个系统的内存使用情况</p>
<p>top/ps来查看某个进程的内存使用情况</p>
<ul>
<li><strong>VIRT</strong> 进程的虚拟内存大小</li>
<li><strong>RES</strong> 常驻内存的大小，即进程实际使用的物理内存大小，不包括swap和共享内存</li>
<li><strong>SHR</strong> 共享内存大小，与其他进程共享的内存，加载的动态链接库以及程序代码段</li>
<li><strong>%MEM</strong> 进程使用物理内存占系统总内存的百分比 </li>
</ul>
<hr>
<h2 id="Lesson-16-怎样理解内存中的Buffer和Cache？"><a href="#Lesson-16-怎样理解内存中的Buffer和Cache？" class="headerlink" title="Lesson 16 怎样理解内存中的Buffer和Cache？"></a>Lesson 16 怎样理解内存中的Buffer和Cache？</h2><h3 id="free数据来源"><a href="#free数据来源" class="headerlink" title="free数据来源"></a>free数据来源</h3><p>在free手册中可以看到buffer和cache的定义，但是并不能直观帮助我们理解</p>
<pre><code>buffers： Memory used by kernel buffers (Buffers in /proc/meminfo)
cache: Memory used by the page cache and slabs (Cache and Sreclaimable in /proc/meminfo)
</code></pre><h3 id="proc文件系统"><a href="#proc文件系统" class="headerlink" title="proc文件系统"></a>proc文件系统</h3><p>接着看proc文件系统中的文档可以看到： Buffers是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据(通常不会特别大)。Cached是从磁盘读取文件的页缓存，用来缓存从文件中读取的数据。Slab包括可回收和不可回收两部分。</p>
<pre><code>Buffers %lu: Relatively temporary storage for raw disk blocks that shouldn&apos;t get tremendously large (20MB or so)    

Cached %lu: In-memory cache for files read from the disk(the page cache). Doesn&apos;t include SwapCached.

SReclaimable %lu: Part of Slab, that might be reclaimed, such as ceches.

Sunreclaim %lu: Part of Slab, that cannot bt reclaimed on memory pressure.
</code></pre><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p><strong>该实验对环境要求较高，需要系用配置多块磁盘，并且分区/dev/sdb1处于未使用状态。如果不满足千万不要尝试，否则会对磁盘分区造成损坏</strong></p>
<p>首先安装sysstat包，然后清理系统缓存</p>
<pre><code>echo 3 &gt; /proc/sys/vm/drop_caches
</code></pre><h4 id="场景1-磁盘和文件写案例"><a href="#场景1-磁盘和文件写案例" class="headerlink" title="场景1 磁盘和文件写案例"></a>场景1 磁盘和文件写案例</h4><pre><code>vmstat 1 #空闲情况下查看系统内存使用情况
dd if=/dev/urandom of=/tmp/file bs=1M count=500 #通过读取随机设备，生产一个500MB大小的文件
#此时观察vmstat，发现cache在不断增长，但是Buffer基本保持不变
#Cache开始增长时，块设备IO很少，dd命令结束后cache不再增长，但块设备写还会持续一段时间

echo 3 &gt;/proc/sys/vm/drop_caches
dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048 #清理缓存后向磁盘分区写入2GB的随机数据
#此时观察vmstat输出，发现写磁盘会时buffer和cache都在增长，但是buffer增长快得多
</code></pre><p>通过该案例可以看出写文件时会用到cache缓存数据，写磁盘时会用到buffer来缓存数据。</p>
<h4 id="场景2-磁盘和文件读案例"><a href="#场景2-磁盘和文件读案例" class="headerlink" title="场景2 磁盘和文件读案例"></a>场景2 磁盘和文件读案例</h4><pre><code>echo 3 &gt; /proc/sys/vm/drop_caches
dd if=/tmp/file of=/dev/null
#观察vmstat输出，发现读取文件时buffer保持不变，cache不停增长

echo 3 &gt;/proc/sys/vm/drop_caches
dd if=/dev/sda1 of=/dev/null bs=1M count=1024
#观察vmstat发现读磁盘时，buffer和cache都在增长，但是buffer增长快得多
</code></pre><p><strong>通过上述实验可以看出buffer是对磁盘数据的缓存，cache是对文件数据的缓存，它们既会用在读请求也会用在写请求中。</strong></p>
<hr>
<h2 id="Lesson-17-如何利用系统缓存优化程序的运行效率"><a href="#Lesson-17-如何利用系统缓存优化程序的运行效率" class="headerlink" title="Lesson 17 如何利用系统缓存优化程序的运行效率"></a>Lesson 17 如何利用系统缓存优化程序的运行效率</h2><h3 id="缓存命中率"><a href="#缓存命中率" class="headerlink" title="缓存命中率"></a>缓存命中率</h3><p><strong>缓存命中率</strong>是指直接通过缓存获取数据的请求次数，占所有请求次数的百分比。<strong>命中率越高说明缓存带来的收益越高，应用程序的性能也就越好。</strong></p>
<p>安装bcc包后可以通过cachestat和cachetop来监测缓存的读写命中情况。</p>
<p>安装pcstat后可以查看文件在内存中的缓存大小以及缓存比例。</p>
<pre><code>#首先安装Go
export GOPATH=~/go
export PATH=~/go/bin:$PATH
go get golang.org/x/sys/unix
go ge github.com/tobert/pcstat/pcstat
</code></pre><h3 id="实验案例一-dd缓存加速"><a href="#实验案例一-dd缓存加速" class="headerlink" title="实验案例一 dd缓存加速"></a>实验案例一 dd缓存加速</h3><pre><code>dd if=/dev/sda1 of=file bs=1M count=512 #生产一个512MB的临时文件
echo 3 &gt; /proc/sys/vm/drop_caches #清理缓存
pcstat file #确定刚才生成文件不在系统缓存中，此时cached和percent都是0
cachetop 5
dd if=file of=/dev/null bs=1M #测试文件读取速度
#此时文件读取性能为30+MB/s，查看cachetop结果发现并不是所有的读都落在磁盘上，读缓存命中率只有50%。
dd if=file of=/dev/null bs=1M #重复上述读文件测试
#此时文件读取性能为4+GB/s，读缓存命中率为100%
pcstat file #查看文件file的缓存情况，100%全部缓存
</code></pre><p>实验表明系统缓存对第二次dd命令有明显的加速效果，大大提高了文件读取的性能。同时要注意如果我们把dd作为性能测试工具时，由于缓存存在会导致测试结果严重失真。</p>
<h3 id="实验案例二-O-DIRECT选项绕过系统缓存"><a href="#实验案例二-O-DIRECT选项绕过系统缓存" class="headerlink" title="实验案例二 O_DIRECT选项绕过系统缓存"></a>实验案例二 O_DIRECT选项绕过系统缓存</h3><pre><code>cachetop 5
sudo docker run --privileged --name=app -itd feisky/app:io-direct
sudo docker logs app #确认案例启动成功
#实验结果表明每读32MB数据都要花0.9s，且cachetop输出中显示1024次缓存全部命中
</code></pre><p>但是凭感觉可知如果缓存命中读速度不应如此慢，读次数时1024，页大小为4K，五秒的时间内读取了1024*4KB数据，即每秒0.8MB，和结果中32MB相差较大。说明该案例没有充分利用缓存，怀疑系统调用设置了直接I/O标志绕过系统缓存。因此接下来观察系统调用</p>
<pre><code>strace -p $(pgrep app)
#strace 结果可以看到openat打开磁盘分区/dev/sdb1，传入参数为O_RDONLY|O_DIRECT
</code></pre><p>这就解释了为什么读32MB数据那么慢，直接从磁盘读写肯定远远慢于缓存。找出问题后我们再看案例的源代码发现flags中指定了直接IO标志。删除该选项后重跑，验证性能变化。</p>
<hr>
<h2 id="Lesson-18-内存泄漏，如何定位和处理？"><a href="#Lesson-18-内存泄漏，如何定位和处理？" class="headerlink" title="Lesson 18 内存泄漏，如何定位和处理？"></a>Lesson 18 内存泄漏，如何定位和处理？</h2><p>对应用程序来说，动态内存的分配和回收是核心又复杂的一个逻辑功能模块。管理内存的过程中会发生各种各样的“事故”：</p>
<ul>
<li>没正确回收分配的内存，导致了泄漏</li>
<li>访问的是已分配内存边界外的地址，导致程序异常退出</li>
<li>…</li>
</ul>
<h3 id="内存的分配与回收"><a href="#内存的分配与回收" class="headerlink" title="内存的分配与回收"></a>内存的分配与回收</h3><p>在Lesson15中我们了解到了虚拟内存分布从低到高分别是<strong>只读段，数据段，堆，内存映射段，栈</strong>五部分。其中会导致内存泄漏的是：</p>
<ul>
<li>堆： 由应用程序自己来分配和管理，除非程序退出这些堆内存不会被系统自动释放。</li>
<li>内存映射段：包括动态链接库和共享内存，其中共享内存由程序自动分配和管理</li>
</ul>
<p><strong>内存泄漏的危害比较大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。</strong> 内存泄漏不断累积甚至会耗尽系统内存。</p>
<h3 id="实验-如何检测内存泄漏"><a href="#实验-如何检测内存泄漏" class="headerlink" title="实验 如何检测内存泄漏"></a>实验 如何检测内存泄漏</h3><p>预先安装systat，docker，bcc</p>
<pre><code>sudo docker run --name=app -itd feisky/app:mem-leak
sudo docker logs app
vmstat 3
</code></pre><p>可以看到free在不断下降，buffer和cache基本保持不变。说明系统的内存一致在升高。但并不能说明存在内存泄漏。此时可以通过memleak工具来跟踪系统或进程的内存分配/释放请求。</p>
<pre><code>/usr/share/bcc/tools/memleak -a -p $(pidof app)
</code></pre><p>从memleak输出可以看到，应用在不停地分配内存，并且这些分配的地址并没有被回收。通过调用栈看到是fibonacci函数分配的内存没有释放。定位到源码后查看源码来修复增加内存释放函数即可。</p>
<p>另外，在该实验中也可以通过将动态分配的内存改为数组来避免内存泄漏的问题，数据放在栈中由系统自动分配与回收。</p>
<hr>
<h2 id="Lesson-19-20-为什么系统的Swap变高"><a href="#Lesson-19-20-为什么系统的Swap变高" class="headerlink" title="Lesson 19/20 为什么系统的Swap变高"></a>Lesson 19/20 为什么系统的Swap变高</h2><p>系统内存资源紧张时通过内存回收和OOM杀死进程来解决。其中可回收内存包括：</p>
<ul>
<li>缓存/缓冲区，属于可回收资源，在文件管理中通常叫做文件页<ul>
<li>被应用程序修改过暂时没写入磁盘的数据(脏页)，要先写入磁盘然后才能内存释放<ul>
<li>在应用程序中通过fsync将脏页同步到磁盘</li>
<li>交给系统，内核线程pdflush负责这些脏页的刷新</li>
</ul>
</li>
</ul>
</li>
<li>内存映射获取的文件映射页，也可以被释放掉，下次访问时从文件重新读取</li>
</ul>
<p>对于程序自动分配的堆内存，也就是我们在内存管理中的匿名页，虽然这些内存不能直接释放，但是Linux提供了Swap机制将不常访问的内存写入到磁盘来释放内存，再次访问时从磁盘读取到内存即可。</p>
<h3 id="Swap原理"><a href="#Swap原理" class="headerlink" title="Swap原理"></a>Swap原理</h3><p>Swap本质就是把一块磁盘空间或者一个本地文件当作内存来使用，包括换入和换出两个过程：</p>
<ul>
<li>换出： 将进程暂时不用的内存数据存储到磁盘中，并释放这些内存</li>
<li>换入： 进程再次访问内存时，将它们从磁盘读到内存中</li>
</ul>
<p>Linux如何衡量内存资源是否紧张？</p>
<ul>
<li><strong>直接内存回收</strong> 新的大块内存分配请求，但剩余内存不足。此时系统会回收一部分内存；</li>
<li><p><strong>kswapd0</strong> 内核线程定期回收内存。为了衡量内存使用情况，定义了pages_min,pages_low,pages_high三个阈值，并根据其来进行内存的回收操作。</p>
<ul>
<li>剩余内存 &lt; pages_min，进程可用内存耗尽了，只有内核才可以分配内存</li>
<li>pages_min &lt; 剩余内存 &lt; pages_low,内存压力较大，kswapd0执行内存回收，直到剩余内存 &gt; pages_high</li>
<li>pages_low &lt; 剩余内存 &lt; pages_high，内存有一定压力，但可以满足新内存请求</li>
<li><p>剩余内存 &gt; pages_high，说明剩余内存较多，无内存压力</p>
<p>pages_low = pages_min <em> 5 / 4<br>pages_high = pages_min </em> 3 / 2</p>
</li>
</ul>
</li>
</ul>
<h3 id="NUMA-与-SWAP"><a href="#NUMA-与-SWAP" class="headerlink" title="NUMA 与 SWAP"></a>NUMA 与 SWAP</h3><p>很多情况下系统剩余内存较多，但SWAP依旧升高，这是由于处理器的NUMA架构。</p>
<p>在NUMA架构下多个处理器划分到不同的Node，每个Node都拥有自己的本地内存空间。在分析内存的使用时应该针对每个Node单独分析。</p>
<pre><code>numactl --hardware #查看处理器在Node的分布情况，以及每个Node的内存使用情况
</code></pre><p>内存三个阈值可以通过/proc/zoneinfo来查看，该文件中还包括活跃和非活跃的匿名页/文件页数。</p>
<p>当某个Node内存不足时，系统可以从其他Node寻找空闲资源，也可以从本地内存中回收内存。 通过/proc/sys/vm/zone_raclaim_mode来调整。</p>
<ul>
<li>0表示既可以从其他Node寻找空闲资源，也可以从本地回收内存</li>
<li>1，2，4表示只回收本地内存，2表示可以会回脏数据回收内存，4表示可以用Swap方式回收内存。</li>
</ul>
<h3 id="swappiness"><a href="#swappiness" class="headerlink" title="swappiness"></a>swappiness</h3><p>在实际回收过程中Linux根据/proc/sys/vm/swapiness选项来调整使用Swap的积极程度，从0-100，数值越大越积极使用Swap，即更倾向于回收匿名页；数值越小越消极使用Swap，即更倾向于回收文件页。</p>
<p><strong>注意：这只是调整Swap积极程度的权重，即使设置为0，当剩余内存+文件页小于页高阈值时，还是会发生Swap。</strong></p>
<h3 id="实验-Swap升高时如何定位分析"><a href="#实验-Swap升高时如何定位分析" class="headerlink" title="实验 Swap升高时如何定位分析"></a>实验 Swap升高时如何定位分析</h3><pre><code>free #首先通过free查看swap使用情况，若swap=0表示未配置Swap
#先创建并开启swap
fallocate -l 8G /mnt/swapfile
chmod 600 /mnt/swapfile
mkswap /mnt/swapfile
swapon /mnt/swapfile

free #再次执行free确保Swap配置成功

dd if=/dev/sda1 of=/dev/null bs=1G count=2048 #模拟大文件读取
sar -r -S 1  #查看内存各个指标变化 -r内存 -S swap
#根据结果可以看出，%memused在不断增长，剩余内存kbmemfress不断减少，缓冲区kbbuffers不断增大，由此可知剩余内存不断分配给了缓冲区
#一段时间之后，剩余内存很小，而缓冲区占用了大部分内存。此时Swap使用之间增大，缓冲区和剩余内存只在小范围波动

停下sar命令
cachetop5 #观察缓存
#可以看到dd进程读写只有50%的命中率，未命中数为4w+页，说明正式dd进程导致缓冲区使用升高
watch -d grep -A 15 ‘Normal’ /proc/zoneinfo #观察内存指标变化
#发现升级内存在一个小范围不停的波动，低于页低阈值时会突然增大到一个大于页高阈值的值
</code></pre><p>说明剩余内存和缓冲区的波动变化正是由于内存回收和缓存再次分配的循环往复。有时候Swap用的多，有时候缓冲区波动更多。此时查看swappiness值为60，是一个相对中和的配置，系统会根据实际运行情况来选去合适的回收类型。</p>
<hr>
<h2 id="Lesson-21-套路篇：如何“快准狠”找到系统内存存在的问题"><a href="#Lesson-21-套路篇：如何“快准狠”找到系统内存存在的问题" class="headerlink" title="Lesson 21 套路篇：如何“快准狠”找到系统内存存在的问题"></a>Lesson 21 套路篇：如何“快准狠”找到系统内存存在的问题</h2><h3 id="内存性能指标"><a href="#内存性能指标" class="headerlink" title="内存性能指标"></a>内存性能指标</h3><p><strong>系统内存指标</strong></p>
<ul>
<li>已用内存/剩余内存</li>
<li>共享内存 （tmpfs实现）</li>
<li>可用内存： 包括剩余内存和可回收内存</li>
<li>缓存：磁盘读取文件的页缓存，slab分配器中的可回收部分</li>
<li>缓冲区： 原始磁盘块的临时存储，缓存将要写入磁盘的数据</li>
</ul>
<p><strong>进程内存指标</strong></p>
<ul>
<li>虚拟内存： 5大部分</li>
<li>常驻内存： 进程实际使用的物理内存，不包括Swap和共享内存</li>
<li>共享内存： 与其他进程共享的内存，以及动态链接库和程序的代码段</li>
<li>Swap内存： 通过Swap换出到磁盘的内存</li>
</ul>
<p><strong>缺页异常</strong></p>
<ul>
<li>可以直接从物理内存中分配，次缺页异常</li>
<li>需要磁盘IO介入(如Swap)，主缺页异常。 此时内存访问会慢很多</li>
</ul>
<h3 id="内存性能工具"><a href="#内存性能工具" class="headerlink" title="内存性能工具"></a>内存性能工具</h3><p>根据不同的性能指标来找合适的工具:<br><img src="/2020/06/14/Linux性能优化实战第三周--内存性能篇/metric_tool.png" alt> </p>
<p>内存分析工具包含的性能指标:<br><img src="/2020/06/14/Linux性能优化实战第三周--内存性能篇/tool_metric.png" alt> </p>
<h3 id="如何迅速分析内存的性能瓶颈"><a href="#如何迅速分析内存的性能瓶颈" class="headerlink" title="如何迅速分析内存的性能瓶颈"></a>如何迅速分析内存的性能瓶颈</h3><p>通常先运行几个覆盖面比较大的性能工具，如free，top，vmstat，pidstat等</p>
<ul>
<li>先用free和top查看系统整体内存使用情况</li>
<li>再用vmstat和pidstat，查看一段时间的趋势，从而判断内存问题的类型</li>
<li>最后进行详细分析，比如内存分配分析，缓存/缓冲区分析，具体进程的内存使用分析等</li>
</ul>
<p>常见的优化思路：</p>
<ul>
<li>最好禁止Swap，若必须开启则尽量降低swappiness的值</li>
<li>减少内存的动态分配，如可以用内存池，HugePage等</li>
<li>尽量使用缓存和缓冲区来访问数据。如用堆栈明确声明内存空间来存储需要缓存的数据，或者用Redis外部缓存组件来优化数据的访问</li>
<li>cgroups等方式来限制进程的内存使用情况，确保系统内存不被异常进程耗尽</li>
<li>/proc/pid/oom_adj调整核心应用的oom_score，保证即使内存紧张核心应用也不会被OOM杀死 </li>
</ul>
<h2 id="Lesson-22-答疑-（略）"><a href="#Lesson-22-答疑-（略）" class="headerlink" title="Lesson 22 答疑 （略）"></a>Lesson 22 答疑 （略）</h2>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Frances Hu" />
          <p class="site-author-name" itemprop="name">Frances Hu</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">31</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Frances Hu</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
