<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Frances Hu's Blog">
<meta property="og:url" content="http://xiaozhazi.github.io/page/2/index.html">
<meta property="og:site_name" content="Frances Hu's Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Frances Hu's Blog">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://xiaozhazi.github.io/page/2/"/>

  <title> Frances Hu's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Frances Hu's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Born to be wild!</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/16/Kubernetes学习——etcd/" itemprop="url">
                  Kubernetes学习——etcd
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2019-06-16T22:57:55+08:00" content="2019-06-16">
              2019-06-16
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="K8S学习之——ETCD"><a href="#K8S学习之——ETCD" class="headerlink" title="K8S学习之——ETCD"></a>K8S学习之——ETCD</h2><p>Etcd is distributed reliable key-value store for the most critical data of a distributed system, with a focus on being:  </p>
<ul>
<li><strong>Simple</strong>: well-defined, user-facing API (gRPC)  </li>
<li><strong>Secure</strong>: automatic TLS with optional client cert authentication  </li>
<li><strong>Fast</strong>: benchmarked 10,000 writes/sec  </li>
<li><strong>Reliable</strong>: properly distributed using Raft<br>简单来说etcd是kubernetes提供默认的存储系统，通过分布式KV存储来保存集群中重要的数据，因此我们使用时需要为etcd数据提供备份计划。</li>
</ul>
<p>Etcd is written in Go and uses the Raft consensus algorithm to manage a highly-available replicated log.<br>它通过raft一致性算法来实现可靠的分布式存储，我们上个月在team meetup中也是主要探讨了raft算法原理以及它与zookeeper的一些区别，在此通过本文记录下。  </p>
<h3 id="Raft算法原理"><a href="#Raft算法原理" class="headerlink" title="Raft算法原理"></a>Raft算法原理</h3><p>Raft算法和zookeeper类似，都是通过大多数机制来保证一致性。在Raft算法中通过选举出来的leader节点来接收客户端的请求日志数据，然后同步到集群中其它节点进行复制，当日志已经同步到超多半数以上节点的时候，该日志状态变为committed可提交状态，即可以提交到状态机中执行。此外leader节点还要通知其他节点哪些日志已经被复制成功。</p>
<p><em>Raft算法将要解决的一致性问题分为以下三个子问题：</em>  </p>
<ul>
<li><strong>leader选举：</strong> 通过心跳机制来触发leader选举，保证集群中存在一个leader节点  </li>
<li><strong>日志复制：</strong> leader节点将来自客户端的请求序列化成日志数据并且同步复制到其它节点  </li>
<li><strong>安全性：</strong> 如果某个节点已经将一条日志数据提交到状态机中执行，那么其他节点不可能再将此条数据输入到状态机中重复执行。  </li>
</ul>
<p><em>Raft算法需要保证以下几个特性：</em>  </p>
<ul>
<li><strong>Election Safety：</strong>在一个任期内最多只能存在一个leader节点  </li>
<li><strong>Leader Append-Only：</strong>leader节点只能添加日志数据，不会删除/覆盖以前的数据  </li>
<li><strong>Log Matching：</strong>如果两个节点的日志在某个索引上的日志数据和任期号都相同，那么在此index之前的日志数据一定也匹配  </li>
<li><strong>Leader Completeness：</strong>如果一条日志在某个任期被提交，那么该条日志数据在leader节点上更高任期号的日志数据中一定存在。  </li>
<li><strong>State Machine Safety：</strong>一条提交到raft状态机执行过的数据不可能再被其他节点提交执行。</li>
</ul>
<h4 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h4><p><strong><em>Node节点状态</em></strong>  </p>
<ul>
<li>Leader： 领导者，一个集群中只能存在一个leader  </li>
<li>Follower：跟随者，一个客户端的操作请求发送到follower上面，会首先由follwer重定向到leader上  </li>
<li>Candidate：参与者，在此状态的节点会发起新的选举  </li>
</ul>
<p>节点状态切换如图所示<br><img src="/2019/06/16/Kubernetes学习——etcd/raft-states.jpg" alt>  </p>
<ol>
<li>所有节点启动时自动进入Follower状态</li>
<li>Follower节点启动时会开启一个选举超时的定时器，当times out时节点切换到Candidate状态发起选举</li>
<li>节点一旦转换为Candidate状态就开始进行选举，如果在下一次选举超时到来之前都没有选出Leader节点，那么节点会保持状态不变重新发起一次新的选举</li>
<li>若Candidate节点收到集群中超过半数的节点选票（包含自身），那么状态切换到Leader</li>
<li>若Candidate节点接收到来自Leader节点的消息，或者任期号更高的消息，即集群中已经存在Leader节点，此时节点状态切换回Follower</li>
<li>Leader节点如果收到来自更高任期号的消息则切换回Follower节点（常见网络分区场景下）</li>
</ol>
<p><strong><em>其中节点之间的通信通过RPC来完成，etcd中主要由两种RPC请求：</em></strong>  </p>
<ul>
<li>RequestVote 用于Candidate节点发起选举  </li>
<li>AppendEntries 用于Leader节点向其他节点进行复制日志数据，以及同步心跳</li>
</ul>
<p><strong><em>Raft算法通过心跳机制来触发Leader选举</em></strong></p>
<p>节点启动时状态为Follower，只要一直接收到来自Leader或者Candidate的正常RPC消息，就会一直保持为Follower状态。<br>Leader节点周期性的向其它节点发送心跳请求来保持Leader状态，其中心跳请求即带有空数据的AppendEntries。<br>每个Follower节点都有一个选举超时定时器，如果在超时之前都没有接收到Leader的心跳请求将发起选举。发起选举时，Follower将自身的任期号+1并且切换到Candidate状态，然后向集群中其它节点发送RequestVote请求进行选举。  </p>
<p><strong><em>Candidate节点保持状态不变，直到以下情况之一发生：</em></strong>   </p>
<ul>
<li>该节点收到集群中半数以上的节点投票，即赢得选举；  </li>
<li>节点收到来自Leader节点的数据；  </li>
<li>选举超时到来</li>
</ul>
<p>第一种情况，因为每个Follower节点在一个任期内只能给一个节点投票先来先到，通过大多数原则保证了每个任期最多只有一个节点赢得选票（存在无节点当选的情况）。当节点成为Leader后会向集群中其它节点发送心跳消息来阻止不必要的选举。<br>第二种情况，处于Candidate状态下的节点收到了来自其它节点的心跳/AppendEntries消息。如果AppendEntries请求中的任期号大于自身任期，说明集群中已经存在Leader节点，此时节点转换成Follower状态。反之如果消息中任期号小于自身，则拒绝该消息，继续保持节点状态不变。<br>第三种情况，在选举超时到来时该节点没有赢得选举且没有收到其它节点的AppendEntries消息，说明集群中没有Leader存在，该Candidate节点任期号+1，再次发起选举。<br>其中，第三种情况发生在集群节点为偶数个，且有两个Candidate节点同时选举。理论上这种无法选择Leader的情况可以无限循环下去。为了避免该情况，每个节点的选举超时时间通过随机函数决定，一般在150ms-300ms之间。 （<strong><em>我们也仔细讨论了这种情况，因为Leader发送心跳消息时间非常短暂，如果节点选举超时时间T都相同，Leader节点挂掉那么可以极端得考虑认为所有Follower节点会在同一时刻（距离上一个心跳消息T时间）得到该信息。此时会有很多Follower节点切换到Candidate状态并进行选举，任意一个节点获得大多数投票的概率都很低。 而如果每个节点的超时时间是个随机值，那么大概率集群中有一个超时时间短的节点先感知到原Leader挂掉的事实，然后首先发起选举，因为RPC消息时间耗时较短，该节点大概率能获得大多数投票。</em></strong>）  </p>
<h4 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h4><p><strong><em>日志形式</em></strong></p>
<p><img src="/2019/06/16/Kubernetes学习——etcd/log-entries.jpg" alt>  </p>
<p>如上图所示，日志主要由以下三个成员组成：  </p>
<ol>
<li>index：索引号，即图中最上方的数字  </li>
<li>term：任期号，每个日志条目中上方数字，表示该日志在哪个任期中生成  </li>
<li>command：日志中的数据修改操作  </li>
</ol>
<p>如果一条日志被Leader节点同步到超过半数的节点中，被称为“<strong>成功复制</strong>”，这个日志条目就是committed状态（已经被提交）。如果某条日志条目状态为committed，那么在此日志之前的所有日志也都是committed状态。（如图中index&lt;=7的日志都是committed状态）<br>committed状态的日志可以被Leader输入到raft状态机中执行，执行过的日志状态为applied。  </p>
<p><strong><em>日志复制过程</em></strong></p>
<p><img src="/2019/06/16/Kubernetes学习——etcd/log-replication-1.jpg" alt><br><img src="/2019/06/16/Kubernetes学习——etcd/log-replication-2.jpg" alt><br><img src="/2019/06/16/Kubernetes学习——etcd/log-replication-3.jpg" alt></p>
<ol>
<li>首先客户端的消息转发到集群Leader节点上</li>
<li>Leader节点首先在本地日志中添加一条日志</li>
<li>Leader节点通过AppendEntries RPC消息的形式向集群中其它节点广播该日志</li>
<li>Follower节点收到日志首先也同样在本地日志中添加一条日志</li>
<li>Follower节点向Leader节点返回AppendEntries应答消息</li>
<li>Leader节点收到超过半数的应答消息时认为该日志已经被成功复制，将本地的committedIndex指向该日志的index</li>
<li>如果该committed状态的日志被提交到状态机中执行，那么Leader节点本地日志中的appliedIndex也要随之更新</li>
<li>Leader节点在下次广播AppendEntries消息时会带上最新的committedIndex和appliedIndex，用于Follower节点本地日志的索引更新。</li>
</ol>
<p><strong><em>日志恢复过程</em></strong></p>
<p>正常情况下Follower节点和Leader节点的日志会始终保持一致，但是当Leader节点突然挂掉时集群中节点日志会出现不一致的情况，当新的Leader选举出来后就需要进行相应的恢复操作。<br><img src="/2019/06/16/Kubernetes学习——etcd/inconsistentlog.jpg" alt></p>
<p>Raft算法通过Leader节点同步来解决数据不一致问题。 对于集群中任意一个节点，Leader中都存储着两个与它们日志相关的数据。  </p>
<ol>
<li>nextIndex： 下一次向该Follower节点同步时的日志索引  </li>
<li>matchIndex： 该Follower节点的最大日志索引<br>在Follower节点和Leader节点日志复制正常的情况下，nextIndex = matchIndex + 1。如果日志不一致情况出现那么该等式不成立。<br>集群中原Leader宕机，新Leader选出后将初始化nextIndex为新Leader节点的最后一条日志索引，matchIndex均为0。这样做的目的是首先从Follower节点的最后一条日志进行探索，如果不匹配则从前向后进行复制。<br>如上图所示，Leader节点存储每个Follower节点的nextIndex=10，matchIndex=0。成为Leader首次向其它节点复制日志时，会复制index&gt;=10的日志，同时带上2⃣️元组<6,10>告知Follower节点将要复制Term=6，index=10的日志数据。  </6,10></li>
</ol>
<ul>
<li>只有（a）节点的最大日志数据2⃣️元组<6,9>和Leader传来的<6,10>紧邻，复制成功且matchIndex也随之更新。  </6,10></6,9></li>
<li>其他几个节点的2⃣️元组都不匹配，返回复制失败。Leader节点收到拒绝消息后修改该follwer节点的索引数据，将nextIndex复制为matchIndex+1即1。下次同步时从索引1到10的数据给该Follwer节点，且Follower节点中未提交的数据被清除。  </li>
</ul>
<h4 id="安全性"><a href="#安全性" class="headerlink" title="安全性"></a>安全性</h4><p>Raft算法对选举进行限制来保证安全性。  </p>
<ul>
<li>一个节点向成为Leader就必须得到集群中超过半数的选票  </li>
<li>节点A投票给另一个节点B的必要条件时B的日志比A的要新（首先对比任期号，相同时再比较索引）</li>
</ul>
<h3 id="Raft算法处理只读请求"><a href="#Raft算法处理只读请求" class="headerlink" title="Raft算法处理只读请求"></a>Raft算法处理只读请求</h3><p>一般情况下，客户端的命令需要经过日志复制过程，当集群超过半数节点应答之后才可以提交给状态机执行，执行结束后再应答给客户端。这样的流程对于只读数据来说太漫长，如果不经过这样的流程Leader节点直接将本节点上保存的数据返回给客户端又是不安全的，因为可能返回的数据已经过期。因此Raft中针对只读请求有特殊的高效处理：  </p>
<ol>
<li>Leader节点需要有当前已提交的日志数据。如果该节点新成为Leader需要提交一个dummy空日志来确保上一个任期的日志全部提交。  </li>
<li>Leader节点保存该只读请求到来时的committedIndex为readIndex  </li>
<li>Leader节点确认自己是否还是集群中的leader（广播心跳给集群中节点，收到半数以上应答即可判断）同时readIndex索引也是当前集群日志AppliedIndex的最大索引  </li>
<li>读取Leader节点中数据  </li>
</ol>
<p>——————————————————————————————————————————————————————————————————<br>LastModify：2019-06-18</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/18/Dynamic-Programming/" itemprop="url">
                  Dynamic Programming
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2019-04-18T23:31:19+08:00" content="2019-04-18">
              2019-04-18
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>职业规划原因今年计划自学Go语言，前两个月也断断续续用Golang刷了100道左右的算法题。然而:(最近一个月工作比较忙就搁置了一段，今天看了下<a href="https://github.com/xiaozhazi/leetcode_go" target="_blank" rel="external">https://github.com/xiaozhazi/leetcode_go</a> 距上次commit已经有近一个月的时间了 - _ -#。</p>
<p>所以今天开始强制自己记录下个人解题过程以及go语言踩雷， 也便于随时复盘【其实是为了自我监督防止懒癌：）</p>
<h1 id="LeetCode真题系列"><a href="#LeetCode真题系列" class="headerlink" title="LeetCode真题系列"></a>LeetCode真题系列</h1><h2 id="Minimum-Path-Sum-64"><a href="#Minimum-Path-Sum-64" class="headerlink" title="Minimum Path Sum(64)"></a>Minimum Path Sum(64)</h2><p><a href="https://leetcode.com/problems/minimum-path-sum/" target="_blank" rel="external">64 Minumum Path Sum</a></p>
<h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>Given a m x n grid filled with non-negative numbers, find a path from top left to bottom right which minimizes the sum of all numbers along its path.</p>
<pre><code>Note: You can only move either down or right at any point in time.
</code></pre><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><pre><code>Input:
[
  [1,3,1],
  [1,5,1],
  [4,2,1]
]
Output: 7
Explanation: Because the path 1→3→1→1→1 minimizes the sum. 
</code></pre><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><p>从（0，0）走到（m-1，n-1）每个阶段只能选择向右或向下两种决策，且每个阶段都会对应一个状态集合。我们把状态定义为dp[i][j]表示从起点走到（i，j）位置时的最短路径长度。 因此它是一个多阶段最优解问题符合动态规划模型。</p>
<p><code>dp[i][j] = min(dp[i-1][j],dp[i][j-1]) + grid[i][j]</code><br>直接动手上代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">func minPathSum(grid [][]int) int &#123;</div><div class="line">  if grid == nil || len(grid) == 0 &#123;</div><div class="line">	return 0</div><div class="line">  &#125;</div><div class="line">  m, n := len(grid), len(grid[0])</div><div class="line">  dp := make([][]int, m)</div><div class="line">	</div><div class="line">  for i := 0; i &lt; m; i++ &#123;</div><div class="line">	dp[i] = make([]int, n)</div><div class="line">  &#125;</div><div class="line">	</div><div class="line">  dp[0][0] = grid[0][0]</div><div class="line">  for i := 1; i &lt; m; i++ &#123;</div><div class="line">	dp[i][0] = dp[i-1][0] + grid[i][0]</div><div class="line">  &#125;</div><div class="line">  for i := 1; i &lt; n; i++ &#123;</div><div class="line">	dp[0][i] = dp[0][i-1] + grid[0][i]</div><div class="line">  &#125;</div><div class="line">  for i := 1; i &lt; m; i++ &#123;</div><div class="line">	for j := 1; j &lt; n; j++ &#123;</div><div class="line">	  dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]</div><div class="line">	&#125;</div><div class="line">  &#125;</div><div class="line">  return dp[m-1][n-1]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>提交后发现 <em>Runtime: 12 ms, faster than 28.57% of Go online submissions.<br>Memory Usage: 4.4 MB, less than 13.64% of Go online submissions for Minimum Path Sum</em>. 效率太低了，想办法来优化下。</p>
<p>实际上我们只需要一个长度为n-1的一维数组来存储状态集合，状态转移的过程都可以基于这个一维数组来操作。继续上代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">func minPathSum(grid [][]int) int &#123;</div><div class="line">  if grid == nil || len(grid) == 0 &#123;</div><div class="line">    return 0</div><div class="line">  &#125;</div><div class="line">  m, n := len(grid), len(grid[0])</div><div class="line">  dp := make([]int, n)</div><div class="line">   </div><div class="line">  for j, _ := range dp &#123;</div><div class="line">    dp[j] = math.MaxInt32</div><div class="line">  &#125;</div><div class="line">  dp[0] = 0</div><div class="line">  for i := 0; i &lt; m; i++ &#123;</div><div class="line">    for j := 0; j &lt; n; j++ &#123;</div><div class="line">      if j == 0 &#123;</div><div class="line">        dp[j] += grid[i][j]</div><div class="line">      &#125; else &#123;</div><div class="line">        dp[j] = min(dp[j], dp[j-1]) + grid[i][j]</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  return dp[n-1]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>再次提交后 <em>Runtime: 8 ms, faster than 100.00% of Go online submissions for Minimum Path Sum.<br>Memory Usage: 3.9 MB, less than 59.09% of Go online submissions for Minimum Path Sum.</em>  : )</p>
<h2 id="Unique-Paths-II-63"><a href="#Unique-Paths-II-63" class="headerlink" title="Unique Paths II(63)"></a>Unique Paths II(63)</h2><p><a href="https://leetcode.com/problems/unique-paths-ii/" target="_blank" rel="external">63 Unique Paths II</a></p>
<h3 id="Question-1"><a href="#Question-1" class="headerlink" title="Question"></a>Question</h3><p>A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).</p>
<p>The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).</p>
<p>Now consider if some obstacles are added to the grids. How many unique paths would there be?</p>
<h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><pre><code>Input:
[
  [0,0,0],
  [0,1,0],
  [0,0,0]
]
Output: 2
Explanation:
There is one obstacle in the middle of the 3x3 grid above.
There are two ways to reach the bottom-right corner:
1. Right -&gt; Right -&gt; Down -&gt; Down
2. Down -&gt; Down -&gt; Right -&gt; Right
</code></pre><h3 id="Analysis-1"><a href="#Analysis-1" class="headerlink" title="Analysis"></a>Analysis</h3><p>第一感觉是可以用回溯来解决此问题。对每一个点先选择向右走，如果遇到障碍物或者右边无路说明此路径不通，回退到该点选择向下走，如果遇到障碍物或下边无路可走说明不存在任何一条路径可以经过该点走到终点。依次类推，每次递归向后走到终点时count加一。</p>
<p><strong>需要特别注意的是要开始OR结束点值为1的情况</strong></p>
<p>核心代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">func DFS(obstacleGrid [][]int, i, j int) &#123;</div><div class="line">    m, n := len(obstacleGrid), len(obstacleGrid[0])</div><div class="line">    if i == m - 1 &amp;&amp; j == n - 1  &amp;&amp; obstacleGrid[i][j] != 1&#123;</div><div class="line">        count++</div><div class="line">        return </div><div class="line">    &#125;</div><div class="line">    if i &lt; m - 1 &amp;&amp; obstacleGrid[i+1][j] == 0 &#123;</div><div class="line">        DFS(obstacleGrid, i + 1, j)</div><div class="line">    &#125;</div><div class="line">    if j &lt; n - 1 &amp;&amp; obstacleGrid[i][j+1] == 0 &#123;</div><div class="line">        DFS(obstacleGrid, i, j + 1)</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>递推算法基本类似于暴力求解，无重复又没有遗漏的穷举出所有走法。果不其然代码提交后TLE : (</p>
<p>继续分析这道题，对于图中任一个非障碍点(i,j)，从起始位置走到该点的路径个数可以通过走到该点的左边位置（i-1,j)和上方位置(i,j-1)的路径个数之和计算， 即</p>
<p><code>dp[i][j] = dp[i-1][j] + dp[i][j-1]</code> <em>if obstacleGrid[i][j] == 0</em></p>
<p>因此可以改用DP算法来解决，同样为了优化算法效率我们通过一个一维数组在存储状态集合，此时状态方程为<br><code>dp[i] += dp[i-1]</code> <em>if obstacleGrid[i][j] == 0</em></p>
<p>具体代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">func uniquePathsWithObstacles(obstacleGrid [][]int) int &#123;</div><div class="line">    if obstacleGrid == nil || len(obstacleGrid) == 0 &#123;</div><div class="line">        return 0</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    m, n, count := len(obstacleGrid), len(obstacleGrid[0]), 0</div><div class="line">    if (obstacleGrid[0][0] == 1 || obstacleGrid[m-1][n-1] == 1) &#123;</div><div class="line">        return count</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    dp := make([]int, n)</div><div class="line">    dp[0] = 1</div><div class="line">    for i := 0; i &lt; m; i++ &#123;</div><div class="line">        for j := 0; j &lt; n; j++ &#123;</div><div class="line">            if obstacleGrid[i][j] == 1 &#123;</div><div class="line">                dp[j] = 0 </div><div class="line">            &#125; else &#123;</div><div class="line">                if j != 0 &#123;</div><div class="line">                    dp[j] += dp[j-1]</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            </div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    return dp[n-1]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><em>Runtime: 0 ms, faster than 100.00% of Go online submissions for Unique Paths II.<br>Memory Usage: 2.6 MB, less than 66.67% of Go online submissions for Unique Paths II.</em></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/17/重归博客/" itemprop="url">
                  重归博客
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2019-04-17T23:38:48+08:00" content="2019-04-17">
              2019-04-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Hello-My-Personal-Blog"><a href="#Hello-My-Personal-Blog" class="headerlink" title="Hello, My Personal Blog"></a>Hello, My Personal Blog</h2><p>After nearly two years of work, i want to share what i have learned in my work and life.</p>
<p>Blogs can also help me to improve my ability to make things clearer and record my growth.</p>
<p>Let’s start!!!</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/31/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十一周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十一周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-12-31T15:54:03+08:00" content="2016-12-31">
              2016-12-31
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="11、Application-Example-Photo-OCR"><a href="#11、Application-Example-Photo-OCR" class="headerlink" title="11、Application Example: Photo OCR"></a>11、Application Example: Photo OCR</h1><h2 id="11-1、Photo-OCR"><a href="#11-1、Photo-OCR" class="headerlink" title="11.1、Photo OCR"></a>11.1、Photo OCR</h2><h3 id="11-1-1、problem-description-and-pipeline"><a href="#11-1-1、problem-description-and-pipeline" class="headerlink" title="11.1.1、problem description and pipeline"></a>11.1.1、problem description and pipeline</h3><p>photo OCR pipeline </p>
<ol>
<li>Text detection</li>
<li>Character segmentation</li>
<li>Character classification</li>
</ol>
<h3 id="11-1-2、sliding-window"><a href="#11-1-2、sliding-window" class="headerlink" title="11.1.2、sliding window"></a>11.1.2、sliding window</h3><p>对整个图片进行分别窗口化检测</p>
<h3 id="11-1-3、Getting-lots-of-data-Artificial-data-synthesis"><a href="#11-1-3、Getting-lots-of-data-Artificial-data-synthesis" class="headerlink" title="11.1.3、Getting lots of data: Artificial data synthesis"></a>11.1.3、Getting lots of data: Artificial data synthesis</h3><p>Synthesizing data by introducing distortions</p>
<ol>
<li>Distortion introduced should be representation of the type of noise/distortions in the test set.</li>
<li>Usually does not help to add purely random/meaningless noise to your data. </li>
</ol>
<h3 id="11-1-4、What-part-of-the-pipeline-to-work-on-next"><a href="#11-1-4、What-part-of-the-pipeline-to-work-on-next" class="headerlink" title="11.1.4、What part of the pipeline to work on next"></a>11.1.4、What part of the pipeline to work on next</h3><p>进行分块处理的目的是，我们可以很容易的分析出那一步骤是系统性能瓶颈，需要在那一步骤上投入更多的精力。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/11_1.png" alt="11_1"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/11_2.png" alt="11_2"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/23/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第十周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第十周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-12-23T15:53:56+08:00" content="2016-12-23">
              2016-12-23
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="10、Large-Scale-Machine-Learning"><a href="#10、Large-Scale-Machine-Learning" class="headerlink" title="10、Large Scale Machine Learning"></a>10、Large Scale Machine Learning</h1><h2 id="10-1、Gradient-Descent-with-Large-Datasets"><a href="#10-1、Gradient-Descent-with-Large-Datasets" class="headerlink" title="10.1、Gradient Descent with Large Datasets"></a>10.1、Gradient Descent with Large Datasets</h2><h3 id="10-1-1、Learning-with-large-dataset"><a href="#10-1-1、Learning-with-large-dataset" class="headerlink" title="10.1.1、Learning with large dataset"></a>10.1.1、Learning with large dataset</h3><p>It’s not who has the best algorithm that wins.<br>It’s who has the most data.</p>
<h3 id="10-1-2、Stochastic-gradient-descent"><a href="#10-1-2、Stochastic-gradient-descent" class="headerlink" title="10.1.2、Stochastic gradient descent"></a>10.1.2、Stochastic gradient descent</h3><p>不像Batch gradient descent每次迭代时都需要将数据集代入计算。</p>
<p>当数据集大的时候我们需要随机shuffle数据集，<br>然后选取第一个数据进行计算，然后进行梯度下降，再接着使用接下来的数据依次进行此过程。</p>
<h3 id="10-1-3、Mini-batch-gradient-descent"><a href="#10-1-3、Mini-batch-gradient-descent" class="headerlink" title="10.1.3、Mini-batch gradient descent"></a>10.1.3、Mini-batch gradient descent</h3><p>batch 和  stochastic的结合。每次选一组数据进行计算，然后再接着使用下一组重复此过程。</p>
<h3 id="10-1-4、Stochastic-gradient-descent-convergence"><a href="#10-1-4、Stochastic-gradient-descent-convergence" class="headerlink" title="10.1.4、Stochastic gradient descent convergence"></a>10.1.4、Stochastic gradient descent convergence</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/10_1.png" alt="10_1"></p>
<p>Learning rate α is tapically held constant. Can slowly descrease α over time if we want θ to converge.</p>
<h2 id="10-2、Online-Learning"><a href="#10-2、Online-Learning" class="headerlink" title="10.2、Online Learning"></a>10.2、Online Learning</h2><p>上述方法可以应用在实时在线学习上，数据集大小不固定，以数据流的形式出现。</p>
<h2 id="10-3、Map-reduce-and-data-parallelism"><a href="#10-3、Map-reduce-and-data-parallelism" class="headerlink" title="10.3、Map-reduce and data parallelism"></a>10.3、Map-reduce and data parallelism</h2><p>在进行梯度下降计算时，中间有步骤需要求和，我们可以利用Map-reduce和并行计算来缩短处理时间。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/16/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第九周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第九周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-12-16T10:56:39+08:00" content="2016-12-16">
              2016-12-16
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="9、Anomaly-detection"><a href="#9、Anomaly-detection" class="headerlink" title="9、Anomaly detection"></a>9、Anomaly detection</h1><h2 id="9-1、Density-Estimation"><a href="#9-1、Density-Estimation" class="headerlink" title="9.1、Density Estimation"></a>9.1、Density Estimation</h2><h3 id="9-1-1、Problem-motivation"><a href="#9-1-1、Problem-motivation" class="headerlink" title="9.1.1、Problem motivation"></a>9.1.1、Problem motivation</h3><p>密度估计，判断一个test实例是否为不正常的。</p>
<p>Anomaly detection example：</p>
<ol>
<li><p>Fraud detection</p>
<pre><code>xi = features of user i&apos;s ativities.
Model p(x) from data.
Identify unusual users by checking which have p(x)&lt;ε
</code></pre></li>
<li><p>Manufacturing</p>
</li>
<li><p>Monitoring computers in a data center.</p>
<pre><code>xi = features of machine i.
memory use,number of disk access/sec,cpu load...
</code></pre></li>
</ol>
<h3 id="9-1-2、Gaussian-distribution"><a href="#9-1-2、Gaussian-distribution" class="headerlink" title="9.1.2、Gaussian distribution"></a>9.1.2、Gaussian distribution</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/9_1.png" alt="9_1"></p>
<h3 id="9-1-3、Anomaly-detection-algorithm"><a href="#9-1-3、Anomaly-detection-algorithm" class="headerlink" title="9.1.3、Anomaly detection algorithm"></a>9.1.3、Anomaly detection algorithm</h3><ol>
<li>Choose features xi that might be indicative of anomalous examples.</li>
<li><p>Fit parameters μ1, … μn,σ12,…σn2</p>
<pre><code>μj = 1/m ξxji
σj2 = 1/m ξ(xji-μj)2
</code></pre></li>
<li>Given new example x, compute p(x),Anomaly if p(x)&lt;ε.</li>
</ol>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_2.png" alt="9_2"></p>
<h3 id="9-1-4、Developing-and-evaluating-an-anomaly-detection-system"><a href="#9-1-4、Developing-and-evaluating-an-anomaly-detection-system" class="headerlink" title="9.1.4、Developing and evaluating an anomaly detection system"></a>9.1.4、Developing and evaluating an anomaly detection system</h3><ol>
<li>The importance of real-number evaluation</li>
</ol>
<p>例如 10000 good engines， 20 flawed engines，我们可以进行如下划分：</p>
<pre><code>Training set：6000 good engines
CV:2000 good engines,10 anomalous
Test:2000 good engines,10 anomalous
</code></pre><ol>
<li>Algorithm evaluation  </li>
</ol>
<p>可以利用F1-score来评估算法，我们也可以用CV来选择参数ε。</p>
<h3 id="9-1-5、Anomaly-detection-VS-supervised-learning"><a href="#9-1-5、Anomaly-detection-VS-supervised-learning" class="headerlink" title="9.1.5、Anomaly detection VS supervised learning"></a>9.1.5、Anomaly detection VS supervised learning</h3><p>Anomaly detection:</p>
<ol>
<li>Very small number of positive example.</li>
<li>Large number of negative example.</li>
<li>Many different types od anomalies. 很难通过positive实例来学习异常的特征</li>
<li>未来和异常和目前的异常实例不相关</li>
</ol>
<p>Supervised Learning：</p>
<ol>
<li>Large number of positive and negative examples.</li>
<li>可以根据大量的positive值推断出其特征值，未来的positive和现在的训练集非常相似</li>
</ol>
<h3 id="9-1-6、多元高斯分布"><a href="#9-1-6、多元高斯分布" class="headerlink" title="9.1.6、多元高斯分布"></a>9.1.6、多元高斯分布</h3><p>通过μ矩阵和ξ矩阵来对多远高斯分布进行调整。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_3.png" alt="9_3"></p>
<h2 id="9-2-推荐系统"><a href="#9-2-推荐系统" class="headerlink" title="9.2 推荐系统"></a>9.2 推荐系统</h2><h3 id="9-3-1-基于内容的推荐"><a href="#9-3-1-基于内容的推荐" class="headerlink" title="9.3.1 基于内容的推荐"></a>9.3.1 基于内容的推荐</h3><p>问题描述：</p>
<ol>
<li>r(i,j)=1 if user j has rated movie i</li>
<li>y(i,j)=rating by user j on movie i</li>
<li>θ(j)=paramater vector for user j</li>
<li>x(i)=feature vector for movie i</li>
<li>For user j,movie i, predicted rating θ(j)T(x(i))</li>
<li>m(j)=no. of movies rated by user j</li>
</ol>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_4.png" alt="9_4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_5.png" alt="9_5"></p>
<h3 id="9-3-2-正交过滤"><a href="#9-3-2-正交过滤" class="headerlink" title="9.3.2 正交过滤"></a>9.3.2 正交过滤</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/9_6.png" alt="9_6"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/9_7.png" alt="9_7"></p>
<h3 id="9-3-2-实现技巧"><a href="#9-3-2-实现技巧" class="headerlink" title="9.3.2 实现技巧"></a>9.3.2 实现技巧</h3><p>归一化，计算平均值，然后同时减去该值</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/08/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第八周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第八周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-12-08T19:59:23+08:00" content="2016-12-08">
              2016-12-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="8、Unsupervised-Learning"><a href="#8、Unsupervised-Learning" class="headerlink" title="8、Unsupervised Learning"></a>8、Unsupervised Learning</h1><h2 id="8-1-Clustering"><a href="#8-1-Clustering" class="headerlink" title="8.1 Clustering"></a>8.1 Clustering</h2><h3 id="8-1-1-Unsupervised-Learning-Introduction"><a href="#8-1-1-Unsupervised-Learning-Introduction" class="headerlink" title="8.1.1 Unsupervised Learning: Introduction"></a>8.1.1 Unsupervised Learning: Introduction</h3><p>Applications of clustering:</p>
<ol>
<li>Market segementation</li>
<li>Social network analysis</li>
<li>Organize computing clusters</li>
<li>Astronomical data analysis</li>
</ol>
<p>无监督学习，数据是没有label的。</p>
<h3 id="8-1-2-K-Means-Algorithm"><a href="#8-1-2-K-Means-Algorithm" class="headerlink" title="8.1.2 K-Means Algorithm"></a>8.1.2 K-Means Algorithm</h3><p>Input:</p>
<pre><code>K (number of clusters)
Training set {x1,x2,...xm}
</code></pre><p>算法：</p>
<p>随机初始化K个聚类簇（cluster centroids）u1,u2…uK</p>
<p>然后repeat操作：</p>
<ol>
<li><p>cluster assignment step：</p>
<pre><code>for     i = 1 to m
  c(i):=index(from 1 to K) of cluster centroids closest to xi
</code></pre></li>
<li><p>Move centroid:</p>
<pre><code>for k = 1 to K
  uk := average(mean)of points assigned to cluster k
</code></pre></li>
</ol>
<h3 id="8-1-3-Optimization-objective"><a href="#8-1-3-Optimization-objective" class="headerlink" title="8.1.3 Optimization objective"></a>8.1.3 Optimization objective</h3><p>最小化distortion：<br><img src="http://ofacak8l3.bkt.clouddn.com/8_1.png" alt="8_1"></p>
<p>因此K-means算法也可以表示为：</p>
<ol>
<li>Randomly initialize K cluster centroids u1,u2,…uk</li>
<li><p>Repeat:</p>
<pre><code>for i = 1 to m
    c(i):= index(from 1 to K) of cluster centroid closest to xi
</code></pre></li>
</ol>
<pre><code>for k = 1 to K
    uk:= average(mean) of points assigned to cluster k [minize J]
</code></pre><h3 id="8-1-4-Random-initialization"><a href="#8-1-4-Random-initialization" class="headerlink" title="8.1.4 Random initialization"></a>8.1.4 Random initialization</h3><p>Random initialize:</p>
<p>首先满足 K &lt; m；</p>
<p>然后随机选择K个训练实例；</p>
<p>将u1,u2,…uk等于这K个实例；</p>
<pre><code>for i = 1 to 100{
    Randomly initialize K-means.
    Run K-means. Get c1,c2,...cm,u1,...uk.
    Compute cost function(distortion)
</code></pre><p>选择最小cost的聚类</p>
<h3 id="8-1-5-Choosing-the-number-of-clusters"><a href="#8-1-5-Choosing-the-number-of-clusters" class="headerlink" title="8.1.5 Choosing the number of clusters"></a>8.1.5 Choosing the number of clusters</h3><p>目前最好的方法还是手动选择；</p>
<p>Elbow cost：手肘方法，不是所有情况都能画出这样的图形，有时候会下降趋势较平滑。</p>
<p>sometimes, you’re running K-means to get clusters to use for some later/downstream purpose.</p>
<p>Evaluate K-means based on a metric for how well it performs for that later purpose.    </p>
<h2 id="8-2-Dimensionality-Reduction"><a href="#8-2-Dimensionality-Reduction" class="headerlink" title="8.2 Dimensionality Reduction"></a>8.2 Dimensionality Reduction</h2><p>介绍了第二种无监督学习算法：维数约减。</p>
<p>主要分为两大类应用：</p>
<ol>
<li>Data Compression</li>
<li>Data Visualization</li>
</ol>
<h3 id="8-2-1-Data-Compression"><a href="#8-2-1-Data-Compression" class="headerlink" title="8.2.1 Data Compression"></a>8.2.1 Data Compression</h3><p>将数据从2D 降维到 1D</p>
<p>将数据从3D 降维到 2D （将三维中的点投影到一个平面上）</p>
<h3 id="8-2-2-Data-Visualization"><a href="#8-2-2-Data-Visualization" class="headerlink" title="8.2.2 Data Visualization"></a>8.2.2 Data Visualization</h3><p>数据可视化，便于用户更好的观察数据，理解数据之间的含义。</p>
<p>将数据比较相关的进行降维，原来N维的数据降低到K维</p>
<p>k &lt;= N； 并且通常K=2或3， 便于进行可视化视图分析 </p>
<h2 id="8-3-Principal-Component-Analysis"><a href="#8-3-Principal-Component-Analysis" class="headerlink" title="8.3 Principal Component Analysis"></a>8.3 Principal Component Analysis</h2><h3 id="8-3-1-Principal-Component-Analysis-Problem-Formulation"><a href="#8-3-1-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="8.3.1 Principal Component Analysis Problem Formulation"></a>8.3.1 Principal Component Analysis Problem Formulation</h3><p>Reduce from 2-dimension to 1-dimension:</p>
<p>Find a direction (a vector v1) onto which to project the data so as to minimize the projection error.</p>
<p>Reduce from n-dimension to k-dimension:</p>
<p>Find k vectors v1,v2…vk on to which to project the data, so as to minimize the projection error.</p>
<p>PCA 不是线性回归：线性回归是竖着映射，PCA是垂直映射。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/8_2.png" alt="8_2"></p>
<h3 id="8-3-2-Principal-Component-Analysis-algorithm"><a href="#8-3-2-Principal-Component-Analysis-algorithm" class="headerlink" title="8.3.2 Principal Component Analysis algorithm"></a>8.3.2 Principal Component Analysis algorithm</h3><p><strong>Data Preprocessing:</strong></p>
<p>Training set: x1,x2,…xm</p>
<p>Preprocessing(feature scaling/mean normalization):</p>
<pre><code>uj = 1/m(x1+x2+...+xm)
xj&apos; = xj - uj
</code></pre><p>If different features on different scales, scale features to have comparable range of values.</p>
<p><strong>Principal Component Analysis (PCA) algorithm</strong></p>
<p>Reduce data from n-dimensions to k-dimensions</p>
<p>compute “covariance matrix”: sigma = 1/m(x1<em>x1T+…+xn</em>xnT)</p>
<p>compute “eigenvectors” of matrix sigma: [U,S,V] = svd(sigma)</p>
<p>Ureduce = U(:,1:k)</p>
<p>z = Ureduce’ * x</p>
<h3 id="8-4-Applying-PCA"><a href="#8-4-Applying-PCA" class="headerlink" title="8.4 Applying PCA"></a>8.4 Applying PCA</h3><h4 id="8-4-1-Reconstruction-from-Compressed-Representation"><a href="#8-4-1-Reconstruction-from-Compressed-Representation" class="headerlink" title="8.4.1 Reconstruction from Compressed Representation"></a>8.4.1 Reconstruction from Compressed Representation</h4><p>Xapprox = Ureduce*z</p>
<h4 id="8-4-2-Choosing-the-number-of-principal-components"><a href="#8-4-2-Choosing-the-number-of-principal-components" class="headerlink" title="8.4.2 Choosing the number of principal components"></a>8.4.2 Choosing the number of principal components</h4><p><strong>choosing k    </strong></p>
<p>(1)Average squared projection error: 1/m(||x1-xapprox1||2 +…)</p>
<p>(2)Total variation in the data : 1/m(||x1||2 + … +||xm||2)</p>
<p>Typically, choose k to be smallest value so that (1)/(2)&lt;=0.01</p>
<p>“99% of variance is retained”    </p>
<p><strong>Advice for applying PCA</strong></p>
<p>Supervised learning speedup</p>
<p>Compression:</p>
<ol>
<li>Reduce memory/disk needed to store data</li>
<li>speed up learning algorithm</li>
</ol>
<p>Visualization</p>
<p><strong>Bad use of PCA: To prevent overfitting</strong></p>
<p>fewer features, less likely to overfit.</p>
<p>This might work ok, but isn’t good way to address overfitting. Use regularization instead.</p>
<p>Before implementing PCA, first try running whatever you want to do with the original/raw data xi. Only if that doesn’t do what you want, then implement PCA and consider using zi.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/04/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第七周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第七周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-12-04T16:41:16+08:00" content="2016-12-04">
              2016-12-04
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="7、Support-Vector-Machines"><a href="#7、Support-Vector-Machines" class="headerlink" title="7、Support Vector Machines"></a>7、Support Vector Machines</h2><h3 id="7-1-Optimization-objective"><a href="#7-1-Optimization-objective" class="headerlink" title="7.1 Optimization objective"></a>7.1 Optimization objective</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/7_1.png" alt="7_1"><br><img src="http://ofacak8l3.bkt.clouddn.com/7_2.png" alt="7_2"></p>
<h3 id="7-2-Large-Margin-Intuition"><a href="#7-2-Large-Margin-Intuition" class="headerlink" title="7.2 Large Margin Intuition"></a>7.2 Large Margin Intuition</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/7_3.png" alt="7_3"></p>
<p>C如果越大，两个分类的间距越小。</p>
<h3 id="7-3-The-Mathematics-behind-large-margin-classification"><a href="#7-3-The-Mathematics-behind-large-margin-classification" class="headerlink" title="7.3 The Mathematics behind large margin classification"></a>7.3 The Mathematics behind large margin classification</h3><p>先讲解了||u||符号的含义,可以利用向量的长度来计算矩阵的乘积.</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_4.png" alt="7_4"><br>SVM Decision Boundary 计算时可以利用这个特性。</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_5.png" alt="7_5"></p>
<h3 id="7-4-Kernels"><a href="#7-4-Kernels" class="headerlink" title="7.4 Kernels"></a>7.4 Kernels</h3><h4 id="7-4-1-Kernels-I"><a href="#7-4-1-Kernels-I" class="headerlink" title="7.4.1 Kernels I"></a>7.4.1 Kernels I</h4><p>Gaussian Kernel.</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/7_6.png" alt="7_6"><br><img src="http://ofacak8l3.bkt.clouddn.com/7_7.png" alt="7_7"></p>
<h4 id="7-5-Kernels-II"><a href="#7-5-Kernels-II" class="headerlink" title="7.5 Kernels II"></a>7.5 Kernels II</h4><p>SVM parameters:<br>Large C: Lower bias, high variance</p>
<p>Small C: High bias, low variance</p>
<p>Large segma*segma: more smoothly. High bias, low variance.</p>
<p>Small segma*segma: less smoothly. Lower bias, higher variance.</p>
<h3 id="7-6-Using-an-SVM"><a href="#7-6-Using-an-SVM" class="headerlink" title="7.6 Using an SVM"></a>7.6 Using an SVM</h3><p>Use SVM software package(liblinear,libsvm…)to solve for parameters theta.</p>
<p><strong>Need to specify:</strong></p>
<pre><code>Choice of parameter C.
Choice of Kernel(similarity function):
    No Kernel(&apos;Linear Kernel&apos;)
    Gaussian Kernel.(need to choose segma)
</code></pre><p><strong>Note:</strong></p>
<pre><code>Do not perform feature scaling before using the Gaussian kernel.
</code></pre><p><strong>Other Choice of kernel:</strong></p>
<pre><code>Not all similarity functions similarity(x,l) make valid kernels. 
(Need to satisfy &apos;mercer&apos;s Theorem&apos; to make sure SVM package&apos;s optimications run correctly, and do not diverge)    
</code></pre><p><strong>Multi-class classification:</strong></p>
<p>Train K SVMs, 需要对K个类进行分类，而非k-1。</p>
<p><strong>Logistic regression vs SVM:</strong></p>
<p>n= number of features, m =number of training examples</p>
<pre><code>if n is large（相对于m）:
    using Logistic regression, or SVM without kernel.
if n is small, m is intermediate:
    use SVM with Gaussian Kernel.
if n is small, m is large:
    create/add more features,
    then use logistic regression or SVM without kernel.

Neural network likely to work well for most od these settings,
but many slower to train.
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/27/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第六周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第六周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-11-27T15:46:44+08:00" content="2016-11-27">
              2016-11-27
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="6、Advice-for-Applying-Machine-Learning"><a href="#6、Advice-for-Applying-Machine-Learning" class="headerlink" title="6、Advice for Applying Machine Learning"></a>6、Advice for Applying Machine Learning</h2><h3 id="6-1、Evaluating-a-learning-Algorithm"><a href="#6-1、Evaluating-a-learning-Algorithm" class="headerlink" title="6.1、Evaluating a learning Algorithm"></a>6.1、Evaluating a learning Algorithm</h3><h4 id="6-1-1、Deciding-What-to-Try-Next"><a href="#6-1-1、Deciding-What-to-Try-Next" class="headerlink" title="6.1.1、Deciding What to Try Next"></a>6.1.1、Deciding What to Try Next</h4><p>Which of the following statements about diagnostics are true? (BCD)</p>
<p>A. It’s hard to tell what will work to improve a learning algorithm, so the best approach is to go with gut feeling and just see what works.</p>
<p>B. Diagnostics can give guidance as to what might be more fruitful things to try to improve a learning algorithm.</p>
<p>C. Diagnostics can be time-consuming to implement and try, but they can still be a very good use of your time.</p>
<p>D. A diagnostic can sometimes rule out certain courses of action(changes to your learning algorithm) as being unlikely to improve its performance signigicantly.</p>
<h4 id="6-1-2、Evaluating-a-Hypothesis"><a href="#6-1-2、Evaluating-a-Hypothesis" class="headerlink" title="6.1.2、Evaluating a Hypothesis"></a>6.1.2、Evaluating a Hypothesis</h4><p>如何评估假设函数，后面可以以此为基础讨论如何避免过拟合和欠拟合的问题。</p>
<p>Suppose an implementation of linear regression(without reguarization) is badly overfitting the training set. In this case, we would expect:</p>
<p>The training error to be low, and the test error to be high.</p>
<p>数据划分为训练集和测试集，一般比例为7:3,random select。如果数据不是随机的，最好自己随机排序或打乱顺序后再选取70%.</p>
<p>Misclassification error(0/1 misclassification error)</p>
<h4 id="6-1-3、Model-Selection-and-Training-Validation-Test-Sets"><a href="#6-1-3、Model-Selection-and-Training-Validation-Test-Sets" class="headerlink" title="6.1.3、Model Selection and Training/Validation/Test Sets"></a>6.1.3、Model Selection and Training/Validation/Test Sets</h4><p>如何确定对于某组数据，最合适的多项式次数是几次？如何选择学习算法中的正规化参数lamda？</p>
<p>模型选择问题。（泛化误差）</p>
<p>我们将数据集分为三部分，training set(60%), cross validation set(20%), test set(20%).</p>
<p>Training error</p>
<p>cross validation error</p>
<p>test error</p>
<p>我们使用交叉验证集来选择模型，看这些假设在交叉验证集表现如何。选择最小交叉验证集误差的模型。</p>
<p>Consider the model selection procedure where we choose the degree of polynomial using a cross validation set. For the final model, we might generally expect Jcv to be lower than Jtest because:</p>
<p>An extra parameter(d,the degree of the polynomial) has been fit to the cross validation set.</p>
<h3 id="6-2、Bias-vs-Variance-偏差和方差"><a href="#6-2、Bias-vs-Variance-偏差和方差" class="headerlink" title="6.2、Bias vs. Variance(偏差和方差)"></a>6.2、Bias vs. Variance(偏差和方差)</h3><h4 id="6-2-1、Diagnosing-Bias-vs-Variance"><a href="#6-2-1、Diagnosing-Bias-vs-Variance" class="headerlink" title="6.2.1、Diagnosing Bias vs. Variance"></a>6.2.1、Diagnosing Bias vs. Variance</h4><p>High Bias Overfitting</p>
<p>High Variance Underfitting</p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/6_1.png" alt="6_1"></p>
<h4 id="6-2-2、Regularization-and-Bias-Variance"><a href="#6-2-2、Regularization-and-Bias-Variance" class="headerlink" title="6.2.2、Regularization and Bias/Variance"></a>6.2.2、Regularization and Bias/Variance</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_2.png" alt="6_2"></p>
<h4 id="6-2-3、Learning-Curves"><a href="#6-2-3、Learning-Curves" class="headerlink" title="6.2.3、Learning Curves"></a>6.2.3、Learning Curves</h4><p>判断一个假设是否存在偏差方差问题</p>
<pre><code>High Bias
If a learning algorithm is suffering from high bias, getting more training data will not help much.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_3.png" alt="6_3"><br>因此知道自己的假设是否存在高偏差/方差问题，非常有用。</p>
<pre><code>High variance
If a learning algorithm is suffering from high variance, getting more training data is likely to help.
</code></pre><p><img src="http://ofacak8l3.bkt.clouddn.com/6_4.png" alt="6_4"></p>
<h4 id="6-2-4、Deciding-What-to-Do-Next-Revisited"><a href="#6-2-4、Deciding-What-to-Do-Next-Revisited" class="headerlink" title="6.2.4、Deciding What to Do Next Revisited"></a>6.2.4、Deciding What to Do Next Revisited</h4><p><img src="http://ofacak8l3.bkt.clouddn.com/6_5.png" alt="6_5"></p>
<h3 id="6-3、Machine-Learning-system-design"><a href="#6-3、Machine-Learning-system-design" class="headerlink" title="6.3、Machine Learning system design"></a>6.3、Machine Learning system design</h3><h4 id="6-3-1、Priorityzing-what-to-work-on"><a href="#6-3-1、Priorityzing-what-to-work-on" class="headerlink" title="6.3.1、Priorityzing what to work on"></a>6.3.1、Priorityzing what to work on</h4><h4 id="6-3-2、Error-Analysis"><a href="#6-3-2、Error-Analysis" class="headerlink" title="6.3.2、Error Analysis"></a>6.3.2、Error Analysis</h4><p>建议的方法：</p>
<p>1、尽快的实现一个最基本的算法，然后在交叉验证数据集上进行验证；</p>
<p>2、画出learning curves，看是否更多的数据、更多的features能提升正确性；</p>
<p>3、Error Analysis：主要分析那些算法预测出错的例子，看能否发现一些系统趋势。</p>
<pre><code>Error analysis may not be helpful for deciding is this is likely to improve performance, only solution is to try it and say if it works.
</code></pre><h4 id="6-3-3-Error-Metrics-for-Skewed-Classes"><a href="#6-3-3-Error-Metrics-for-Skewed-Classes" class="headerlink" title="6.3.3 Error Metrics for Skewed Classes"></a>6.3.3 Error Metrics for Skewed Classes</h4><p>不能仅仅依靠错误率来判断算法的性能，还需要利用Precision 和 Recall<br><img src="http://ofacak8l3.bkt.clouddn.com/6_6.png" alt="6_6"></p>
<h4 id="6-3-4-Trading-off-precision-and-recall"><a href="#6-3-4-Trading-off-precision-and-recall" class="headerlink" title="6.3.4 Trading off precision and recall"></a>6.3.4 Trading off precision and recall</h4><p>我们可以通过计算F1来自动选择性能较高的算法。</p>
<pre><code>F1 = 2*(P*R)/(P+R);
</code></pre><h4 id="6-3-5-Data-for-Machine-Learning"><a href="#6-3-5-Data-for-Machine-Learning" class="headerlink" title="6.3.5 Data for Machine Learning"></a>6.3.5 Data for Machine Learning</h4><p>Banko anf Brill 在2001年提出了一个观点：</p>
<pre><code>It&apos;s not who has the best algorithm that wins, 
It&apos;s who has the most data.
</code></pre><p>这个观点有时候不一定正确：</p>
<p>当feature很少不足以预测时（例，预测房价时仅知道尺寸），此时再多的数据可能也无法提升算法的性能； </p>
<p>当算法参数很多时（例，逻辑回归和线性回归有很多的feature，或者神经网络有很多的隐藏单元），更大的测试集可以尽可能的减少过拟合。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/11/24/AI入门之——Andrew-Ng-“Machine-Learning”课程学习笔记第五周/" itemprop="url">
                  AI入门之——Andrew Ng “Machine Learning”课程学习笔记第五周
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-11-24T15:10:34+08:00" content="2016-11-24">
              2016-11-24
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="5、Neural-Networks：Learning"><a href="#5、Neural-Networks：Learning" class="headerlink" title="5、Neural Networks：Learning"></a>5、Neural Networks：Learning</h2><h3 id="5-1、Cost-Function-and-Backpropagation"><a href="#5-1、Cost-Function-and-Backpropagation" class="headerlink" title="5.1、Cost Function and Backpropagation"></a>5.1、Cost Function and Backpropagation</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_1.png" alt="5_1"></p>
<h3 id="5-2、Backprogation-algorithm"><a href="#5-2、Backprogation-algorithm" class="headerlink" title="5.2、Backprogation algorithm"></a>5.2、Backprogation algorithm</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_2.png" alt="5_2"></p>
<h3 id="5-3、Gradient-checking"><a href="#5-3、Gradient-checking" class="headerlink" title="5.3、Gradient checking"></a>5.3、Gradient checking</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_3.png" alt="5_3"></p>
<pre><code>Important:
Be sure to disable your gradient checking code before training your claaifier. otherwise code will be very slow.
</code></pre><h3 id="5-4-Random-initialization"><a href="#5-4-Random-initialization" class="headerlink" title="5.4 Random initialization"></a>5.4 Random initialization</h3><pre><code>initial each initialtheta to a random value in[-EPSILON,EPSILON]
</code></pre><h3 id="5-5-Put-it-together"><a href="#5-5-Put-it-together" class="headerlink" title="5.5 Put it together"></a>5.5 Put it together</h3><p><img src="http://ofacak8l3.bkt.clouddn.com/5_4.png" alt="5_4"></p>
<p><img src="http://ofacak8l3.bkt.clouddn.com/5_5.png" alt="5_5"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Frances Hu" />
          <p class="site-author-name" itemprop="name">Frances Hu</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">29</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Frances Hu</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
